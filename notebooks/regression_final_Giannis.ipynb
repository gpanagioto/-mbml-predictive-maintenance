{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7279b216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "import torch\n",
    "import pyro.distributions as dist\n",
    "import torch.nn as nn\n",
    "\n",
    "from pyro.nn import PyroModule, PyroSample\n",
    "from pyro.infer import SVI, Trace_ELBO, MCMC, NUTS\n",
    "from pyro.optim import ClippedAdam, Adam\n",
    "from pyro.contrib.autoguide import AutoDiagonalNormal\n",
    "from pyro.infer import Predictive\n",
    "from typing import Dict, Tuple, List\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas import DataFrame\n",
    "from tqdm.autonotebook import tqdm\n",
    "from pyro.nn import PyroModule, PyroSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c536413",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/raw/raw2'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a2d1a2c",
   "metadata": {},
   "source": [
    "# Predictive maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a97bfda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"marm.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c0e516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5699889",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b44f9301",
   "metadata": {},
   "source": [
    "Read the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf57d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "failures_path = os.path.join(data_path, 'PdM_failures.csv')\n",
    "errors_path = os.path.join(data_path, 'PdM_errors.csv')\n",
    "machines_path = os.path.join(data_path, 'PdM_machines.csv')\n",
    "maint_path = os.path.join(data_path, 'PdM_maint.csv')\n",
    "telemetry_path = os.path.join(data_path, 'PdM_telemetry.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1124d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "failures_df = pd.read_csv(failures_path)\n",
    "errors_df = pd.read_csv(errors_path)\n",
    "machines_df = pd.read_csv(machines_path)\n",
    "maint_df = pd.read_csv(maint_path)\n",
    "telemetry_df = pd.read_csv(telemetry_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d01458f9",
   "metadata": {},
   "source": [
    "Transform `datetime` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f197c9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "maint_df['datetime'] = pd.to_datetime(maint_df['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "failures_df['datetime'] = pd.to_datetime(failures_df['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "errors_df['datetime'] = pd.to_datetime(errors_df['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "telemetry_df['datetime'] = pd.to_datetime(telemetry_df['datetime'], format=\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "588ef274",
   "metadata": {},
   "source": [
    "#### Dataset transformation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "843f0a23",
   "metadata": {},
   "source": [
    "Maintenance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af80ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function that takes a column and returns its mean and std of a moving window of 3 hours\n",
    "def rolling_mean_std(col:str,window:int) -> float:\n",
    "    \n",
    "    '''\n",
    "    returns the mean and the std of a moving window of 3 hours for a column\n",
    "    '''\n",
    "    return col.rolling(window).agg(['mean', 'std'])\n",
    "\n",
    "#apply the function to the telemetry data\n",
    "def telem_(telemetry:DataFrame , column:str, window:int) -> DataFrame:\n",
    "    \n",
    "    '''\n",
    "    telemetry data where we apply the moving window of 3 hours\n",
    "    '''\n",
    "    \n",
    "    telemetry[[column+'mean_'+str(window)+'h', column+'sd_'+str(window)+'h']] = telemetry.groupby('machineID')[column].apply(rolling_mean_std,window)\n",
    "    \n",
    "    return telemetry\n",
    "\n",
    "def lifespan(replacement_event_df: DataFrame) -> DataFrame:\n",
    "    \n",
    "    '''\n",
    "    Receives a dataframe with timestamp and columns that signify when a component is replaced, with 1.\n",
    "    Returns a dataframe with the days since the last replacement for the component\n",
    "    '''\n",
    "    \n",
    "    comp_rep=replacement_event_df.copy()\n",
    "    points = comp_rep['machineID'].unique()\n",
    "    final=pd.DataFrame()\n",
    "    \n",
    "    for i in tqdm(points, desc='Machine'):\n",
    "        df = comp_rep[(comp_rep['machineID']==i)][['datetime','machineID','comp_comp1', 'comp_comp2', 'comp_comp3', 'comp_comp4']]\n",
    "        for comp in ['comp_comp1', 'comp_comp2', 'comp_comp3', 'comp_comp4']:\n",
    "            \n",
    "            # keep the last part of component name\n",
    "            life = comp[5:]\n",
    "            \n",
    "            #apply function in each row of df[life] column where if row[comp]==1 then value=0.041667 else 0\n",
    "            df[life+'_maint'] = df.apply(lambda row: 0 if row[comp]==1 else 0.041667, axis=1)\n",
    "\n",
    "            df_maint = df[life+'_maint'] != 0\n",
    "            df[life+'_maint'] = df_maint.cumsum()-df_maint.cumsum().where(~df_maint).ffill().fillna(0).astype(int)\n",
    "            df[life+'_maint'] = df[life+'_maint'].apply(lambda x: x*0.041667)\n",
    "            \n",
    "            \n",
    "        final=pd.concat([final,df],axis=0)\n",
    "        final=final[['datetime', 'machineID', 'comp1_maint', 'comp2_maint', 'comp3_maint', 'comp4_maint']]\n",
    "        \n",
    "    return final.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36129b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "telemetry_df = telem_(telemetry_df, column='volt', window=3)\n",
    "telemetry_df = telem_(telemetry_df,'rotate',3)\n",
    "telemetry_df = telem_(telemetry_df,'pressure',3)\n",
    "telemetry_df = telem_(telemetry_df,'vibration',3)\n",
    "telemetry_df = telem_(telemetry_df,'volt',24)\n",
    "telemetry_df = telem_(telemetry_df,'rotate',24)\n",
    "telemetry_df = telem_(telemetry_df,'pressure',24)\n",
    "telemetry_df = telem_(telemetry_df,'vibration',24)\n",
    "#telemetry_df=telemetry_df.drop(['volt','rotate','pressure','vibration'],axis=1)\n",
    "telemetry_df=telemetry_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ef9f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "maint_transf_df = pd.get_dummies(maint_df, columns=['comp'])\n",
    "maint_transf_df = telemetry_df.merge(maint_transf_df, on=['datetime', 'machineID'], how='left')\n",
    "maint_transf_df= maint_transf_df[['datetime', 'machineID', 'comp_comp1', 'comp_comp2', 'comp_comp3', 'comp_comp4']]\n",
    "maint_transf_df = maint_transf_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bd151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "maintenance_df = lifespan(maint_transf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c3f93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "maintenance_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31f84547",
   "metadata": {},
   "source": [
    "#### Merging the rest of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdad100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fget dummies for errorID\n",
    "error_count = pd.get_dummies(errors_df, columns=['errorID'])\n",
    "error_count.rename(columns={'errorID_error5':'error5count'}, inplace=True)\n",
    "error_count.rename(columns={'errorID_error4':'error4count'}, inplace=True)\n",
    "error_count.rename(columns={'errorID_error3':'error3count'}, inplace=True)\n",
    "error_count.rename(columns={'errorID_error2':'error2count'}, inplace=True)\n",
    "error_count.rename(columns={'errorID_error1':'error1count'}, inplace=True)\n",
    "\n",
    "features = telemetry_df.merge(error_count, on=['datetime', 'machineID'], how='left')\n",
    "\n",
    "# Propagate the error information per error type\n",
    "features[['error1count','error2count','error3count','error4count','error5count']] = features[['error1count','error2count','error3count','error4count','error5count']].fillna(method='ffill')\n",
    "# Fill the iinital error count with 0\n",
    "features = features.fillna(0)\n",
    "\n",
    "# turn \"model\" variable into dummy variables\n",
    "machines_df['model'] = machines_df['model'].astype('category')\n",
    "machines_dummy = pd.get_dummies(machines_df, drop_first=False)\n",
    "\n",
    "# Add the machine metadata information\n",
    "features = features.merge(machines_df[['machineID','model']], on=['machineID'], how='left')\n",
    "features = features.merge(machines_dummy, on=['machineID'], how='left')\n",
    "features = features.merge(maintenance_df, on=['datetime', 'machineID'], how='left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "023436ef",
   "metadata": {},
   "source": [
    "Merge the failures dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5592b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "fails = pd.get_dummies(failures_df,columns=['failure'])\n",
    "#fails.rename(columns={'failure_comp1':'comp1'}, inplace=True)\n",
    "#fails.rename(columns={'failure_comp2':'comp2'}, inplace=True)\n",
    "#fails.rename(columns={'failure_comp3':'comp3'}, inplace=True)\n",
    "#fails.rename(columns={'failure_comp4':'comp4'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69342e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lifespan_fails(comp_rep0: DataFrame) -> DataFrame:\n",
    "    \n",
    "    '''\n",
    "    creates the reverse counting column until the component's fail\n",
    "\n",
    "    '''\n",
    "    comp_rep=comp_rep0.copy()\n",
    "    points = comp_rep['machineID'].unique()\n",
    "    final=pd.DataFrame()\n",
    "\n",
    "    for i in points:\n",
    "        df = comp_rep[(comp_rep['machineID']==i)][['datetime','machineID','failure_comp1','failure_comp2','failure_comp3','failure_comp4']]\n",
    "        for comp in ['failure_comp1','failure_comp2','failure_comp3','failure_comp4']:\n",
    "            life=comp.split('_')[1]+'_life'\n",
    "            df[life] = df.apply(lambda row: row['datetime'] if row[comp]==0 else np.nan, axis=1)\n",
    "            df[df[life].isna()==False].index\n",
    "            df[life].fillna(method='backfill', inplace=True)\n",
    "            df[life] = pd.to_datetime(df[life]) - df['datetime']\n",
    "            df[life] = df[life].apply(lambda row: row.total_seconds()/86400)\n",
    "            \n",
    "        final=pd.concat([final,df],axis=0)\n",
    "        \n",
    "    return final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ce270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fails_ = telemetry_df.merge(fails, on=['datetime', 'machineID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8599f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fails_ = fails_.fillna(method='bfill', limit=24) # fill backward up to 24h if all data, otherwise it must be  7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07adcd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "fails_transf = lifespan_fails(fails_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6c7774",
   "metadata": {},
   "outputs": [],
   "source": [
    "fails_transf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da269f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10854d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the individual components that fail , the columns with 0 or 1, after the merge and backfill\n",
    "fails_transf = fails_transf.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f218cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features = features.merge(fails_transf, on=['datetime', 'machineID'], how='left')\n",
    "# labeled_features = labeled_features.fillna(method='bfill', limit=24) # fill backward up to 24h if all data, otherwise it must be  7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305ea944",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20a218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features.to_csv('../data/processed/labeled_1h_data.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b787046",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d87d28dc",
   "metadata": {},
   "source": [
    "Machine age - machine age by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3c3fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxh_groupby(df: DataFrame, feature_name: str, by: str):\n",
    "    \"\"\"\n",
    "    Box plot with groupby\n",
    "    \n",
    "    df: DataFrame\n",
    "    feature_name: Name of the feature to be plotted\n",
    "    by: Name of the feature based on which groups are created\n",
    "    \"\"\"\n",
    "    df.boxplot(column=feature_name, by=by, vert=False, \n",
    "                              figsize=(10, 6))\n",
    "    plt.title(f'Distribution of {feature_name} by {by}')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_histogram(df: DataFrame, attribute:str, title_name:str, bins: int, figsize=(9,3), alpha=1, label=None):\n",
    "    \n",
    "    '''\n",
    "    histogram plot\n",
    "    '''\n",
    "    \n",
    "    df[attribute].plot(kind='hist', \n",
    "                              bins=bins, \n",
    "                              figsize=figsize,\n",
    "                              alpha=alpha,\n",
    "                              label=label,\n",
    "                              title=f'{title_name.title()} distribution')\n",
    "    \n",
    "def plot_bar_sortvals(df: DataFrame, attribute: str, title: str, figsize=(5,5)):\n",
    "    df[attribute].value_counts(\n",
    "                normalize=False, dropna=False).sort_values().plot(\n",
    "                kind='bar', figsize=figsize, grid=True,\n",
    "                title=title)\n",
    "\n",
    "    \n",
    "def plot_scatter(df: DataFrame, x_axis_attr: str , y_axis_attr: str, figsize=(5,5), title=None, legend=None):\n",
    "    df.plot.scatter(x_axis_attr, y_axis_attr, \n",
    "                    figsize=figsize, title=title, \n",
    "                    legend=legend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa842fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features.boxplot(column='age', vert=False, figsize=(5,3))\n",
    "plt.title('Age distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8421c49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features.boxplot(column='age', by='model', vert=False, figsize=(8,5))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ba7cf1f",
   "metadata": {},
   "source": [
    "```\n",
    "We can see that `model 4` has the lowest median age\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f521edcc",
   "metadata": {},
   "source": [
    "Failure per component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f8b5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_sortvals(failures_df, 'failure', 'Number of failures per component')\n",
    "plt.ylabel('Failures')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46ca890e",
   "metadata": {},
   "source": [
    "Telemetry data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b4e1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(labeled_features, 'volt', 'Voltage', bins=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c078fa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(labeled_features, 'rotate', 'RPM', bins=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236abe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(labeled_features, 'pressure', 'Pressure', bins=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d242ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(labeled_features, 'vibration', 'Vibration', bins=300)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6bdb5d03",
   "metadata": {},
   "source": [
    "```\n",
    "All the telemetry data, taking into consideration all machines, look to be normally distributed. The odd one being rotation, which looks a skewed on the left.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93caac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in labeled_features.groupby(['model']):\n",
    "    plot_histogram(model[1], 'volt', 'Voltage',bins=300, alpha=0.3, label=model[0])\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e58888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in labeled_features.groupby(['model']):\n",
    "    plot_histogram(model[1], 'rotate', 'RPM',bins=300, alpha=0.3, label=model[0])\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52339df4",
   "metadata": {},
   "source": [
    "#### Create a temporary dataframe, which is the `labeled_features` dataframe used in training, but augmented with time features:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "474ae189",
   "metadata": {},
   "source": [
    "Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b71d930",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_sortvals(errors_df, 'errorID', title='Number of error per type')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44919d20",
   "metadata": {},
   "source": [
    "```\n",
    "The most common error type is 1, while the least common is error 5.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e018b5c2",
   "metadata": {},
   "source": [
    "Plot age vs errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc03edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_per_machine = errors_df.groupby(\"machineID\").size()\n",
    "errors_per_machine = pd.DataFrame(errors_per_machine, columns=[\"num_errors\"]).reset_index()\n",
    "\n",
    "machines_fail_data = pd.merge(machines_df, errors_per_machine, how='left', on=\"machineID\")\n",
    "\n",
    "maint_per_machine = maint_df.groupby(\"machineID\").size()\n",
    "maint_per_machine = pd.DataFrame(maint_per_machine, columns=[\"num_maint\"]).reset_index()\n",
    "\n",
    "machines_fail_data = pd.merge(machines_fail_data, maint_per_machine, how='left', on=\"machineID\")\n",
    "\n",
    "failure_per_machine = failures_df.groupby(\"machineID\").size()\n",
    "failure_per_machine = pd.DataFrame(failure_per_machine, columns=[\"num_failure\"]).reset_index()\n",
    "\n",
    "machines_fail_data = pd.merge(machines_fail_data, failure_per_machine, how='left', on=\"machineID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a6e3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "machines_fail_data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011e0599",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(machines_fail_data, \"age\", \"num_errors\", \n",
    "             title=\"Age versus number of errors\")\n",
    "\n",
    "plot_scatter(machines_fail_data, \"age\", \"num_failure\", \n",
    "             title=\"Age versus number of failures\")\n",
    "\n",
    "plot_scatter(machines_fail_data, \"age\", \"num_maint\", \n",
    "             title=\"Age versus total number of\\ncomponent maintenance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2c1223",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = machines_fail_data.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm').set_precision(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1087117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_features = labeled_features.copy()\n",
    "augmented_features['month'] = augmented_features.datetime.dt.month\n",
    "augmented_features['week_of_year'] = augmented_features.datetime.dt.isocalendar().week\n",
    "augmented_features['hour'] = augmented_features.datetime.dt.hour"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4061f240",
   "metadata": {},
   "source": [
    "Failures per machine per component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1860527",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = failures_df.groupby([\"machineID\", \"failure\"]).size().reset_index()\n",
    "temp_df.columns = [\"machineID\", \"comp\", \"num_fail\"]\n",
    "temp_df_pivot = pd.pivot(temp_df, index=\"machineID\", columns=\"comp\", values=\"num_fail\").rename_axis(None, axis=1)\n",
    "\n",
    "temp_df_pivot.plot.bar(stacked=True, figsize=(20, 6), title=\"Count of failures per component for different Machines\")\n",
    "plt.xlabel(\"Machine ID\")\n",
    "plt.ylabel(\"Number of components that failed\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0398d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_features['month'].value_counts(\n",
    "                normalize=False, dropna=False).sort_values().plot(\n",
    "                kind='bar', figsize=(5,5), grid=True,\n",
    "                title='Number of error per type')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b56ad53",
   "metadata": {},
   "source": [
    "### data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b384e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features = pd.read_csv('../data/processed/labeled_1h_data.csv',  index_col='datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d27bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c0fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6606d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77036eb6",
   "metadata": {},
   "source": [
    "## Regression\n",
    "The following part includes the regression models (linear, poisson, heteroscedatic) and techniques of SVI and MCMC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c54aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(data: DataFrame, test_size: float) ->  tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \n",
    "    '''\n",
    "    data split in a way that all the machines' models are represented in both the train and test data.\n",
    "    '''\n",
    "    machines = machines_df\n",
    "    \n",
    "    x_train_,x_test_,y_train_,y_test_,train_idx,test_idx = train_test_split(machines, machines.model, machines.machineID.index, test_size=test_size, stratify=machines.model, random_state=42)\n",
    "    \n",
    "    training=data[data['machineID'].isin(train_idx)]\n",
    "    training=training.drop(columns=['machineID']).to_numpy()\n",
    "    test=data[data['machineID'].isin(test_idx)]\n",
    "    test=test.drop(columns=['machineID']).to_numpy()\n",
    "    \n",
    "    x_train=training[:,:-1]\n",
    "    y_train=training[:,-1]\n",
    "    x_test=test[:,:-1]\n",
    "    y_test=test[:,-1]\n",
    "\n",
    "    return x_train,x_test,y_train,y_test\n",
    "\n",
    "def get_data_for_component(data, component):\n",
    "    components_cols = ['comp1_maint', 'comp2_maint', 'comp3_maint', 'comp4_maint',\n",
    "       'failure_comp1', 'failure_comp2', 'failure_comp3', 'failure_comp4',\n",
    "       'comp1_life', 'comp2_life', 'comp3_life', 'comp4_life']\n",
    "    \n",
    "    cols = ['machineID', 'voltmean_3h', 'rotatemean_3h',\n",
    "                'pressuremean_3h', 'vibrationmean_3h', 'voltsd_3h', 'rotatesd_3h',\n",
    "                'pressuresd_3h', 'vibrationsd_3h', 'voltmean_24h', 'rotatemean_24h',\n",
    "                'pressuremean_24h', 'vibrationmean_24h', 'voltsd_24h', 'rotatesd_24h',\n",
    "                'pressuresd_24h', 'vibrationsd_24h', 'error1count', 'error2count',\n",
    "                'error3count', 'error4count', 'error5count','age',\n",
    "                'model_model1', 'model_model2', 'model_model3', 'model_model4'] + [word for word in components_cols if str(component) in ([*word]) and 'life' not in word.split('_')]\n",
    "\n",
    "    return data[cols]\n",
    "\n",
    "def preprocess(X_init, test_size, model:str, classi=False, splitting=True) -> tuple[np.ndarray, np.ndarray, torch.tensor, torch.tensor, torch.tensor, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "\n",
    "    '''\n",
    "    preprocess including split and standarization\n",
    "    '''\n",
    "    if splitting:\n",
    "        X_train_unsc, X_test_unsc, y_train_unsc, y_test_unsc =split(X_init,test_size=test_size)\n",
    "        X_init = X_init.drop(columns=['machineID']).to_numpy()\n",
    "        y = X_init[:,-1]\n",
    "        X = X_init[:,:-1]\n",
    "    else:\n",
    "        X_init = X_init.drop(columns=['machineID']).to_numpy()\n",
    "        y = X_init[:,-1]\n",
    "        X = X_init[:,:-1]\n",
    "        X_train_unsc, X_test_unsc, y_train_unsc, y_test_unsc = train_test_split(X, y, test_size=test_size, random_state=0)\n",
    "            \n",
    "        #print(X_train_unsc.shape, y_train_unsc.shape, X_test_unsc.shape, y_test_unsc.shape)\n",
    "        \n",
    "    X_mean = X_train_unsc.mean(axis=0)\n",
    "    X_std = X_train_unsc.std(axis=0)\n",
    "\n",
    "    y_std = y_train_unsc.std()\n",
    "    y_mean = y_train_unsc.mean()\n",
    "\n",
    "    X_train = (X_train_unsc - X_mean)/ X_std\n",
    "    X_test = (X_test_unsc - X_mean)/X_std\n",
    "\n",
    "    y_train = (y_train_unsc- y_mean)/ y_std \n",
    "    y_test = (y_test_unsc- y_mean)/y_std        \n",
    "\n",
    "    X_train_torch = torch.tensor(X_train).float()\n",
    "        \n",
    "    if model == SVI_model_format.poisson['name']:\n",
    "        y_train_torch = torch.tensor(y_train * y_std + y_mean).int()\n",
    "    else:\n",
    "        y_train_torch = torch.tensor(y_train).float()\n",
    "        \n",
    "    if classi:\n",
    "        y_train_torch = torch.tensor(y_train_unsc).float()\n",
    "        y_train = y_train_unsc\n",
    "        y_test = y_test_unsc\n",
    "    X_test_torch = torch.tensor(X_test).float()\n",
    "\n",
    "    return y, X, X_train_torch, y_train_torch,X_test_torch, X_test, y_test, X_train, y_train, y_std, y_mean\n",
    "\n",
    "    \n",
    "def linear_model(X : np.ndarray, obs=None) -> torch.tensor:\n",
    "    \n",
    "    '''\n",
    "    pyro model for linear regression prediction \n",
    "    '''\n",
    "    alpha = pyro.sample(\"alpha\", dist.Normal(0., 1.))                   # Prior for the bias/intercept\n",
    "    beta  = pyro.sample(\"beta\", dist.Normal(torch.zeros(X.shape[1]), \n",
    "                                            torch.ones(X.shape[1])).to_event())    # Priors for the regression coeffcients\n",
    "    sigma = pyro.sample(\"sigma\", dist.HalfCauchy(5.))                   # Prior for the variance\n",
    "    \n",
    "    with pyro.plate(\"data\"):\n",
    "        y = pyro.sample(\"y\", dist.Normal(alpha + X.matmul(beta), sigma), obs=obs)\n",
    "      \n",
    "    return y\n",
    "\n",
    "\n",
    "def poisson_model(X : np.ndarray, obs=None):\n",
    "    \n",
    "    '''\n",
    "     pyro model for poisson regression prediction\n",
    "    ''' \n",
    "    alpha = pyro.sample(\"alpha\", dist.Normal(0., 1.))                   # Prior for the bias/intercept\n",
    "    beta  = pyro.sample(\"beta\", dist.Normal(torch.zeros(X.shape[1]), \n",
    "                                            torch.ones(X.shape[1])).to_event())    # Priors for the regression coeffcients\n",
    "    \n",
    "    with pyro.plate(\"data\"):\n",
    "        y = pyro.sample(\"y\", dist.Poisson(torch.exp(alpha + X.matmul(beta))), obs=obs)\n",
    "        \n",
    "    return y\n",
    "\n",
    "\n",
    "def heteroscedastic_model(X : np.ndarray, obs=None):\n",
    "    \n",
    "    '''\n",
    "    pyro model for heteroscedastic regression prediction \n",
    "    '''\n",
    "    alpha_mu = pyro.sample(\"alpha_mu\", dist.Normal(0., 1.))                 # Prior for the bias/intercept of the mean\n",
    "    beta_mu  = pyro.sample(\"beta_mu\", dist.Normal(torch.zeros(X.shape[1]), \n",
    "                                               torch.ones(X.shape[1])).to_event())     # Priors for the regression coeffcients of the mean\n",
    "    alpha_v = pyro.sample(\"alpha_v\", dist.Normal(0., 1.))                   # Prior for the bias/intercept of the variance\n",
    "    beta_v  = pyro.sample(\"beta_v\", dist.Normal(torch.zeros(X.shape[1]), \n",
    "                                               torch.ones(X.shape[1])).to_event())     # Priors for the regression coeffcients of the variance\n",
    "    \n",
    "    with pyro.plate(\"data\"):\n",
    "        y = pyro.sample(\"y\", dist.Normal(alpha_mu + X.matmul(beta_mu), torch.exp(alpha_v + X.matmul(beta_v))), obs=obs)\n",
    "        \n",
    "    return y\n",
    "\n",
    "def compute_error(trues: np.array, predicted: np.array, threshold: int):\n",
    "    \n",
    "    '''\n",
    "    error calculator\n",
    "    '''\n",
    "    if threshold:\n",
    "        predicted_thres = predicted[np.where(trues<threshold)]\n",
    "        trues_thres  = trues[np.where(trues<threshold)[0]]\n",
    "    else:\n",
    "        print('No threshold')\n",
    "        pass\n",
    "        \n",
    "    corr = np.corrcoef(predicted, trues)[0,1]\n",
    "    mae = np.mean(np.abs(predicted - trues))\n",
    "    rae = np.sum(np.abs(predicted - trues)) / np.sum(np.abs(trues - np.mean(trues)))\n",
    "    rmse = np.sqrt(np.mean((predicted - trues)**2))\n",
    "    r2 = max(0, 1 - np.sum((trues-predicted)**2) / np.sum((trues - np.mean(trues))**2))\n",
    "    try:\n",
    "        return corr, mae, rae, rmse, r2, predicted_thres, trues_thres\n",
    "    except:\n",
    "         return corr, mae, rae, rmse, r2, predicted, trues\n",
    "        \n",
    "def results(dataset: pd.DataFrame, features_importance: np.array, results_dict: Dict, model:str, comp_number: int, y_trues:np.array, y_preds:np.array) -> Dict:\n",
    "    \n",
    "    '''\n",
    "    produce the results of a model train and test, including errors and feautures importance. \n",
    "    A dictionary with MAE and most important feature is returned.\n",
    "    '''\n",
    "    dataset.drop(columns='machineID', inplace=True, axis=1)\n",
    "  \n",
    "    corr, mae, rae, rmse, r2, svi_trues, svi_pred = compute_error(trues=y_trues, predicted=y_preds, threshold=None)\n",
    "\n",
    "    results_dict[model][f\"comp_{comp_number}\"]['MAE'] = mae\n",
    "    \n",
    "    print(\"CorrCoef: %.3f\\nMAE: %.3f\\nRMSE: %.3f\\nR2: %.3f\" % (corr, mae, rmse, r2))\n",
    "    \n",
    "    sort = features_importance.argsort()\n",
    "    \n",
    "    feautures_dict = dict(zip(dataset.columns[sort].tolist()[0], features_importance[0][sort].tolist()[0]))\n",
    "    \n",
    "    results_dict[model][f\"comp_{comp_number}\"]['FI'] = list(feautures_dict.keys())[list(feautures_dict.values()).index(max(list(feautures_dict.values())) )]\n",
    "    plt.barh(dataset.columns[sort].tolist()[0], features_importance[0][sort].tolist()[0])\n",
    "    plt.xlabel(\"Feature Importance\")\n",
    "    plt.show()\n",
    "  \n",
    "    return results_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca31e841",
   "metadata": {},
   "source": [
    "### SVI\n",
    "In this part we implement the SVI (Stochastic Variational Inference) Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d90e0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVI_model_format():\n",
    "    '''\n",
    "    defining the type of the regression\n",
    "    '''\n",
    "    poisson = {'name':\"POISSON\", 'model':poisson_model}\n",
    "    heterosc = {'name':\"HETEROSCEDASTIC\", 'model':heteroscedastic_model}\n",
    "    linear = {'name':\"LINEAR\", 'model':linear_model}\n",
    "\n",
    "class SVI_regression_model():\n",
    "    '''\n",
    "    SVI regression: choosing the correct data, preprocess them, pyro inference and prediction\n",
    "    '''\n",
    "    def __init__(self, data: pd.DataFrame, component: int) -> None:\n",
    "        \n",
    "        self.data = data\n",
    "        self.component = component\n",
    "        print(self.component)\n",
    "\n",
    "    def get_data_for_component(self) -> pd.DataFrame:\n",
    "        \n",
    "        '''\n",
    "        returns the feautures of the dataset and the component we want to predict for\n",
    "        '''\n",
    "        \n",
    "        components_cols = ['comp1_maint', 'comp2_maint', 'comp3_maint', 'comp4_maint',\n",
    "                           'failure_comp1', 'failure_comp2', 'failure_comp3', 'failure_comp4',\n",
    "                           'comp1_life', 'comp2_life', 'comp3_life', 'comp4_life']\n",
    "    \n",
    "        cols = ['machineID', 'voltmean_3h', 'rotatemean_3h', 'pressuremean_3h', 'vibrationmean_3h',\n",
    "                'voltsd_3h', 'rotatesd_3h', 'pressuresd_3h', 'vibrationsd_3h', 'voltmean_24h',\n",
    "                'rotatemean_24h', 'pressuremean_24h', 'vibrationmean_24h', 'voltsd_24h', 'rotatesd_24h',\n",
    "                'pressuresd_24h', 'vibrationsd_24h', 'error1count', 'error2count','error3count',\n",
    "                'error4count', 'error5count','age', 'model_model1', 'model_model2', \n",
    "                'model_model3', 'model_model4'] + [word for word in components_cols if str(self.component) in ([*word]) and 'failure' not in word.split('_')]\n",
    "\n",
    "\n",
    "        return self.data[cols]\n",
    "    \n",
    "    \n",
    "    def Preprocess(self, X_init: pd.DataFrame, model: str):\n",
    "\n",
    "        '''\n",
    "        the necessary data preprocess before procceding in pyro inference\n",
    "        '''\n",
    "        print(f\"{model} Regression {self.component}\")\n",
    "        print(X_init.columns)\n",
    "        print('\\n-----------------------------------------------------------------------')\n",
    "\n",
    "        y, X, X_train_torch, y_train_torch, X_test_torch, X_test, y_test, X_train, y_train, y_std, y_mean = preprocess(X_init, 0.1, model, classi=False, splitting=True)\n",
    " \n",
    "        return y, X, X_train_torch, y_train_torch, X_test_torch, X_test, y_test, X_train, y_train, y_std, y_mean\n",
    "        \n",
    "    def pyro_inference(self, X_train_torch: torch.tensor, y_train_torch: torch.tensor, model: object, steps: int):\n",
    "        '''\n",
    "        pyro inference\n",
    "        '''\n",
    "        \n",
    "        # Reset parameter values\n",
    "        pyro.clear_param_store()\n",
    "\n",
    "        # Define guide function\n",
    "        guide = AutoDiagonalNormal(model)\n",
    "\n",
    "        # Define the number of optimization steps\n",
    "        n_steps = steps\n",
    "\n",
    "        # Setup the optimizer\n",
    "        adam_params = {\"lr\": 0.0001} # learning rate (lr) of optimizer\n",
    "        optimizer = ClippedAdam(adam_params)\n",
    "\n",
    "        # Setup the inference algorithm\n",
    "        elbo = Trace_ELBO(num_particles=1)\n",
    "        svi = SVI(model, guide, optimizer, loss=elbo)\n",
    "\n",
    "        # Do gradient steps\n",
    "        for step in range(n_steps):\n",
    "            elbo = svi.step(X_train_torch, y_train_torch)\n",
    "            if step % 100 == 0:\n",
    "                print(\"[%d] ELBO: %.1f\" % (step, elbo))\n",
    "                \n",
    "        return guide\n",
    "    \n",
    "    def post_process(self, guide: object, model: object, X_train_torch: torch.tensor, y_train_torch: torch.tensor, X_test: np.ndarray, y_test: np.ndarray, y_std: np.float64, y_mean: np.float64):\n",
    "\n",
    "        '''\n",
    "        after the train we make the prediction and calculate the features importance\n",
    "        '''\n",
    "        if model['name'] != SVI_model_format.heterosc['name']:\n",
    "            predictive = Predictive(model=model['model'], guide=guide, num_samples=1000, return_sites=(\"alpha\", \"beta\", \"sigma\"))\n",
    "            samples = predictive(X_train_torch, y_train_torch)\n",
    "\n",
    "            alpha_samples = samples[\"alpha\"].detach().numpy()\n",
    "            beta_samples = samples[\"beta\"].detach().numpy()\n",
    "\n",
    "            y_hat = np.mean(np.exp(alpha_samples.T + np.dot(X_test, beta_samples[:,0].T)), axis=1)\n",
    "\n",
    "        else:\n",
    "            predictive = Predictive(model=model['model'], guide=guide, num_samples=1000,\n",
    "                        return_sites=(\"alpha_mu\", \"beta_mu\", \"alpha_v\", \"beta_v\"))\n",
    "            samples = predictive(X_train_torch, y_train_torch)\n",
    "\n",
    "            alpha_samples = samples[\"alpha_mu\"].detach().numpy()\n",
    "            beta_samples = samples[\"beta_mu\"].detach().numpy()\n",
    "\n",
    "            y_hat = np.mean(alpha_samples.T + np.dot(X_test, beta_samples[:,0].T), axis=1)\n",
    "            \n",
    "        # Calculate feature importance\n",
    "        feature_importance = np.mean(np.abs(beta_samples), axis=0)\n",
    "        feature_importance /= np.sum(feature_importance)\n",
    "\n",
    "        # convert back to the original scale\n",
    "        if model['name'] == SVI_model_format.poisson['name']:\n",
    "            preds = y_hat # no need to do any conversion here because the Poisson model received untransformed y's\n",
    "        else:\n",
    "            preds = y_hat * y_std + y_mean\n",
    "            \n",
    "        y_true = y_test * y_std + y_mean\n",
    "\n",
    "        return preds, y_true, feature_importance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a41231a9",
   "metadata": {},
   "source": [
    "#### Poisson Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd5a4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_results = {\"poisson\": {'comp_1':  {'MAE':0, 'FI':''}, 'comp_2':  {'MAE':0, 'FI':''}, 'comp_3':  {'MAE':0, 'FI':''}, 'comp_4':  {'MAE':0, 'FI':''}}}\n",
    "\n",
    "for comp_number in range(1,5):\n",
    "    print(f\"\\n--------------------------------  component {comp_number}  ---------------------------------------\")\n",
    "    svi_regression = SVI_regression_model(labeled_features, comp_number)\n",
    "    svi_dataset = svi_regression.get_data_for_component()\n",
    "    poisson = SVI_model_format.poisson \n",
    "\n",
    "    y_poisson, X_poisson, X_train_torch_poisson, y_train_torch_poisson, X_test_torch_poisson, X_test_poisson, y_test_poisson, X_train_poisson, y_train_poisson, y_std_poisson, y_mean_poisson = svi_regression.Preprocess(X_init=svi_dataset, model=poisson['name'])\n",
    "    poisson_guide = svi_regression.pyro_inference(X_train_torch=X_train_torch_poisson, y_train_torch=y_train_torch_poisson, model=poisson['model'], steps=5000)\n",
    "    poisson_preds, poisson_y_true, poisson_features_importance = svi_regression.post_process(guide=poisson_guide, model=poisson, X_train_torch=X_train_torch_poisson, y_train_torch=y_train_torch_poisson, X_test=X_test_poisson, y_test=y_test_poisson, y_std=y_std_poisson, y_mean=y_mean_poisson)\n",
    "    \n",
    "    poisson_results = results(svi_dataset, poisson_features_importance, poisson_results, 'poisson', comp_number, poisson_y_true, poisson_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1134b886",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "poisson_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f46f46fe",
   "metadata": {},
   "source": [
    "#### Heteorscedastic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7aa62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hetero_results = {'hetero': {'comp_1':  {'MAE':0, 'FI':''}, 'comp_2':  {'MAE':0, 'FI':''}, 'comp_3':  {'MAE':0, 'FI':''}, 'comp_4':  {'MAE':0, 'FI':''}}}\n",
    "\n",
    "for comp_number in range(1,5):\n",
    "    \n",
    "    print(f\"\\n--------------------------------  component {comp_number}  ---------------------------------------\")\n",
    "    svi_regression = SVI_regression_model(labeled_features, comp_number)\n",
    "    svi_dataset = svi_regression.get_data_for_component()\n",
    "    hetero = SVI_model_format.heterosc\n",
    "    \n",
    "    y_hetero, X_hetero, X_train_torch_hetero, y_train_torch_hetero, X_test_torch_hetero, X_test_hetero, y_test_hetero, X_train_hetero, y_train_hetero, y_std_hetero, y_mean_hetero = svi_regression.Preprocess(X_init=svi_dataset, model=hetero['name'])\n",
    "    hetero_guide = svi_regression.pyro_inference(X_train_torch=X_train_torch_hetero, y_train_torch=y_train_torch_hetero, model=hetero['model'], steps=5000)\n",
    "    hetero_preds, hetero_y_true, hetero_features_importance = svi_regression.post_process(guide=hetero_guide, model=hetero, X_train_torch=X_train_torch_hetero, y_train_torch=y_train_torch_hetero, X_test=X_test_hetero, y_test=y_test_hetero, y_std=y_std_hetero, y_mean=y_mean_hetero)\n",
    "\n",
    "    hetero_results = results(svi_dataset, hetero_features_importance, hetero_results, 'hetero', comp_number, hetero_y_true, hetero_preds)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68981060",
   "metadata": {},
   "outputs": [],
   "source": [
    "hetero_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55829176",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75397c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svi_results = {'linear_svi': {'comp_1':  {'MAE':0, 'FI':''}, 'comp_2':  {'MAE':0, 'FI':''}, 'comp_3':  {'MAE':0, 'FI':''}, 'comp_4':  {'MAE':0, 'FI':''}}}\n",
    "\n",
    "for comp_number in range(1,5):\n",
    "    \n",
    "    print(f\"\\n--------------------------------  component {comp_number}  ---------------------------------------\")\n",
    "    svi_regression = SVI_regression_model(labeled_features, comp_number)\n",
    "    svi_dataset = svi_regression.get_data_for_component()\n",
    "    linear_svi = SVI_model_format.linear\n",
    " \n",
    "    y_linear_svi, X_linear_svi, X_train_torch_linear_svi, y_train_torch_linear_svi,  X_test_torch_linear_svi, X_test_linear_svi, y_test_linear_svi, X_train_linear_svi, y_train_linear_svi, y_std_linear_svi, y_mean_linear_svi = svi_regression.Preprocess(X_init=svi_dataset, model=linear_svi['name'])\n",
    "    linear_svi_guide = svi_regression.pyro_inference(X_train_torch=X_train_torch_linear_svi, y_train_torch=y_train_torch_linear_svi, model=linear_svi['model'], steps=5000)\n",
    "    linear_svi_preds, linear_svi_y_true, linear_features_importance = svi_regression.post_process(guide=linear_svi_guide, model=linear_svi, X_train_torch=X_train_torch_linear_svi, y_train_torch=y_train_torch_linear_svi, X_test=X_test_linear_svi, y_test=y_test_linear_svi, y_std=y_std_linear_svi, y_mean=y_mean_linear_svi)\n",
    "    \n",
    "    linear_svi_results = results(svi_dataset, linear_features_importance, linear_svi_results, 'linear_svi', comp_number, linear_svi_y_true, linear_svi_preds)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e101fc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svi_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "918eeb21",
   "metadata": {},
   "source": [
    "### MCMC\n",
    "In this part we implement the MCMC (Markov Chain Monte Carlo) Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85da0c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCMC_model_format():\n",
    "    '''\n",
    "    defining the type of the regression\n",
    "    '''\n",
    "    poisson = {'name':\"POISSON\", 'model':poisson_model}\n",
    "    heterosc = {'name':\"HETEROSCEDASTIC\", 'model':heteroscedastic_model}\n",
    "    linear = {'name':\"LINEAR\", 'model':linear_model}\n",
    "\n",
    "class MCMC_regression_model():\n",
    "    \n",
    "    '''\n",
    "    implementation of mcmc regression\n",
    "    '''\n",
    "\n",
    "    def __init__(self, data: pd.DataFrame, component: int) -> None:\n",
    "        \n",
    "        self.data = data\n",
    "        self.component = component\n",
    "        print(self.component)\n",
    "\n",
    "    def get_data_for_component(self) -> pd.DataFrame:\n",
    "        \n",
    "        '''\n",
    "        returns the feautures of the dataset and the component we want to predict for\n",
    "        '''\n",
    "\n",
    "    \n",
    "        components_cols = ['comp1_maint', 'comp2_maint', 'comp3_maint', 'comp4_maint',\n",
    "                           'failure_comp1', 'failure_comp2', 'failure_comp3', 'failure_comp4',\n",
    "                           'comp1_life', 'comp2_life', 'comp3_life', 'comp4_life']\n",
    "        \n",
    "        cols = ['machineID', 'voltmean_3h', 'rotatemean_3h',\n",
    "                'pressuremean_3h', 'vibrationmean_3h', 'voltsd_3h', 'rotatesd_3h',\n",
    "                'pressuresd_3h', 'vibrationsd_3h', 'voltmean_24h', 'rotatemean_24h',\n",
    "                'pressuremean_24h', 'vibrationmean_24h', 'voltsd_24h', 'rotatesd_24h',\n",
    "                'pressuresd_24h', 'vibrationsd_24h', 'error1count', 'error2count',\n",
    "                'error3count', 'error4count', 'error5count','age',\n",
    "                'model_model1', 'model_model2', 'model_model3', 'model_model4'] + [word for word in components_cols if str(self.component) in ([*word]) and 'failure' not in word.split('_')]\n",
    "\n",
    "\n",
    "        return self.data[cols]\n",
    "    \n",
    "    def Preprocess(self, X_init: pd.DataFrame, model: str) -> \\\n",
    "        Tuple[np.ndarray, np.ndarray,  torch.tensor,  torch.tensor, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.float64, np.float64]:\n",
    "\n",
    "        '''\n",
    "        the necessary data preprocess before procceding in pyro inference\n",
    "        '''\n",
    "        print(f\"{model} Regression {self.component}\")\n",
    "        print(X_init.columns)\n",
    "        print('\\n-----------------------------------------------------------------------')\n",
    "\n",
    "        y, X, X_train_torch, y_train_torch, X_test_torch, X_test, y_test, X_train, y_train, y_std, y_mean = preprocess(X_init,0.1,model,classi=False,splitting=True)\n",
    " \n",
    "        return y, X, X_train_torch, y_train_torch, X_test_torch, X_test, y_test, X_train, y_train, y_std, y_mean\n",
    "    \n",
    "\n",
    "    def pyro_inference(self, X_train_torch, y_train_torch, model, num_samples):\n",
    "\n",
    "        # Reset parameter values\n",
    "        pyro.clear_param_store()\n",
    "\n",
    "        # Run inference in Pyro\n",
    "        nuts_kernel = NUTS(model)\n",
    "        \n",
    "        mcmc = MCMC(nuts_kernel, num_samples=num_samples, warmup_steps=200, num_chains=1)\n",
    "        mcmc.run(X_train_torch, y_train_torch)\n",
    "        \n",
    "        # Show summary of inference results\n",
    "        print(mcmc.summary())\n",
    "\n",
    "        return mcmc.get_samples()\n",
    "\n",
    "    def post_process(self, X_test, X_train, samples, y_std, y_mean, y_test):\n",
    "\n",
    "        posterior_samples = samples\n",
    "\n",
    "        # Compute predictions\n",
    "        y_hat = np.mean(posterior_samples[\"alpha\"].numpy().T + np.dot(X_test, posterior_samples[\"beta\"].numpy().T), axis=1)\n",
    "        y_hat_train = np.mean(posterior_samples[\"alpha\"].numpy().T + np.dot(X_train, posterior_samples[\"beta\"].numpy().T), axis=1)\n",
    "\n",
    "        # Convert back to the original scale\n",
    "        preds = y_hat * y_std + y_mean\n",
    "        preds_train = y_hat_train * y_std + y_mean\n",
    "        y_true = y_test * y_std + y_mean\n",
    "\n",
    "        return preds, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5e50bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_mcmc_results = {'linear_mcmc': {'comp_1': 0, 'comp_2':0, 'comp_3': 0, 'comp_4':0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194898c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for comp_number in range(1,5):\n",
    "    print(f\"\\n--------------------------------  component {comp_number}  ---------------------------------------\")\n",
    "    mcmc_regression = MCMC_regression_model(labeled_features, comp_number)\n",
    "    mcmc_dataset = mcmc_regression.get_data_for_component()\n",
    "    linear_mcmc = MCMC_model_format.linear\n",
    " \n",
    "\n",
    "    y_linear_mcmc, X_linear_mcmc, X_train_torch_linear_mcmc, y_train_torch_linear_mcmc,  X_test_torch_linear_mcmc, X_test_linear_mcmc, y_test_linear_mcmc, X_train_linear_mcmc, y_train_linear_mcmc, y_std_linear_mcmc, y_mean_linear_mcmc = mcmc_regression.Preprocess(X_init=mcmc_dataset, model=linear_mcmc['name'])\n",
    "  \n",
    "    linear_mcmc_samples = mcmc_regression.pyro_inference(X_train_torch=X_train_torch_linear_mcmc, y_train_torch=y_train_torch_linear_mcmc, model=linear_mcmc['model'], num_samples=100)\n",
    "    linear_mcmc_preds, linear_mcmc_y_true = mcmc_regression.post_process(samples=linear_mcmc_samples, X_train = X_train_linear_mcmc, X_test = X_test_linear_mcmc, y_test=y_test_linear_mcmc, y_std=y_std_linear_mcmc, y_mean=y_mean_linear_mcmc)\n",
    "    \n",
    "    corr_linear_mcmc, mae_linear_mcmc, rae_linear_mcmc, rmse_linear_mcmc, r2_linear_mcmc, mcmc_trues_linear_mcmc, mcmc_pred_linear_mcmc = compute_error(trues=linear_mcmc_y_true, predicted=linear_mcmc_preds, threshold=None)\n",
    "    linear_mcmc_results['linear_mcmc'][f\"comp_{comp_number}\"] = [mae_linear_mcmc]\n",
    "    print(\"CorrCoef: %.3f\\nMAE: %.3f\\nRMSE: %.3f\\nR2: %.3f\" % (corr_linear_mcmc, mae_linear_mcmc, rmse_linear_mcmc, r2_linear_mcmc))\n",
    "    print('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49f42cfe",
   "metadata": {},
   "source": [
    "### NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b497cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelFormat():\n",
    "    '''\n",
    "    defining the type of the regression\n",
    "    '''\n",
    "    neural_netwrok = {'name':'NN'}\n",
    "    poisson = {'name':\"POISSON\", 'model':poisson_model}\n",
    "    heterosc = {'name':\"HETEROSCEDASTIC\", 'model':heteroscedastic_model}\n",
    "    linear = {'name':\"LINEAR\", 'model':linear_model}\n",
    "    classification_model = {'name':\"CLASSIFICATION\"}\n",
    "\n",
    "class FFNN(PyroModule):\n",
    "    \n",
    "    '''\n",
    "    FNN implementation\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_in, n_hidden, n_out, type_forward, feature):\n",
    "        \n",
    "        self.type_forward = type_forward\n",
    "        self.feature = feature\n",
    "        super(FFNN, self).__init__()\n",
    "        \n",
    "        # Architecture\n",
    "        self.in_layer = PyroModule[nn.Linear](n_in, n_hidden)\n",
    "        self.in_layer.weight = PyroSample(dist.Normal(0., 1.).expand([n_hidden, n_in]).to_event(2))\n",
    "\n",
    "        self.h_layer = PyroModule[nn.Linear](n_hidden, n_hidden)\n",
    "        self.h_layer.weight = PyroSample(dist.Normal(0., 1.).expand([n_hidden, n_hidden]).to_event(2))\n",
    "\n",
    "        self.h_layer = PyroModule[nn.Linear](n_hidden, n_hidden)\n",
    "        self.h_layer.weight = PyroSample(dist.Normal(0., 1.).expand([n_hidden, n_hidden]).to_event(2))\n",
    "\n",
    "        self.out_layer = PyroModule[nn.Linear](n_hidden, n_out)\n",
    "        self.out_layer.weight = PyroSample(dist.Normal(0., 1.).expand([n_out, n_hidden]).to_event(2))\n",
    "\n",
    "        # Activation functions\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, X, y=None):\n",
    "        \n",
    "        if self.type_forward == 'simple':\n",
    "            X = self.tanh(self.in_layer(X))\n",
    "            X = self.tanh(self.h_layer(X))\n",
    "            X = self.out_layer(X)\n",
    "            prediction_mean = X.squeeze(-1)\n",
    "            with pyro.plate(\"observations\"):\n",
    "                y = pyro.sample(\"obs\", dist.Normal(prediction_mean, 0.1), obs=y)\n",
    "        \n",
    "        elif self.type_forward == 'interpretable':\n",
    "            X_nn = X[:,1:]\n",
    "            X_nn = self.tanh(self.in_layer(X_nn))\n",
    "            X_nn = self.tanh(self.h_layer(X_nn))\n",
    "            X_nn = self.out_layer(X_nn)\n",
    "            nn_out = X_nn.squeeze(-1)\n",
    "\n",
    "            beta_lin = pyro.sample(\"beta\", dist.Normal(0, 1))\n",
    "            X_linear = X[:,0]\n",
    "            with pyro.plate(\"observations\"):\n",
    "                linear_out = X_linear*beta_lin\n",
    "                y = pyro.sample(\"obs\", dist.Normal(nn_out+linear_out, 0.1), obs=y)\n",
    "\n",
    "        return y\n",
    "\n",
    "def train_nn(model0, X_train_torch, y_train_torch, feature):\n",
    "    \n",
    "    if model0 ==\"simple\":\n",
    "        model = FFNN(n_in=X_train_torch.shape[1],feature=feature, n_hidden=32, n_out=1, type_forward=model0)\n",
    "        \n",
    "    elif model0 == \"interpretable\":\n",
    "        model = FFNN(n_in=X_train_torch.shape[1]-1,feature=feature, n_hidden=32, n_out=1, type_forward=model0)\n",
    "        \n",
    "        \n",
    "    guide = AutoDiagonalNormal(model)\n",
    "    pyro.clear_param_store()\n",
    "    # Define the number of optimization steps\n",
    "    n_steps = 1000\n",
    "\n",
    "    # Setup the optimizer\n",
    "    adam_params = {\"lr\": 0.01}\n",
    "    optimizer = Adam(adam_params)\n",
    "\n",
    "    # Setup the inference algorithm\n",
    "    elbo = Trace_ELBO(num_particles=1)\n",
    "    svi = SVI(model, guide, optimizer, loss=elbo)\n",
    "\n",
    "    # Do gradient steps\n",
    "    for step in range(n_steps):\n",
    "        elbo = svi.step(X_train_torch,y_train_torch)\n",
    "        if step % 100 == 0:\n",
    "            print(\"[%d] ELBO: %.1f\" % (step, elbo))\n",
    "    \n",
    "    return model, guide\n",
    "\n",
    "def get_data_for_component(data, component) -> pd.DataFrame:\n",
    "        \n",
    "    '''\n",
    "    returns the feautures of the dataset and the component we want to predict for\n",
    "    '''\n",
    "    \n",
    "    components_cols = ['comp1_maint', 'comp2_maint', 'comp3_maint', 'comp4_maint',\n",
    "                       'failure_comp1', 'failure_comp2', 'failure_comp3', 'failure_comp4',\n",
    "                       'comp1_life', 'comp2_life', 'comp3_life', 'comp4_life']\n",
    "        \n",
    "    cols = ['machineID', 'voltmean_3h', 'rotatemean_3h', 'pressuremean_3h', 'vibrationmean_3h',\n",
    "            'voltsd_3h', 'rotatesd_3h', 'pressuresd_3h', 'vibrationsd_3h', 'voltmean_24h',\n",
    "            'rotatemean_24h', 'pressuremean_24h', 'vibrationmean_24h', 'voltsd_24h', 'rotatesd_24h',\n",
    "            'pressuresd_24h', 'vibrationsd_24h', 'error1count', 'error2count', 'error3count',\n",
    "            'error4count', 'error5count','age', 'model_model1', 'model_model2',\n",
    "            'model_model3', 'model_model4'] + [word for word in components_cols if str(component) in ([*word]) and 'failure' not in word.split('_')]\n",
    "\n",
    "\n",
    "    return data[cols]\n",
    "\n",
    "def test_nn(model,guide,X_test_torch):\n",
    "    \n",
    "    # Predict\n",
    "    predictive = pyro.infer.Predictive(model, guide=guide, num_samples=1000,return_sites=(\"obs\", \"_RETURN\"))\n",
    "    samples = predictive(X_test_torch)\n",
    "    y_pred = samples[\"obs\"].mean(axis=0).detach().numpy()\n",
    "    \n",
    "    y_preds = y_pred * y_std + y_mean\n",
    "    y_true = y_test * y_std + y_mean\n",
    "    \n",
    "    return y_preds, y_true\n",
    "\n",
    "def test_nn_beta(model,guide,X_test_torch):\n",
    "    # Predict\n",
    "    predictive = pyro.infer.Predictive(model, guide=guide, num_samples=1000,return_sites=(\"beta\",))\n",
    "    samples = predictive(X_test_torch)\n",
    "    print(\"Estimated beta:\", samples[\"beta\"].mean(axis=0).detach().numpy())\n",
    "    \n",
    "    y_pred = samples[\"beta\"].mean(axis=0).detach().numpy()\n",
    "    \n",
    "    y_preds = y_pred * y_std + y_mean\n",
    "    y_true = y_test * y_std + y_mean\n",
    "    \n",
    "    return y_preds, y_true\n",
    "\n",
    "\n",
    "def mae_test(y_pred, y_test):\n",
    "\n",
    "    mae = np.mean(np.abs(y_pred - y_test))\n",
    "    print(\"MAE:\", mae)\n",
    "    \n",
    "    return mae\n",
    "\n",
    "def plot_pred(y_pred,y_test,y_std,y_mean,threshold,start=None,end=None):\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "    start = start\n",
    "    end = end\n",
    "    y_true = y_test * y_std + y_mean\n",
    "    y_pre = y_pred * y_std + y_mean\n",
    "\n",
    "    plt.plot(y_true[y_pre>threshold], 'r.-', label='test')\n",
    "    plt.plot(y_pre[y_pre>threshold], 'b-', label='pred')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return y_true,y_pre"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b051e7ed",
   "metadata": {},
   "source": [
    "#### Features importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baec336",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_for_comp = {'comp_1':  {'Feature':0}, 'comp_2':  {'Feature':0}, 'comp_3':  {'Feature':0}, 'comp_4':  {'Feature':0}}\n",
    "\n",
    "for i in range(1,3):\n",
    "    feature = poisson_results['poisson'][f\"comp_{i}\"][\"FI\"]\n",
    "    feature_for_comp[f\"comp_{i}\"][\"Feature\"] = svi_dataset.columns.get_loc(feature)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d24062f0",
   "metadata": {},
   "source": [
    "#### FNN Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9b8294",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,5):\n",
    "    print(f\"\\n--------------------------------  component {i}  ---------------------------------------\")\n",
    "    nn_dataset = get_data_for_component(labeled_features, i)\n",
    "    y, X, X_train_torch, y_train_torch,X_test_torch, X_test, y_test, X_train, y_train, y_std, y_mean = preprocess(nn_dataset, 0.1, 'neural_network')\n",
    "    \n",
    "    model_FFNN, guide = train_nn(\"simple\", X_train_torch, y_train_torch, -1)\n",
    "    y_preds, y_true = test_nn(model_FFNN, guide, X_test_torch)\n",
    "    mae = mae_test(y_preds, y_true)\n",
    "    print('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccd3a561",
   "metadata": {},
   "source": [
    "#### FNN Interpretable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92b1122",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,5):\n",
    "    print(f\"\\n--------------------------------  component {i}  ---------------------------------------\")\n",
    "    nn_dataset = get_data_for_component(labeled_features, i)\n",
    "    y, X, X_train_torch, y_train_torch,X_test_torch, X_test, y_test, X_train, y_train, y_std, y_mean = preprocess(nn_dataset, 0.1, 'neural_network')\n",
    "    \n",
    "    model_FFNN, guide = train_nn(\"interpretable\", X_train_torch, y_train_torch, feature_for_comp[f\"comp_{i}\"]['Feature']-1)\n",
    "    y_preds, y_true = test_nn_beta(model_FFNN, guide, X_test_torch)\n",
    "    mae = mae_test(y_preds, y_true)\n",
    "    print('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae62b976",
   "metadata": {},
   "source": [
    "### Data Threshold"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "381458d7",
   "metadata": {},
   "source": [
    "```\n",
    "Considering that trying to predict the fail of a component among others on 150 or 300 days period is very difficult and not realistic, at the end it doesn't make sense. therefore, we introduce a more realistic and clear threshold of 30 days, where we train the best of our models per component in a more limited dataset regarding the days until the fail of the component.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e199752",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(labeled_features[[\"comp1_life\",\"comp2_life\",\"comp3_life\",\"comp4_life\"]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ff9ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_for_comp = {'comp_1':  {'Feature':0}, 'comp_2':  {'Feature':0}, 'comp_3':  {'Feature':0}, 'comp_4':  {'Feature':0}}\n",
    "for i in range(1,5):\n",
    "    feature = poisson_results['poisson'][f\"comp_{i}\"]['FI']\n",
    "    feature_for_comp[f\"comp_{i}\"]['Feature'] = svi_dataset.columns.get_loc(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd277583",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_for_comp\n",
    "# feature_for_comp = {'comp_1': {'Feature': 24},'comp_2': {'Feature': 9},'comp_3': {'Feature': 25},'comp_4': {'Feature': 26}}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "805ff2e2",
   "metadata": {},
   "source": [
    "For components 1 and 4 the simple FNN model has been qualified as the best model to be trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24024316",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "threshold = 30\n",
    "print(labeled_features.shape)\n",
    "for i in [1,4]:\n",
    "    print(f\"\\n--------------------------------  component {i}  ---------------------------------------\")\n",
    "    data = labeled_features[labeled_features[f\"comp{i}_life\"]<=threshold]\n",
    "    nn_dataset = get_data_for_component(data, i)\n",
    "    print(nn_dataset.shape)\n",
    "    print(nn_dataset[f\"comp{i}_life\"].max())\n",
    "    y, X, X_train_torch, y_train_torch,X_test_torch, X_test, y_test, X_train, y_train, y_std, y_mean = preprocess(nn_dataset, 0.1, 'neural_network')\n",
    "    \n",
    "    model_FFNN, guide = train_nn(\"simple\", X_train_torch, y_train_torch, -1)\n",
    "    y_preds, y_true = test_nn(model_FFNN, guide, X_test_torch)\n",
    "    mae = mae_test(y_preds, y_true)\n",
    "    print('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c8c0427",
   "metadata": {},
   "source": [
    "For components 2 and 3 the interpretable FNN model has been qualified as the best model to be trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba75cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_for_comp = {'comp_1': {'Feature': 24},'comp_2': {'Feature': 9},'comp_3': {'Feature': 25},'comp_4': {'Feature': 26}}\n",
    "threshold = 30\n",
    "print(labeled_features.shape)\n",
    "for i in [2,3]:\n",
    "    print(f\"\\n--------------------------------  component {i}  ---------------------------------------\")\n",
    "    data = labeled_features[labeled_features[f\"comp{i}_life\"]<=threshold]\n",
    "    nn_dataset = get_data_for_component(data, i)\n",
    "    print(nn_dataset.shape)\n",
    "    print(nn_dataset[f\"comp{i}_life\"].max())\n",
    "    y, X, X_train_torch, y_train_torch,X_test_torch, X_test, y_test, X_train, y_train, y_std, y_mean = preprocess(nn_dataset, 0.1, 'neural_network')\n",
    "    \n",
    "    model_FFNN, guide = train_nn(\"interpretable\", X_train_torch, y_train_torch, feature_for_comp[f\"comp_{i}\"]['Feature']-1)\n",
    "    y_preds, y_true = test_nn_beta(model_FFNN, guide, X_test_torch)\n",
    "    mae = mae_test(y_preds, y_true)\n",
    "    print('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abd338d4",
   "metadata": {},
   "source": [
    "## Binary classification\n",
    "The following part includes the classification models (categorical, binary logistic regression, probit regression and NN) and techniques of SVI and MCMC. Due to computational limitations we proceeded mainly with the SVI method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6d136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cat(X, obs=None):\n",
    "    n_cat=2\n",
    "    input_dim = X.shape[1]\n",
    "    alpha = pyro.sample(\"alpha\", dist.Normal(torch.zeros(1, n_cat), \n",
    "                                             torch.ones(1, n_cat)).to_event())  # Prior for the bias/intercept\n",
    "    beta  = pyro.sample(\"beta\", dist.Normal(torch.zeros(input_dim, n_cat), \n",
    "                                            torch.ones(input_dim, n_cat)).to_event()) # Priors for the regression coeffcients\n",
    "    with pyro.plate(\"data\"):\n",
    "        y = pyro.sample(\"y\", dist.Categorical(logits=alpha + X.matmul(beta)), obs=obs)\n",
    "    return y\n",
    "\n",
    "\n",
    "def model_bin_lr(X, obs=None):\n",
    "    alpha = pyro.sample(\"alpha\", dist.Normal(0, 1)) # Prior for the bias/intercept\n",
    "    beta  = pyro.sample(\"beta\", dist.Normal(torch.zeros(X.shape[1]), \n",
    "                                            torch.ones(X.shape[1])).to_event()) # Priors for the regression coeffcients\n",
    "    with pyro.plate(\"data\"):\n",
    "        logits = alpha + X.matmul(beta)\n",
    "        y = pyro.sample(\"y\", dist.Bernoulli(logits=logits), obs=obs)\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "def model_cdf(X, obs=None):\n",
    "    std_normal = torch.distributions.Normal(0,1)\n",
    "    alpha = pyro.sample(\"alpha\", dist.Normal(0, 1)) # Prior for the bias/intercept\n",
    "    beta  = pyro.sample(\"beta\", dist.Normal(torch.zeros(X.shape[1]), \n",
    "                                            torch.ones(X.shape[1])).to_event()) # Priors for the regression coeffcients\n",
    "    with pyro.plate(\"data\"):\n",
    "        probs = std_normal.cdf(alpha + X.matmul(beta))\n",
    "        y = pyro.sample(\"y\", dist.Bernoulli(probs=probs), obs=obs)\n",
    "    return y\n",
    "\n",
    "   \n",
    "class FFNN_c(PyroModule):\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        super(FFNN_c, self).__init__()\n",
    "        \n",
    "        # Architecture\n",
    "        self.in_layer = PyroModule[nn.Linear](n_in, n_hidden*2)\n",
    "        self.in_layer.weight = PyroSample(dist.Normal(0., 1.).expand([n_hidden*2, n_in]).to_event(2))\n",
    "\n",
    "        self.h_layer = PyroModule[nn.Linear](n_hidden*2, n_hidden)\n",
    "        self.h_layer.weight = PyroSample(dist.Normal(0., 1.).expand([n_hidden, n_hidden*2]).to_event(2))\n",
    "\n",
    "        self.h_layer1 = PyroModule[nn.Linear](n_hidden, n_hidden)\n",
    "        self.h_layer1.weight = PyroSample(dist.Normal(0., 1.).expand([n_hidden, n_hidden]).to_event(2))\n",
    "\n",
    "        self.out_layer = PyroModule[nn.Linear](n_hidden, n_out)\n",
    "        self.out_layer.weight = PyroSample(dist.Normal(0., 1.).expand([n_out, n_hidden]).to_event(2))\n",
    "\n",
    "        # Activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, X, y=None):\n",
    "        X = self.relu(self.in_layer(X))\n",
    "        X = self.relu(self.h_layer(X))\n",
    "        X = self.relu(self.h_layer1(X))\n",
    "        X = self.out_layer(X)\n",
    "        prediction_mean = torch.sigmoid(X).squeeze(-1)\n",
    "        #not enough memory for this\n",
    "        \"\"\"\n",
    "        with pyro.plate(\"data\", X.shape[0]):\n",
    "            y = pyro.sample(\"obs\", dist.Categorical(logits=prediction_mean), obs=y)    \n",
    "        \n",
    "        with pyro.plate(\"observations\"):\n",
    "            y = pyro.sample(\"obs\", dist.Normal(prediction_mean, 0.1), obs=y)  \n",
    "        \"\"\"\n",
    "        with pyro.plate(\"observations\"):\n",
    "            y = pyro.sample(\"obs\", dist.Bernoulli(probs=prediction_mean), obs=y)\n",
    "        return y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e04fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_component_class(data, component):\n",
    "    components_cols = ['comp1_maint', 'comp2_maint', 'comp3_maint', 'comp4_maint',\n",
    "       'failure_comp1', 'failure_comp2', 'failure_comp3', 'failure_comp4',\n",
    "       'comp1_life', 'comp2_life', 'comp3_life', 'comp4_life']\n",
    "    \n",
    "    cols = ['machineID', 'voltmean_3h', 'rotatemean_3h',\n",
    "                'pressuremean_3h', 'vibrationmean_3h', 'voltsd_3h', 'rotatesd_3h',\n",
    "                'pressuresd_3h', 'vibrationsd_3h', 'voltmean_24h', 'rotatemean_24h',\n",
    "                'pressuremean_24h', 'vibrationmean_24h', 'voltsd_24h', 'rotatesd_24h',\n",
    "                'pressuresd_24h', 'vibrationsd_24h', 'error1count', 'error2count',\n",
    "                'error3count', 'error4count', 'error5count','age',\n",
    "                'model_model1', 'model_model2', 'model_model3', 'model_model4'] + [word for word in components_cols if str(component) in ([*word]) and 'life' not in word.split('_')]\n",
    "\n",
    "    return data[cols]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79a3137e",
   "metadata": {},
   "source": [
    "Setting the training of the SVI and MCMC methods stands for inference to estimate the parameters and posterior distributions of the aforementioned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc2cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define guide function\n",
    "from pyro.contrib.autoguide import AutoMultivariateNormal\n",
    "from src.models.models import FFNN_c\n",
    "\n",
    "def train_nn(model0, X_train_torch, y_train_torch):\n",
    "    if model0 == \"modelFFNN_c\":\n",
    "        model = FFNN_c(n_in=X_train_torch.shape[1], n_hidden=32, n_out=1)\n",
    "    guide = AutoDiagonalNormal(model)\n",
    "    pyro.clear_param_store()\n",
    "    # Define the number of optimization steps\n",
    "    n_steps = 2000\n",
    "\n",
    "    # Setup the optimizer\n",
    "    adam_params = {\"lr\": 0.01}\n",
    "    optimizer = Adam(adam_params)\n",
    "\n",
    "    # Setup the inference algorithm\n",
    "    elbo = Trace_ELBO(num_particles=1)\n",
    "    svi = SVI(model, guide, optimizer, loss=elbo)\n",
    "\n",
    "    # Do gradient steps\n",
    "    for step in range(n_steps):\n",
    "        elbo = svi.step(X_train_torch,y_train_torch)\n",
    "        if step % 500 == 0:\n",
    "            print(\"[%d] ELBO: %.1f\" % (step, elbo))\n",
    "    return model,guide\n",
    "\n",
    "\n",
    "\n",
    "#model using SVI\n",
    "def train_c_svi(model, X_train, y_train, steps, lrate):\n",
    "    # Define guide function\n",
    "    guide = AutoMultivariateNormal(model)\n",
    "    \n",
    "    # Reset parameter values\n",
    "    pyro.clear_param_store()\n",
    "\n",
    "    # Define the number of optimization steps\n",
    "    n_steps = steps\n",
    "\n",
    "    # Set parameters of the optimizer\n",
    "    adam_params = {\"lr\": lrate}\n",
    "    optimizer = ClippedAdam(adam_params)\n",
    "\n",
    "    # Setup the inference algorithm\n",
    "    elbo = Trace_ELBO(num_particles=1)\n",
    "    svi = SVI(model, guide, optimizer, loss=elbo)\n",
    "\n",
    "    # Do gradient steps\n",
    "    for step in range(n_steps):\n",
    "        elbo = svi.step(X_train, y_train)\n",
    "        if step % 1000 == 0:\n",
    "            print(\"[%d] ELBO: %.1f\" % (step, elbo))\n",
    "    #use the Predictive class to extract samples from posterior:\n",
    "    predictive = Predictive(model, guide=guide, num_samples=2000,return_sites=(\"alpha\", \"beta\"))\n",
    "    samples = predictive(X_train, y_train)\n",
    "    #extract the inferred posteriors to make predictions for the testset\n",
    "    alpha_hat = samples[\"alpha\"].detach().squeeze().mean(axis=0).numpy()\n",
    "    beta_hat = samples[\"beta\"].detach().squeeze().mean(axis=0).numpy()\n",
    "    return model,guide, alpha_hat, beta_hat\n",
    "\n",
    "\n",
    "#model using MCMC\n",
    "def train_c_mcmc(model, X_train, y_train, num_samples,w_steps,chains):\n",
    "    # Initialize NUTS kernel for the MCMC sampler\n",
    "    nuts_kernel = NUTS(model)\n",
    "    # create MCMC inference object with the needed paramters\n",
    "    mcmc = MCMC(nuts_kernel, num_samples=num_samples, warmup_steps=w_steps, num_chains=chains)\n",
    "    # run the inference\n",
    "    mcmc.run(X_train, y_train)\n",
    "    # extract the inferred posteriors to make predictions for the testset\n",
    "    samples = mcmc.get_samples()\n",
    "    alpha_hat = samples[\"alpha\"].detach().squeeze().mean(axis=0).numpy()\n",
    "    beta_hat = samples[\"beta\"].detach().squeeze().mean(axis=0).numpy()\n",
    "    return mcmc, alpha_hat, beta_hat\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d5c5286",
   "metadata": {},
   "source": [
    "Setting the test and the evaluation of the SVI and MCMC methods stands for inference of the estimated the parameters and posterior distributions of the aforementioned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e95e3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, recall_score, accuracy_score, precision_score\n",
    "\n",
    "def test_nn_c(model,guide,X_test_torch,thres):\n",
    "    # make predictions with threshold for test set using the trained model\n",
    "    predictive = pyro.infer.Predictive(model, guide=guide, num_samples=2000,return_sites=(\"obs\", \"_RETURN\"))\n",
    "    samples = predictive(X_test_torch)\n",
    "    y_pred = samples[\"obs\"].mean(axis=0).detach().numpy()\n",
    "    #threshold predictions\n",
    "    y_pred[y_pred<=thres]=0\n",
    "    y_pred[y_pred>thres]=1\n",
    "    return y_pred\n",
    "\n",
    "def test_model_c(alpha_hat,beta_hat,X_test_torch,thres=None):\n",
    "    # make predictions for test set either using threshold or argmax\n",
    "    y_hat = alpha_hat + np.dot(X_test_torch, beta_hat)\n",
    "    if thres:\n",
    "        thres=0.01\n",
    "        y_hat[y_hat<=thres]=0\n",
    "        y_hat[y_hat>thres]=1\n",
    "    else: \n",
    "        y_hat = np.argmax(y_hat, axis=1)\n",
    "    return y_hat\n",
    "\n",
    "# function to evaluate the accuracy of the model\n",
    "def test_c(y_hat,y_test):\n",
    "    # evaluate prediction accuracy\n",
    "    print(\"Accuracy:\", 1.0*np.sum(y_hat == y_test) / len(y_test))\n",
    "\n",
    "# function to evaluate predictions\n",
    "def evaluate(y_test, y_hat):\n",
    "    # calculate and display confusion matrix\n",
    "    labels = np.unique(y_test)\n",
    "    cm = confusion_matrix(y_test, y_hat, labels=labels)\n",
    "    print('Confusion matrix\\n- x-axis is true labels (no failure, failure)\\n- y-axis is predicted labels')\n",
    "    print(cm)\n",
    "    # calculate precision, recall, and F1 score\n",
    "    accuracy = float(np.trace(cm)) / np.sum(cm)\n",
    "    precision = precision_score(y_test, y_hat, average=None, labels=labels)[1]\n",
    "    recall = recall_score(y_test, y_hat, average=None, labels=labels)[1]\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    print(\"accuracy:\", accuracy)\n",
    "    print(\"precision:\", precision)\n",
    "    print(\"recall:\", recall)\n",
    "    print(\"f1 score:\", f1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c06eb3d",
   "metadata": {},
   "source": [
    "Categorical Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5da457",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,5):\n",
    "    print(f\"\\n--------------------------------  component {i}  ---------------------------------------\")\n",
    "    cat_dataset = get_data_for_component_class(labeled_features, i)\n",
    "    y, X, X_train_torch, y_train_torch,X_test_torch, X_test, y_test, X_train, y_train, y_std, y_mean = preprocess(cat_dataset, 0.1, 'classification_model',classi=True,splitting=True)\n",
    "    model__cat,guide_cat, alpha_hat_cat, beta_hat_cat=train_c_svi(model_cat, X_train_torch, y_train_torch,steps=10000,lrate=0.001)\n",
    "    y_hat=test_model_c(alpha_hat_cat, beta_hat_cat,X_test_torch)\n",
    "    evaluate(y_hat,y_test)\n",
    "    print('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6212077a",
   "metadata": {},
   "source": [
    "Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1390ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,5):\n",
    "    print(f\"\\n--------------------------------  component {i}  ---------------------------------------\")\n",
    "    cat_dataset = get_data_for_component_class(labeled_features, i)\n",
    "    y, X, X_train_torch, y_train_torch,X_test_torch, X_test, y_test, X_train, y_train, y_std, y_mean = preprocess(cat_dataset, 0.1, 'classification_model',classi=True,splitting=True)\n",
    "    model__bin,guide_bin, alpha_hat_bin, beta_hat_bin=train_c_svi(model_bin_lr, X_train_torch, y_train_torch,steps=30000,lrate=0.001)\n",
    "    y_hat=test_model_c(alpha_hat_bin, beta_hat_bin,X_test_torch,thres=0.5)\n",
    "    evaluate(y_hat,y_test)\n",
    "    print('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "daea3f16",
   "metadata": {},
   "source": [
    "Probit Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620d8d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,5):\n",
    "    print(f\"\\n--------------------------------  component {i}  ---------------------------------------\")\n",
    "    cat_dataset = get_data_for_component_class(labeled_features, i)\n",
    "    y, X, X_train_torch, y_train_torch,X_test_torch, X_test, y_test, X_train, y_train, y_std, y_mean = preprocess(cat_dataset, 0.1, 'classification_model',classi=True,splitting=True)\n",
    "    model__cdf,guide_cdf, alpha_hat_cdf, beta_hat_cdf=train_c_svi(model_cdf, X_train_torch, y_train_torch.squeeze(),steps=20000,lrate=0.0005)\n",
    "    y_hat=test_model_c(alpha_hat_cdf, beta_hat_cdf,X_test_torch,thres=0.5)\n",
    "    evaluate(y_hat,y_test)\n",
    "    print('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13fee43f",
   "metadata": {},
   "source": [
    "Neural Network Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deef541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,5):\n",
    "    print(f\"\\n--------------------------------  component {i}  ---------------------------------------\")\n",
    "    cat_dataset = get_data_for_component_class(labeled_features, i)\n",
    "    y, X, X_train_torch, y_train_torch,X_test_torch, X_test, y_test, X_train, y_train, y_std, y_mean = preprocess(cat_dataset, 0.1, 'classification_model',classi=True,splitting=True)\n",
    "    modelFFNN_c,guide=train_nn(\"modelFFNN_c\", X_train_torch, y_train_torch)\n",
    "    y_hat=test_nn_c(modelFFNN_c,guide,X_test_torch,thres=0.5)\n",
    "    evaluate(y_hat,y_test)\n",
    "    print('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
