{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7279b216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "import torch\n",
    "import pyro.distributions as dist\n",
    "import torch.nn as nn\n",
    "\n",
    "from pyro.infer import SVI, Trace_ELBO, MCMC, NUTS\n",
    "from pyro.optim import ClippedAdam\n",
    "from pyro.contrib.autoguide import AutoDiagonalNormal\n",
    "from pyro.infer import Predictive\n",
    "from typing import Dict, Tuple, List\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas import DataFrame\n",
    "from tqdm.autonotebook import tqdm\n",
    "from pyro.nn import PyroModule, PyroSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c536413",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/raw/raw2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5699889",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44f9301",
   "metadata": {},
   "source": [
    "Read the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caf57d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "failures_path = os.path.join(data_path, 'PdM_failures.csv')\n",
    "errors_path = os.path.join(data_path, 'PdM_errors.csv')\n",
    "machines_path = os.path.join(data_path, 'PdM_machines.csv')\n",
    "maint_path = os.path.join(data_path, 'PdM_maint.csv')\n",
    "telemetry_path = os.path.join(data_path, 'PdM_telemetry.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1124d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "failures_df = pd.read_csv(failures_path)\n",
    "errors_df = pd.read_csv(errors_path)\n",
    "machines_df = pd.read_csv(machines_path)\n",
    "maint_df = pd.read_csv(maint_path)\n",
    "telemetry_df = pd.read_csv(telemetry_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01458f9",
   "metadata": {},
   "source": [
    "Transform `datetime` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f197c9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "maint_df['datetime'] = pd.to_datetime(maint_df['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "failures_df['datetime'] = pd.to_datetime(failures_df['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "errors_df['datetime'] = pd.to_datetime(errors_df['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "telemetry_df['datetime'] = pd.to_datetime(telemetry_df['datetime'], format=\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588ef274",
   "metadata": {},
   "source": [
    "#### Dataset transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843f0a23",
   "metadata": {},
   "source": [
    "Maintenance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af80ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function that takes a column and returns its mean and std of a moving window of 3 hours\n",
    "def rolling_mean_std(col,window):\n",
    "    return col.rolling(window).agg(['mean', 'std'])\n",
    "\n",
    "#apply the function to the telemetry data\n",
    "def telem_(telemetry,column,window):\n",
    "    telemetry[[column+'mean_'+str(window)+'h', column+'sd_'+str(window)+'h']] = telemetry.groupby('machineID')[column].apply(rolling_mean_std,window)\n",
    "    return telemetry\n",
    "\n",
    "def lifespan(replacement_event_df: DataFrame)->DataFrame:\n",
    "    \n",
    "    '''\n",
    "    Receives a dataframe with timestamp and columns that signify when a component is replaced, with 1.\n",
    "    Returns a dataframe with the days since the last replacement for the component\n",
    "    '''\n",
    "    \n",
    "    comp_rep=replacement_event_df.copy()\n",
    "    points = comp_rep['machineID'].unique()\n",
    "    final=pd.DataFrame()\n",
    "    \n",
    "    for i in tqdm(points, desc='Machine'):\n",
    "        df = comp_rep[(comp_rep['machineID']==i)][['datetime','machineID','comp_comp1', 'comp_comp2', 'comp_comp3', 'comp_comp4']]\n",
    "        for comp in ['comp_comp1', 'comp_comp2', 'comp_comp3', 'comp_comp4']:\n",
    "            \n",
    "            # keep the last part of component name\n",
    "            life = comp[5:]\n",
    "            \n",
    "            #apply function in each row of df[life] column where if row[comp]==1 then value=0.041667 else 0\n",
    "            df[life+'_maint'] = df.apply(lambda row: 0 if row[comp]==1 else 0.041667, axis=1)\n",
    "\n",
    "            df_maint = df[life+'_maint'] != 0\n",
    "            df[life+'_maint'] = df_maint.cumsum()-df_maint.cumsum().where(~df_maint).ffill().fillna(0).astype(int)\n",
    "            df[life+'_maint'] = df[life+'_maint'].apply(lambda x: x*0.041667)\n",
    "            \n",
    "            \n",
    "        final=pd.concat([final,df],axis=0)\n",
    "        final=final[['datetime', 'machineID', 'comp1_maint', 'comp2_maint', 'comp3_maint', 'comp4_maint']]\n",
    "        \n",
    "    return final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36129b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "telemetry_df = telem_(telemetry_df,'volt',3)\n",
    "telemetry_df = telem_(telemetry_df,'rotate',3)\n",
    "telemetry_df = telem_(telemetry_df,'pressure',3)\n",
    "telemetry_df = telem_(telemetry_df,'vibration',3)\n",
    "telemetry_df = telem_(telemetry_df,'volt',24)\n",
    "telemetry_df = telem_(telemetry_df,'rotate',24)\n",
    "telemetry_df = telem_(telemetry_df,'pressure',24)\n",
    "telemetry_df = telem_(telemetry_df,'vibration',24)\n",
    "#telemetry_df=telemetry_df.drop(['volt','rotate','pressure','vibration'],axis=1)\n",
    "telemetry_df=telemetry_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ef9f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "maint_transf_df = pd.get_dummies(maint_df, columns=['comp'])\n",
    "maint_transf_df = telemetry_df.merge(maint_transf_df, on=['datetime', 'machineID'], how='left')\n",
    "maint_transf_df= maint_transf_df[['datetime', 'machineID', 'comp_comp1', 'comp_comp2', 'comp_comp3', 'comp_comp4']]\n",
    "maint_transf_df = maint_transf_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bd151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "maintenance_df = lifespan(maint_transf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c3f93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "maintenance_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f84547",
   "metadata": {},
   "source": [
    "#### Merging the rest of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdad100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fget dummies for errorID\n",
    "error_count = pd.get_dummies(errors_df, columns=['errorID'])\n",
    "error_count.rename(columns={'errorID_error5':'error5count'}, inplace=True)\n",
    "error_count.rename(columns={'errorID_error4':'error4count'}, inplace=True)\n",
    "error_count.rename(columns={'errorID_error3':'error3count'}, inplace=True)\n",
    "error_count.rename(columns={'errorID_error2':'error2count'}, inplace=True)\n",
    "error_count.rename(columns={'errorID_error1':'error1count'}, inplace=True)\n",
    "\n",
    "features = telemetry_df.merge(error_count, on=['datetime', 'machineID'], how='left')\n",
    "\n",
    "# Propagate the error information per error type\n",
    "features[['error1count','error2count','error3count','error4count','error5count']] = features[['error1count','error2count','error3count','error4count','error5count']].fillna(method='ffill')\n",
    "# Fill the iinital error count with 0\n",
    "features = features.fillna(0)\n",
    "\n",
    "# turn \"model\" variable into dummy variables\n",
    "machines_df['model'] = machines_df['model'].astype('category')\n",
    "machines_dummy = pd.get_dummies(machines_df, drop_first=False)\n",
    "\n",
    "# Add the machine metadata information\n",
    "features = features.merge(machines_df[['machineID','model']], on=['machineID'], how='left')\n",
    "features = features.merge(machines_dummy, on=['machineID'], how='left')\n",
    "features = features.merge(maintenance_df, on=['datetime', 'machineID'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023436ef",
   "metadata": {},
   "source": [
    "Merge the failures dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5592b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "fails = pd.get_dummies(failures_df,columns=['failure'])\n",
    "#fails.rename(columns={'failure_comp1':'comp1'}, inplace=True)\n",
    "#fails.rename(columns={'failure_comp2':'comp2'}, inplace=True)\n",
    "#fails.rename(columns={'failure_comp3':'comp3'}, inplace=True)\n",
    "#fails.rename(columns={'failure_comp4':'comp4'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69342e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lifespan_fails(comp_rep0):\n",
    "    comp_rep=comp_rep0.copy()\n",
    "    points = comp_rep['machineID'].unique()\n",
    "    final=pd.DataFrame()\n",
    "\n",
    "    for i in points:\n",
    "        df = comp_rep[(comp_rep['machineID']==i)][['datetime','machineID','failure_comp1','failure_comp2','failure_comp3','failure_comp4']]\n",
    "        for comp in ['failure_comp1','failure_comp2','failure_comp3','failure_comp4']:\n",
    "            life=comp.split('_')[1]+'_life'\n",
    "#             prob=comp+'_prob'\n",
    "#             probkm=comp+'_probkm'\n",
    "            df[life] = df.apply(lambda row: row['datetime'] if row[comp]==0 else np.nan, axis=1)\n",
    "            df[df[life].isna()==False].index\n",
    "            df[life].fillna(method='backfill', inplace=True)\n",
    "            df[life] = pd.to_datetime(df[life]) - df['datetime']\n",
    "            df[life] = df[life].apply(lambda row: row.total_seconds()/86400)\n",
    "#             df[prob] = df[life]/(df[life]+df[comp])\n",
    "#             df[prob].fillna(1, inplace=True)\n",
    "#             df[probkm] = 1-1/(df[life]+df[comp])\n",
    "            #back fill with pad\n",
    "#             df[probkm]=df[probkm].replace(-np.inf, np.nan)\n",
    "            \n",
    "        final=pd.concat([final,df],axis=0)\n",
    "    return final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ce270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fails_ = telemetry_df.merge(fails, on=['datetime', 'machineID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8599f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fails_ = fails_.fillna(method='bfill', limit=24) # fill backward up to 24h if all data, otherwise it must be  7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07adcd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "fails_transf = lifespan_fails(fails_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6c7774",
   "metadata": {},
   "outputs": [],
   "source": [
    "fails_transf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da269f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10854d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the individual components that fail , the columns with 0 or 1, after the merge and backfill\n",
    "fails_transf = fails_transf.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f218cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features = features.merge(fails_transf, on=['datetime', 'machineID'], how='left')\n",
    "# labeled_features = labeled_features.fillna(method='bfill', limit=24) # fill backward up to 24h if all data, otherwise it must be  7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e9321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert \"failure\" target variables into multiple binary targets \n",
    "# # i.e. one per component indicating failure/no failure\n",
    "# labeled_features['comp1_fail'] = (labeled_features['failure'] == 'comp1').astype(int)\n",
    "# labeled_features['comp2_fail'] = (labeled_features['failure'] == 'comp2').astype(int)\n",
    "# labeled_features['comp3_fail'] = (labeled_features['failure'] == 'comp3').astype(int)\n",
    "# labeled_features['comp4_fail'] = (labeled_features['failure'] == 'comp4').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cd6f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccf6a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82891c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20a218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features.to_csv('../data/processed/labeled_1h_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b787046",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87d28dc",
   "metadata": {},
   "source": [
    "Machine age - machine age by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3c3fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxh_groupby(df, feature_name, by):\n",
    "    \"\"\"\n",
    "    Box plot with groupby\n",
    "    \n",
    "    df: DataFrame\n",
    "    feature_name: Name of the feature to be plotted\n",
    "    by: Name of the feature based on which groups are created\n",
    "    \"\"\"\n",
    "    df.boxplot(column=feature_name, by=by, vert=False, \n",
    "                              figsize=(10, 6))\n",
    "    plt.title(f'Distribution of {feature_name} by {by}')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_histogram(df, attribute, title_name, bins, figsize=(9,3), alpha=1, label=None):\n",
    "    df[attribute].plot(kind='hist', \n",
    "                              bins=bins, \n",
    "                              figsize=figsize,\n",
    "                              alpha=alpha,\n",
    "                              label=label,\n",
    "                              title=f'{title_name.title()} distribution')\n",
    "    \n",
    "def plot_bar_sortvals(df, attribute, title, figsize=(5,5)):\n",
    "    df[attribute].value_counts(\n",
    "                normalize=False, dropna=False).sort_values().plot(\n",
    "                kind='bar', figsize=figsize, grid=True,\n",
    "                title=title)\n",
    "\n",
    "    \n",
    "def plot_scatter(df, x_axis_attr, y_axis_attr, figsize=(5,5), title=None, legend=None):\n",
    "    df.plot.scatter(x_axis_attr, y_axis_attr, \n",
    "                    figsize=figsize, title=title, \n",
    "                    legend=legend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa842fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features.boxplot(column='age', vert=False, figsize=(5,3))\n",
    "plt.title('Age distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8421c49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features.boxplot(column='age', by='model', vert=False, figsize=(8,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba7cf1f",
   "metadata": {},
   "source": [
    "```\n",
    "We can see that `model 4` has the lowest median age\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f521edcc",
   "metadata": {},
   "source": [
    "Failure per component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f8b5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_sortvals(failures_df, 'failure', 'Number of failures per component')\n",
    "plt.ylabel('Failures')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ca890e",
   "metadata": {},
   "source": [
    "Telemetry data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b4e1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(labeled_features, 'volt', 'Voltage', bins=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c078fa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(labeled_features, 'rotate', 'RPM', bins=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236abe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(labeled_features, 'pressure', 'Pressure', bins=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d242ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(labeled_features, 'vibration', 'Vibration', bins=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdb5d03",
   "metadata": {},
   "source": [
    "```\n",
    "All the telemetry data, taking into consideration all machines, look to be normally distributed. The odd one being rotation, which looks a skewed on the left.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93caac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in labeled_features.groupby(['model']):\n",
    "    plot_histogram(model[1], 'volt', 'Voltage',bins=300, alpha=0.3, label=model[0])\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e58888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in labeled_features.groupby(['model']):\n",
    "    plot_histogram(model[1], 'rotate', 'RPM',bins=300, alpha=0.3, label=model[0])\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52339df4",
   "metadata": {},
   "source": [
    "#### Create a temporary dataframe, which is the `labeled_features` dataframe used in training, but augmented with time features:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474ae189",
   "metadata": {},
   "source": [
    "Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b71d930",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_sortvals(errors_df, 'errorID', title='Number of error per type')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44919d20",
   "metadata": {},
   "source": [
    "```\n",
    "The most common error type is 1, while the least common is error 5.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e018b5c2",
   "metadata": {},
   "source": [
    "Plot age vs errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc03edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_per_machine = errors_df.groupby(\"machineID\").size()\n",
    "errors_per_machine = pd.DataFrame(errors_per_machine, columns=[\"num_errors\"]).reset_index()\n",
    "\n",
    "machines_fail_data = pd.merge(machines_df, errors_per_machine, how='left', on=\"machineID\")\n",
    "\n",
    "maint_per_machine = maint_df.groupby(\"machineID\").size()\n",
    "maint_per_machine = pd.DataFrame(maint_per_machine, columns=[\"num_maint\"]).reset_index()\n",
    "\n",
    "machines_fail_data = pd.merge(machines_fail_data, maint_per_machine, how='left', on=\"machineID\")\n",
    "\n",
    "failure_per_machine = failures_df.groupby(\"machineID\").size()\n",
    "failure_per_machine = pd.DataFrame(failure_per_machine, columns=[\"num_failure\"]).reset_index()\n",
    "\n",
    "machines_fail_data = pd.merge(machines_fail_data, failure_per_machine, how='left', on=\"machineID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a6e3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "machines_fail_data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011e0599",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(machines_fail_data, \"age\", \"num_errors\", \n",
    "             title=\"Age versus number of errors\")\n",
    "\n",
    "plot_scatter(machines_fail_data, \"age\", \"num_failure\", \n",
    "             title=\"Age versus number of failures\")\n",
    "\n",
    "plot_scatter(machines_fail_data, \"age\", \"num_maint\", \n",
    "             title=\"Age versus total number of\\ncomponent maintenance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2c1223",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = machines_fail_data.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm').set_precision(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1087117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_features = labeled_features.copy()\n",
    "augmented_features['month'] = augmented_features.datetime.dt.month\n",
    "augmented_features['week_of_year'] = augmented_features.datetime.dt.isocalendar().week\n",
    "augmented_features['hour'] = augmented_features.datetime.dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4061f240",
   "metadata": {},
   "source": [
    "Failures per machine per component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1860527",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = failures_df.groupby([\"machineID\", \"failure\"]).size().reset_index()\n",
    "temp_df.columns = [\"machineID\", \"comp\", \"num_fail\"]\n",
    "temp_df_pivot = pd.pivot(temp_df, index=\"machineID\", columns=\"comp\", values=\"num_fail\").rename_axis(None, axis=1)\n",
    "\n",
    "temp_df_pivot.plot.bar(stacked=True, figsize=(20, 6), title=\"Count of failures per component for different Machines\")\n",
    "plt.xlabel(\"Machine ID\")\n",
    "plt.ylabel(\"Number of components that failed\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0398d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_features['month'].value_counts(\n",
    "                normalize=False, dropna=False).sort_values().plot(\n",
    "                kind='bar', figsize=(5,5), grid=True,\n",
    "                title='Number of error per type')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b384e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features = pd.read_csv('../data/processed/labeled_1h_data.csv',  index_col='datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d27bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77036eb6",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c54aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(data, test_size):\n",
    "    \n",
    "    machines = machines_df\n",
    "    \n",
    "    x_train_,x_test_,y_train_,y_test_,train_idx,test_idx = train_test_split(machines, machines.model, machines.machineID.index, test_size=test_size, stratify=machines.model, random_state=42)\n",
    "    \n",
    "    training=data[data['machineID'].isin(train_idx)]\n",
    "    training=training.drop(columns=['machineID']).to_numpy()\n",
    "    test=data[data['machineID'].isin(test_idx)]\n",
    "    test=test.drop(columns=['machineID']).to_numpy()\n",
    "\n",
    "    \n",
    "    x_train=training[:,:-1]\n",
    "    y_train=training[:,-1]\n",
    "    x_test=test[:,:-1]\n",
    "    y_test=test[:,-1]\n",
    "\n",
    "    return x_train,x_test,y_train,y_test\n",
    "\n",
    "def preprocess(X_init, test_size, model:str, classi=False, splitting=True):\n",
    "\n",
    "        if splitting:\n",
    "            X_train_unsc, X_test_unsc, y_train_unsc, y_test_unsc =split(X_init,test_size=test_size)\n",
    "            X_init = X_init.to_numpy()\n",
    "            y = X_init[:,-1]\n",
    "            X = X_init[:,:-1]\n",
    "        else:\n",
    "            X_init = X_init.drop(columns=['machineID']).to_numpy()\n",
    "            y = X_init[:,-1]\n",
    "            X = X_init[:,:-1]\n",
    "            X_train_unsc, X_test_unsc, y_train_unsc, y_test_unsc = train_test_split(X, y, test_size=test_size, random_state=0)\n",
    "            \n",
    "        #print(X_train_unsc.shape, y_train_unsc.shape, X_test_unsc.shape, y_test_unsc.shape)\n",
    "        \n",
    "        X_mean = X_train_unsc.mean(axis=0)\n",
    "        X_std = X_train_unsc.std(axis=0)\n",
    "\n",
    "        y_std = y_train_unsc.std()\n",
    "        y_mean = y_train_unsc.mean()\n",
    "\n",
    "        X_train = (X_train_unsc - X_mean)/ X_std\n",
    "        X_test = (X_test_unsc - X_mean)/X_std\n",
    "\n",
    "        y_train = (y_train_unsc- y_mean)/ y_std \n",
    "        y_test = (y_test_unsc- y_mean)/y_std        \n",
    "\n",
    "        X_train_torch = torch.tensor(X_train).float()\n",
    "        \n",
    "        if model == SVI_model_format.poisson['name']:\n",
    "            y_train_torch = torch.tensor(y_train * y_std + y_mean).int()\n",
    "        else:\n",
    "            y_train_torch = torch.tensor(y_train).float()\n",
    "        \n",
    "        if classi:\n",
    "            y_train_torch = torch.tensor(y_train_unsc).float()\n",
    "            y_train = y_train_unsc\n",
    "            y_test = y_test_unsc\n",
    "        X_test_torch = torch.tensor(X_test).float()\n",
    "\n",
    "        return y, X, X_train_torch, y_train_torch,X_test_torch, X_test, y_test, X_train, y_train, y_std, y_mean\n",
    "\n",
    "    \n",
    "def linear_model(X, obs=None):\n",
    "        \n",
    "    alpha = pyro.sample(\"alpha\", dist.Normal(0., 1.))                   # Prior for the bias/intercept\n",
    "    beta  = pyro.sample(\"beta\", dist.Normal(torch.zeros(X.shape[1]), \n",
    "                                            torch.ones(X.shape[1])).to_event())    # Priors for the regression coeffcients\n",
    "    sigma = pyro.sample(\"sigma\", dist.HalfCauchy(5.))                   # Prior for the variance\n",
    "    \n",
    "    with pyro.plate(\"data\"):\n",
    "        y = pyro.sample(\"y\", dist.Normal(alpha + X.matmul(beta), sigma), obs=obs)\n",
    "      \n",
    "    return y\n",
    "\n",
    "\n",
    "def poisson_model(X, obs=None):\n",
    "    alpha = pyro.sample(\"alpha\", dist.Normal(0., 1.))                   # Prior for the bias/intercept\n",
    "    beta  = pyro.sample(\"beta\", dist.Normal(torch.zeros(X.shape[1]), \n",
    "                                            torch.ones(X.shape[1])).to_event())    # Priors for the regression coeffcients\n",
    "    \n",
    "    with pyro.plate(\"data\"):\n",
    "        y = pyro.sample(\"y\", dist.Poisson(torch.exp(alpha + X.matmul(beta))), obs=obs)\n",
    "        \n",
    "    return y\n",
    "\n",
    "\n",
    "def heteroscedastic_model(X, obs=None):\n",
    "    alpha_mu = pyro.sample(\"alpha_mu\", dist.Normal(0., 1.))                 # Prior for the bias/intercept of the mean\n",
    "    beta_mu  = pyro.sample(\"beta_mu\", dist.Normal(torch.zeros(X.shape[1]), \n",
    "                                               torch.ones(X.shape[1])).to_event())     # Priors for the regression coeffcients of the mean\n",
    "    alpha_v = pyro.sample(\"alpha_v\", dist.Normal(0., 1.))                   # Prior for the bias/intercept of the variance\n",
    "    beta_v  = pyro.sample(\"beta_v\", dist.Normal(torch.zeros(X.shape[1]), \n",
    "                                               torch.ones(X.shape[1])).to_event())     # Priors for the regression coeffcients of the variance\n",
    "    \n",
    "    with pyro.plate(\"data\"):\n",
    "        y = pyro.sample(\"y\", dist.Normal(alpha_mu + X.matmul(beta_mu), torch.exp(alpha_v + X.matmul(beta_v))), obs=obs)\n",
    "        \n",
    "    return y\n",
    "\n",
    "def compute_error(trues: np.array, predicted: np.array, threshold: int):\n",
    "    if threshold:\n",
    "        predicted_thres = predicted[np.where(trues<threshold)]\n",
    "        trues_thres  = trues[np.where(trues<threshold)[0]]\n",
    "    else:\n",
    "        print('No threshold')\n",
    "        pass\n",
    "    corr = np.corrcoef(predicted, trues)[0,1]\n",
    "    mae = np.mean(np.abs(predicted - trues))\n",
    "    rae = np.sum(np.abs(predicted - trues)) / np.sum(np.abs(trues - np.mean(trues)))\n",
    "    rmse = np.sqrt(np.mean((predicted - trues)**2))\n",
    "    r2 = max(0, 1 - np.sum((trues-predicted)**2) / np.sum((trues - np.mean(trues))**2))\n",
    "    try:\n",
    "        return corr, mae, rae, rmse, r2, predicted_thres, trues_thres\n",
    "    except:\n",
    "         return corr, mae, rae, rmse, r2, predicted, trues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca31e841",
   "metadata": {},
   "source": [
    "### SVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d90e0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVI_model_format():\n",
    "    '''\n",
    "    defining the type of the regression\n",
    "    '''\n",
    "    poisson = {'name':\"POISSON\", 'model':poisson_model}\n",
    "    heterosc = {'name':\"HETEROSCEDASTIC\", 'model':heteroscedastic_model}\n",
    "    linear = {'name':\"LINEAR\", 'model':linear_model}\n",
    "\n",
    "class SVI_regression_model():\n",
    "    '''\n",
    "    SVI regression: choosing the correct data, preprocess them, pyro inference and prediction\n",
    "    '''\n",
    "    def __init__(self, data: pd.DataFrame, component: int) -> None:\n",
    "        \n",
    "        self.data = data\n",
    "        self.component = component\n",
    "        print(self.component)\n",
    "\n",
    "    def get_data_for_component(self) -> pd.DataFrame:\n",
    "        \n",
    "        '''\n",
    "        returns the feautures of the dataset and the component we want to predict for\n",
    "        '''\n",
    "    \n",
    "        cols = ['machineID', 'voltmean_3h', 'rotatemean_3h',\n",
    "                'pressuremean_3h', 'vibrationmean_3h', 'voltsd_3h', 'rotatesd_3h',\n",
    "                'pressuresd_3h', 'vibrationsd_3h', 'voltmean_24h', 'rotatemean_24h',\n",
    "                'pressuremean_24h', 'vibrationmean_24h', 'voltsd_24h', 'rotatesd_24h',\n",
    "                'pressuresd_24h', 'vibrationsd_24h', 'error1count', 'error2count',\n",
    "                'error3count', 'error4count', 'error5count','age',\n",
    "                'model_model1', 'model_model2', 'model_model3', 'model_model4'] + [word for word in test if str(self.component) in ([*word]) and 'failure' not in word.split('_')]\n",
    "\n",
    "\n",
    "        return self.data[cols]\n",
    "    \n",
    "    def Preprocess(self, X_init: pd.DataFrame, model: str) -> \\\n",
    "        Tuple[np.ndarray, np.ndarray,  torch.tensor,  torch.tensor, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.float64, np.float64]:\n",
    "\n",
    "        '''\n",
    "        the necessary data preprocess before procceding in pyro inference\n",
    "        '''\n",
    "        print(f\"{model} Regression {self.component}\")\n",
    "        print(X_init.columns)\n",
    "        print('\\n-----------------------------------------------------------------------')\n",
    "\n",
    "        y, X, X_train_torch, y_train_torch, X_test_torch, X_test, y_test, X_train, y_train, y_std, y_mean = preprocess(X_init,0.1,model,classi=False,splitting=True)\n",
    " \n",
    "        return y, X, X_train_torch, y_train_torch, X_test_torch, X_test, y_test, X_train, y_train, y_std, y_mean\n",
    "        \n",
    "    def pyro_inference(self, X_train_torch: torch.tensor, y_train_torch: torch.tensor, model: object, steps: int):\n",
    "        '''\n",
    "        pyro inference\n",
    "        '''\n",
    "        \n",
    "        # Reset parameter values\n",
    "        pyro.clear_param_store()\n",
    "\n",
    "\n",
    "        # Define guide function\n",
    "        guide = AutoDiagonalNormal(model)\n",
    "\n",
    "        # Reset parameter values\n",
    "        pyro.clear_param_store()\n",
    "\n",
    "        # Define the number of optimization steps\n",
    "        n_steps = steps\n",
    "\n",
    "        # Setup the optimizer\n",
    "        adam_params = {\"lr\": 0.0001} # learning rate (lr) of optimizer\n",
    "        optimizer = ClippedAdam(adam_params)\n",
    "\n",
    "        # Setup the inference algorithm\n",
    "        elbo = Trace_ELBO(num_particles=1)\n",
    "        svi = SVI(model, guide, optimizer, loss=elbo)\n",
    "\n",
    "        # Do gradient steps\n",
    "        for step in range(n_steps):\n",
    "            elbo = svi.step(X_train_torch, y_train_torch)\n",
    "            if step % 100 == 0:\n",
    "                print(\"[%d] ELBO: %.1f\" % (step, elbo))\n",
    "\n",
    "        return guide\n",
    "    \n",
    "    def post_process(self, guide: object, model: object, X_train_torch: torch.tensor, y_train_torch: torch.tensor, X_test: np.ndarray, y_test: np.ndarray, y_std: np.float64, y_mean: np.float64):\n",
    "\n",
    "        if model['name'] != SVI_model_format.heterosc['name']:\n",
    "            predictive = Predictive(model=model['model'], guide=guide, num_samples=1000, return_sites=(\"alpha\", \"beta\", \"sigma\"))\n",
    "            samples = predictive(X_train_torch, y_train_torch)\n",
    "\n",
    "            alpha_samples = samples[\"alpha\"].detach().numpy()\n",
    "            beta_samples = samples[\"beta\"].detach().numpy()\n",
    "\n",
    "            y_hat = np.mean(np.exp(alpha_samples.T + np.dot(X_test, beta_samples[:,0].T)), axis=1)\n",
    "\n",
    "        else:\n",
    "            predictive = Predictive(model=model['model'], guide=guide, num_samples=1000,\n",
    "                        return_sites=(\"alpha_mu\", \"beta_mu\", \"alpha_v\", \"beta_v\"))\n",
    "            samples = predictive(X_train_torch, y_train_torch)\n",
    "\n",
    "            alpha_samples = samples[\"alpha_mu\"].detach().numpy()\n",
    "            beta_samples = samples[\"beta_mu\"].detach().numpy()\n",
    "\n",
    "            y_hat = np.mean(alpha_samples.T + np.dot(X_test, beta_samples[:,0].T), axis=1)\n",
    "\n",
    "        # convert back to the original scale\n",
    "        preds = y_hat # no need to do any conversion here because the Poisson model received untransformed y's\n",
    "        y_true = y_test * y_std + y_mean\n",
    "\n",
    "        return preds, y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a617e7",
   "metadata": {},
   "source": [
    "```\n",
    "class SVI_model_format():\n",
    "    '''\n",
    "    defining the type of the regression\n",
    "    '''\n",
    "    poisson = {'name':\"POISSON\", 'model':poisson_model}\n",
    "    heterosc = {'name':\"HETEROSCEDASTIC\", 'model':heteroscedastic_model}\n",
    "    linear = {'name':\"LINEAR\", 'model':linear_model}\n",
    "\n",
    "class SVI_regression_model():\n",
    "    '''\n",
    "    SVI regression: choosing the correct data, preprocess them, pyro inference and prediction\n",
    "    '''\n",
    "    def __init__(self, data: pd.DataFrame, component: int) -> None:\n",
    "        \n",
    "        self.data = data\n",
    "        self.component = component\n",
    "        print(self.component)\n",
    "\n",
    "    def get_data_for_component(self) -> pd.DataFrame:\n",
    "        \n",
    "        '''\n",
    "        returns the feautures of the dataset and the component we want to predict for\n",
    "        '''\n",
    "    \n",
    "        cols = ['voltmean_3h', 'rotatemean_3h',\n",
    "                'pressuremean_3h', 'vibrationmean_3h', 'voltsd_3h', 'rotatesd_3h',\n",
    "                'pressuresd_3h', 'vibrationsd_3h', 'voltmean_24h', 'rotatemean_24h',\n",
    "                'pressuremean_24h', 'vibrationmean_24h', 'voltsd_24h', 'rotatesd_24h',\n",
    "                'pressuresd_24h', 'vibrationsd_24h', 'error1count', 'error2count',\n",
    "                'error3count', 'error4count', 'error5count','age',\n",
    "                'model_model1', 'model_model2', 'model_model3', 'model_model4'] + [word for word in test if str(self.component) in ([*word]) and 'failure' not in word.split('_')]\n",
    "\n",
    "\n",
    "        return self.data[cols]\n",
    "    \n",
    "    def Preprocess(self, X_init: pd.DataFrame, model: str) -> \\\n",
    "        Tuple[np.ndarray, np.ndarray,  torch.tensor,  torch.tensor, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.float64, np.float64]:\n",
    "\n",
    "        '''\n",
    "        the necessary data preprocess before procceding in pyro inference\n",
    "        '''\n",
    "        print(f\"{model} Regression {self.component}\")\n",
    "        print(X_init.columns)\n",
    "        print('\\n-----------------------------------------------------------------------')\n",
    "\n",
    "        X = X_init.to_numpy()\n",
    "\n",
    "        # Keep the last column as target y\n",
    "        y = X[:,-1]\n",
    "        X = X[:,:-1]\n",
    "\n",
    "        # standardize input features\n",
    "        X_mean = X.mean(axis=0)\n",
    "        X_std = X.std(axis=0)\n",
    "        X = (X - X_mean) / X_std\n",
    "\n",
    "        # standardize pickups\n",
    "        y_mean = y.mean()\n",
    "        y_std = y.std()\n",
    "        y = (y - y_mean) / y_std\n",
    "\n",
    "        train_perc = 0.80 # percentage of training data\n",
    "        split_point = int(train_perc*len(y))\n",
    "        perm = np.random.permutation(len(y))\n",
    "        ix_train = perm[:split_point]\n",
    "        ix_test = perm[split_point:]\n",
    "        X_train = X[ix_train,:]\n",
    "        X_test = X[ix_test,:]\n",
    "        y_train = y[ix_train]\n",
    "        y_test = y[ix_test]\n",
    "\n",
    "        # Prepare data for Pyro model\n",
    "        X_train_torch = torch.tensor(X_train).float()\n",
    "\n",
    "        if model == SVI_model_format.poisson['name']:\n",
    "            y_train_torch = torch.tensor(y_train * y_std + y_mean).int()\n",
    "        else:\n",
    "            y_train_torch = torch.tensor(y_train).float()\n",
    "           \n",
    "        return y, X, X_train_torch, y_train_torch, X_test, y_test, X_train, y_train, y_std, y_mean\n",
    "\n",
    "        \n",
    "    def pyro_inference(self, X_train_torch: torch.tensor, y_train_torch: torch.tensor, model: object, steps: int):\n",
    "        '''\n",
    "        pyro inference\n",
    "        '''\n",
    "\n",
    "        # Define guide function\n",
    "        guide = AutoDiagonalNormal(model)\n",
    "\n",
    "        # Reset parameter values\n",
    "        pyro.clear_param_store()\n",
    "\n",
    "        # Define the number of optimization steps\n",
    "        n_steps = steps\n",
    "\n",
    "        # Setup the optimizer\n",
    "        adam_params = {\"lr\": 0.0001} # learning rate (lr) of optimizer\n",
    "        optimizer = ClippedAdam(adam_params)\n",
    "\n",
    "        # Setup the inference algorithm\n",
    "        elbo = Trace_ELBO(num_particles=1)\n",
    "        svi = SVI(model, guide, optimizer, loss=elbo)\n",
    "\n",
    "        # Do gradient steps\n",
    "        for step in range(n_steps):\n",
    "            elbo = svi.step(X_train_torch, y_train_torch)\n",
    "            if step % 100 == 0:\n",
    "                print(\"[%d] ELBO: %.1f\" % (step, elbo))\n",
    "\n",
    "        return guide\n",
    "    \n",
    "    def post_process(self, guide: object, model: object, X_train_torch: torch.tensor, y_train_torch: torch.tensor, X_test: np.ndarray, y_test: np.ndarray, y_std: np.float64, y_mean: np.float64):\n",
    "\n",
    "        if model['name'] != SVI_model_format.heterosc['name']:\n",
    "            predictive = Predictive(model=model['model'], guide=guide, num_samples=1000, return_sites=(\"alpha\", \"beta\", \"sigma\"))\n",
    "            samples = predictive(X_train_torch, y_train_torch)\n",
    "\n",
    "            alpha_samples = samples[\"alpha\"].detach().numpy()\n",
    "            beta_samples = samples[\"beta\"].detach().numpy()\n",
    "\n",
    "            y_hat = np.mean(np.exp(alpha_samples.T + np.dot(X_test, beta_samples[:,0].T)), axis=1)\n",
    "\n",
    "        else:\n",
    "            predictive = Predictive(model=model['model'], guide=guide, num_samples=1000,\n",
    "                        return_sites=(\"alpha_mu\", \"beta_mu\", \"alpha_v\", \"beta_v\"))\n",
    "            samples = predictive(X_train_torch, y_train_torch)\n",
    "\n",
    "            alpha_samples = samples[\"alpha_mu\"].detach().numpy()\n",
    "            beta_samples = samples[\"beta_mu\"].detach().numpy()\n",
    "\n",
    "            y_hat = np.mean(alpha_samples.T + np.dot(X_test, beta_samples[:,0].T), axis=1)\n",
    "\n",
    "        # convert back to the original scale\n",
    "        preds = y_hat # no need to do any conversion here because the Poisson model received untransformed y's\n",
    "        y_true = y_test * y_std + y_mean\n",
    "\n",
    "        return preds, y_true\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fd25d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(874906, 45)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de5d0ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'machineID', 'volt', 'rotate', 'pressure', 'vibration',\n",
       "       'voltmean_3h', 'voltsd_3h', 'rotatemean_3h', 'rotatesd_3h',\n",
       "       'pressuremean_3h', 'pressuresd_3h', 'vibrationmean_3h',\n",
       "       'vibrationsd_3h', 'voltmean_24h', 'voltsd_24h', 'rotatemean_24h',\n",
       "       'rotatesd_24h', 'pressuremean_24h', 'pressuresd_24h',\n",
       "       'vibrationmean_24h', 'vibrationsd_24h', 'error1count', 'error2count',\n",
       "       'error3count', 'error4count', 'error5count', 'model', 'age',\n",
       "       'model_model1', 'model_model2', 'model_model3', 'model_model4',\n",
       "       'comp1_maint', 'comp2_maint', 'comp3_maint', 'comp4_maint',\n",
       "       'failure_comp1', 'failure_comp2', 'failure_comp3', 'failure_comp4',\n",
       "       'comp1_life', 'comp2_life', 'comp3_life', 'comp4_life'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_features.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41231a9",
   "metadata": {},
   "source": [
    "#### Poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96dabb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_results = {'poisson': {'comp_1': 0, 'comp_2': 0, 'comp_3': 0, 'comp_4': 0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd5a4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for comp_number in range(1,5):\n",
    "    print(f\"\\n--------------------------------  component {comp_number}  ---------------------------------------\")\n",
    "    svi_regression = SVI_regression_model(labeled_features, comp_number)\n",
    "    svi_dataset = svi_regression.get_data_for_component()\n",
    "    poisson = SVI_model_format.poisson \n",
    "    \n",
    "    y_poisson, X_poisson, X_train_torch_poisson, y_train_torch_poisson, X_test_torch_poisson, X_test_poisson, y_test_poisson, X_train_poisson, y_train_poisson, y_std_poisson, y_mean_poisson = svi_regression.Preprocess(X_init=svi_dataset, model=poisson['name'])\n",
    "    poisson_guide = svi_regression.pyro_inference(X_train_torch=X_train_torch_poisson, y_train_torch=y_train_torch_poisson, model=poisson['model'], steps=10000)\n",
    "    poisson_preds, poisson_y_true = svi_regression.post_process(guide=poisson_guide, model=poisson, X_train_torch=X_train_torch_poisson, y_train_torch=y_train_torch_poisson, X_test=X_test_poisson, y_test=y_test_poisson, y_std=y_std_poisson, y_mean=y_mean_poisson)\n",
    "    corr_poisson, mae_poisson, rae_poisson, rmse_poisson, r2_poisson, svi_trues_poisson, svi_pred_poisson = compute_error(trues=poisson_y_true, predicted=poisson_preds, threshold=None)\n",
    "    poisson_results['poisson'][f\"comp_{comp_number}\"] = [mae_poisson]\n",
    "    print(\"CorrCoef: %.3f\\nMAE: %.3f\\nRMSE: %.3f\\nR2: %.3f\" % (corr_poisson, mae_poisson, rmse_poisson, r2_poisson))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff7aff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46f46fe",
   "metadata": {},
   "source": [
    "#### Heteorscedastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27e2351",
   "metadata": {},
   "outputs": [],
   "source": [
    "hetero_results = {'hetero': {'comp_1': 0, 'comp_2':0, 'comp_3': 0, 'comp_4':0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7aa62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for comp_number in range(1,5):\n",
    "    print(f\"\\n--------------------------------  component {comp_number}  ---------------------------------------\")\n",
    "    svi_regression = SVI_regression_model(labeled_features, comp_number)\n",
    "    svi_dataset = svi_regression.get_data_for_component()\n",
    "    hetero = SVI_model_format.heterosc\n",
    "    \n",
    "    y_hetero, X_hetero, X_train_torch_hetero, y_train_torch_hetero, X_test_torch_hetero, X_test_hetero, y_test_hetero, X_train_hetero, y_train_hetero, y_std_hetero, y_mean_hetero = svi_regression.Preprocess(X_init=svi_dataset, model=hetero['name'])\n",
    "   \n",
    "    hetero_guide = svi_regression.pyro_inference(X_train_torch=X_train_torch_hetero, y_train_torch=y_train_torch_hetero, model=hetero['model'], steps=10000)\n",
    "   \n",
    "    hetero_preds, hetero_y_true = svi_regression.post_process(guide=hetero_guide, model=hetero, X_train_torch=X_train_torch_hetero, y_train_torch=y_train_torch_hetero, X_test=X_test_hetero, y_test=y_test_hetero, y_std=y_std_hetero, y_mean=y_mean_hetero)\n",
    "    corr_hetero, mae_hetero, rae_hetero, rmse_hetero, r2_hetero, svi_trues_hetero, svi_pred_hetero = compute_error(trues=hetero_y_true, predicted=hetero_preds, threshold=None)\n",
    "    hetero_results['hetero'][f\"comp_{comp_number}\"] = [mae_hetero]\n",
    "    print(\"CorrCoef: %.3f\\nMAE: %.3f\\nRMSE: %.3f\\nR2: %.3f\" % (corr_hetero, mae_hetero, rmse_hetero, r2_hetero))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68981060",
   "metadata": {},
   "outputs": [],
   "source": [
    "hetero_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55829176",
   "metadata": {},
   "source": [
    "#### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4f7b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svi_results = {'linear_svi': {'comp_1': 0, 'comp_2':0, 'comp_3': 0, 'comp_4':0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75397c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for comp_number in range(2,5):\n",
    "    print(f\"\\n--------------------------------  component {comp_number}  ---------------------------------------\")\n",
    "    svi_regression = SVI_regression_model(labeled_features, comp_number)\n",
    "    svi_dataset = svi_regression.get_data_for_component()\n",
    "    linear_svi = SVI_model_format.linear\n",
    " \n",
    "    y_linear_svi, X_linear_svi, X_train_torch_linear_svi, y_train_torch_linear_svi,  X_test_torch_linear_svi, X_test_linear_svi, y_test_linear_svi, X_train_linear_svi, y_train_linear_svi, y_std_linear_svi, y_mean_linear_svi = svi_regression.Preprocess(X_init=svi_dataset, model=linear_svi['name'])\n",
    "  \n",
    "    linear_svi_guide = svi_regression.pyro_inference(X_train_torch=X_train_torch_linear_svi, y_train_torch=y_train_torch_linear_svi, model=linear_svi['model'], steps=10000)\n",
    "    linear_svi_preds, linear_svi_y_true = svi_regression.post_process(guide=linear_svi_guide, model=linear_svi, X_train_torch=X_train_torch_linear_svi, y_train_torch=y_train_torch_linear_svi, X_test=X_test_linear_svi, y_test=y_test_linear_svi, y_std=y_std_linear_svi, y_mean=y_mean_linear_svi)\n",
    "    \n",
    "    corr_linear_svi, mae_linear_svi, rae_linear_svi, rmse_linear_svi, r2_linear_svi, svi_trues_linear_svi, svi_pred_linear_svi = compute_error(trues=linear_svi_y_true, predicted=linear_svi_preds, threshold=None)\n",
    "    linear_svi_results['linear_svi'][f\"comp_{comp_number}\"] = [mae_linear_svi]\n",
    "    print(\"CorrCoef: %.3f\\nMAE: %.3f\\nRMSE: %.3f\\nR2: %.3f\" % (corr_linear_svi, mae_linear_svi, rmse_linear_svi, r2_linear_svi))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e101fc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svi_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918eeb21",
   "metadata": {},
   "source": [
    "### MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85da0c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCMC_model_format():\n",
    "    '''\n",
    "    defining the type of the regression\n",
    "    '''\n",
    "    poisson = {'name':\"POISSON\", 'model':poisson_model}\n",
    "    heterosc = {'name':\"HETEROSCEDASTIC\", 'model':heteroscedastic_model}\n",
    "    linear = {'name':\"LINEAR\", 'model':linear_model}\n",
    "\n",
    "class MCMC_regression_model():\n",
    "\n",
    "    def __init__(self, data: pd.DataFrame, component: int) -> None:\n",
    "        \n",
    "        self.data = data\n",
    "        self.component = component\n",
    "        print(self.component)\n",
    "\n",
    "    def get_data_for_component(self) -> pd.DataFrame:\n",
    "        \n",
    "        '''\n",
    "        returns the feautures of the dataset and the component we want to predict for\n",
    "        '''\n",
    "    \n",
    "        components_cols = ['comp1_maint', 'comp2_maint', 'comp3_maint', 'comp4_maint',\n",
    "       'failure_comp1', 'failure_comp2', 'failure_comp3', 'failure_comp4',\n",
    "       'comp1_life', 'comp2_life', 'comp3_life', 'comp4_life']\n",
    "        \n",
    "        cols = ['machineID', 'voltmean_3h', 'rotatemean_3h',\n",
    "                'pressuremean_3h', 'vibrationmean_3h', 'voltsd_3h', 'rotatesd_3h',\n",
    "                'pressuresd_3h', 'vibrationsd_3h', 'voltmean_24h', 'rotatemean_24h',\n",
    "                'pressuremean_24h', 'vibrationmean_24h', 'voltsd_24h', 'rotatesd_24h',\n",
    "                'pressuresd_24h', 'vibrationsd_24h', 'error1count', 'error2count',\n",
    "                'error3count', 'error4count', 'error5count','age',\n",
    "                'model_model1', 'model_model2', 'model_model3', 'model_model4'] + [word for word in components_cols if str(self.component) in ([*word]) and 'failure' not in word.split('_')]\n",
    "\n",
    "\n",
    "        return self.data[cols]\n",
    "    \n",
    "    def Preprocess(self, X_init: pd.DataFrame, model: str) -> \\\n",
    "        Tuple[np.ndarray, np.ndarray,  torch.tensor,  torch.tensor, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.float64, np.float64]:\n",
    "\n",
    "        '''\n",
    "        the necessary data preprocess before procceding in pyro inference\n",
    "        '''\n",
    "        print(f\"{model} Regression {self.component}\")\n",
    "        print(X_init.columns)\n",
    "        print('\\n-----------------------------------------------------------------------')\n",
    "\n",
    "        y, X, X_train_torch, y_train_torch, X_test_torch, X_test, y_test, X_train, y_train, y_std, y_mean = preprocess(X_init,0.1,model,classi=False,splitting=True)\n",
    " \n",
    "        return y, X, X_train_torch, y_train_torch, X_test_torch, X_test, y_test, X_train, y_train, y_std, y_mean\n",
    "    \n",
    "\n",
    "    def pyro_inference(self, X_train_torch, y_train_torch, model, num_samples):\n",
    "\n",
    "        # Reset parameter values\n",
    "        pyro.clear_param_store()\n",
    "\n",
    "        # Run inference in Pyro\n",
    "        nuts_kernel = NUTS(model)\n",
    "        \n",
    "        mcmc = MCMC(nuts_kernel, num_samples=num_samples, warmup_steps=200, num_chains=1)\n",
    "        mcmc.run(X_train_torch, y_train_torch)\n",
    "        \n",
    "        # Compute feature importance\n",
    "        feature_names = self.data.columns[1:-1]\n",
    "        print(feature_names)\n",
    "        \n",
    "        feature_importance = {}\n",
    "\n",
    "        for feature in feature_names:\n",
    "            marginal = EmpiricalMarginal(mcmc.get_samples(), sites=feature)\n",
    "            feature_importance[feature] = torch.std(marginal._get_samples_and_weights()[0]).item()\n",
    "\n",
    "        print(\"Feature Importance:\")\n",
    "        for feature, importance in feature_importance.items():\n",
    "            print(f\"{feature}: {importance}\")\n",
    "\n",
    "        # Show summary of inference results\n",
    "        print(mcmc.summary())\n",
    "\n",
    "        return mcmc.get_samples()\n",
    "\n",
    "    def post_process(self, X_test, X_train, samples, y_std, y_mean, y_test):\n",
    "\n",
    "        posterior_samples = samples\n",
    "\n",
    "        # Compute predictions\n",
    "        y_hat = np.mean(posterior_samples[\"alpha\"].numpy().T + np.dot(X_test, posterior_samples[\"beta\"].numpy().T), axis=1)\n",
    "        y_hat_train = np.mean(posterior_samples[\"alpha\"].numpy().T + np.dot(X_train, posterior_samples[\"beta\"].numpy().T), axis=1)\n",
    "\n",
    "        # Convert back to the original scale\n",
    "        preds = y_hat * y_std + y_mean\n",
    "        preds_train = y_hat_train * y_std + y_mean\n",
    "        y_true = y_test * y_std + y_mean\n",
    "\n",
    "        return preds, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be5e50bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_mcmc_results = {'linear_mcmc': {'comp_1': 0, 'comp_2':0, 'comp_3': 0, 'comp_4':0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "194898c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------  component 1  ---------------------------------------\n",
      "1\n",
      "LINEAR Regression 1\n",
      "Index(['machineID', 'voltmean_3h', 'rotatemean_3h', 'pressuremean_3h',\n",
      "       'vibrationmean_3h', 'voltsd_3h', 'rotatesd_3h', 'pressuresd_3h',\n",
      "       'vibrationsd_3h', 'voltmean_24h', 'rotatemean_24h', 'pressuremean_24h',\n",
      "       'vibrationmean_24h', 'voltsd_24h', 'rotatesd_24h', 'pressuresd_24h',\n",
      "       'vibrationsd_24h', 'error1count', 'error2count', 'error3count',\n",
      "       'error4count', 'error5count', 'age', 'model_model1', 'model_model2',\n",
      "       'model_model3', 'model_model4', 'comp1_maint', 'comp1_life'],\n",
      "      dtype='object')\n",
      "\n",
      "-----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample:  72%|██████████████████████████████▏           | 216/300 [12:04,  3.04s/it, step size=2.41e-03, acc. prob=0.920]                                                                       "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m linear_mcmc \u001b[38;5;241m=\u001b[39m MCMC_model_format\u001b[38;5;241m.\u001b[39mlinear\n\u001b[1;32m      8\u001b[0m y_linear_mcmc, X_linear_mcmc, X_train_torch_linear_mcmc, y_train_torch_linear_mcmc,  X_test_torch_linear_mcmc, X_test_linear_mcmc, y_test_linear_mcmc, X_train_linear_mcmc, y_train_linear_mcmc, y_std_linear_mcmc, y_mean_linear_mcmc \u001b[38;5;241m=\u001b[39m mcmc_regression\u001b[38;5;241m.\u001b[39mPreprocess(X_init\u001b[38;5;241m=\u001b[39mmcmc_dataset, model\u001b[38;5;241m=\u001b[39mlinear_mcmc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 10\u001b[0m linear_mcmc_samples \u001b[38;5;241m=\u001b[39m \u001b[43mmcmc_regression\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpyro_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_torch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train_torch_linear_mcmc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_torch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train_torch_linear_mcmc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinear_mcmc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m linear_mcmc_preds, linear_mcmc_y_true \u001b[38;5;241m=\u001b[39m mcmc_regression\u001b[38;5;241m.\u001b[39mpost_process(samples\u001b[38;5;241m=\u001b[39mlinear_mcmc_samples, X_train \u001b[38;5;241m=\u001b[39m X_train_linear_mcmc, X_test \u001b[38;5;241m=\u001b[39m X_test_linear_mcmc, y_test\u001b[38;5;241m=\u001b[39my_test_linear_mcmc, y_std\u001b[38;5;241m=\u001b[39my_std_linear_mcmc, y_mean\u001b[38;5;241m=\u001b[39my_mean_linear_mcmc)\n\u001b[1;32m     13\u001b[0m corr_linear_mcmc, mae_linear_mcmc, rae_linear_mcmc, rmse_linear_mcmc, r2_linear_mcmc, mcmc_trues_linear_mcmc, mcmc_pred_linear_mcmc \u001b[38;5;241m=\u001b[39m compute_error(trues\u001b[38;5;241m=\u001b[39mlinear_mcmc_y_true, predicted\u001b[38;5;241m=\u001b[39mlinear_mcmc_preds, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[19], line 62\u001b[0m, in \u001b[0;36mMCMC_regression_model.pyro_inference\u001b[0;34m(self, X_train_torch, y_train_torch, model, num_samples)\u001b[0m\n\u001b[1;32m     59\u001b[0m nuts_kernel \u001b[38;5;241m=\u001b[39m NUTS(model)\n\u001b[1;32m     61\u001b[0m mcmc \u001b[38;5;241m=\u001b[39m MCMC(nuts_kernel, num_samples\u001b[38;5;241m=\u001b[39mnum_samples, warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, num_chains\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 62\u001b[0m \u001b[43mmcmc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_torch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Compute feature importance\u001b[39;00m\n\u001b[1;32m     65\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m seld\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/.virtualenvs/mbml/lib/python3.10/site-packages/pyro/poutine/messenger.py:12\u001b[0m, in \u001b[0;36m_context_wrap\u001b[0;34m(context, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_context_wrap\u001b[39m(context, fn, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m context:\n\u001b[0;32m---> 12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/mbml/lib/python3.10/site-packages/pyro/infer/mcmc/api.py:563\u001b[0m, in \u001b[0;36mMCMC.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m optional(\n\u001b[1;32m    555\u001b[0m     pyro\u001b[38;5;241m.\u001b[39mvalidation_enabled(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable_validation),\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable_validation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;66;03m# This also resolves \"RuntimeError: Cowardly refusing to serialize non-leaf tensor which\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;66;03m# requires_grad\", which happens with `jit_compile` under PyTorch 1.7\u001b[39;00m\n\u001b[1;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m [arg\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(arg) \u001b[38;5;28;01melse\u001b[39;00m arg \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x, chain_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    564\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m num_samples[chain_id] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    565\u001b[0m             num_samples[chain_id] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.virtualenvs/mbml/lib/python3.10/site-packages/pyro/infer/mcmc/api.py:223\u001b[0m, in \u001b[0;36m_UnarySampler.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m logger \u001b[38;5;241m=\u001b[39m initialize_logger(logger, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, progress_bar)\n\u001b[1;32m    222\u001b[0m hook_w_logging \u001b[38;5;241m=\u001b[39m _add_logging_hook(logger, progress_bar, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook)\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m _gen_samples(\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel,\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarmup_steps,\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples,\n\u001b[1;32m    227\u001b[0m     hook_w_logging,\n\u001b[1;32m    228\u001b[0m     i \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_chains \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    231\u001b[0m ):\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m sample, i  \u001b[38;5;66;03m# sample, chain_id\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mcleanup()\n",
      "File \u001b[0;32m~/.virtualenvs/mbml/lib/python3.10/site-packages/pyro/infer/mcmc/api.py:158\u001b[0m, in \u001b[0;36m_gen_samples\u001b[0;34m(kernel, warmup_steps, num_samples, hook, chain_id, *args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     hook(\n\u001b[1;32m    152\u001b[0m         kernel,\n\u001b[1;32m    153\u001b[0m         params,\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarmup [\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(chain_id) \u001b[38;5;28;01mif\u001b[39;00m chain_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarmup\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    155\u001b[0m         i,\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples):\n\u001b[0;32m--> 158\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[43mkernel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m     hook(\n\u001b[1;32m    160\u001b[0m         kernel,\n\u001b[1;32m    161\u001b[0m         params,\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample [\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(chain_id) \u001b[38;5;28;01mif\u001b[39;00m chain_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    163\u001b[0m         i,\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     flat \u001b[38;5;241m=\u001b[39m [params[name]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m save_params]\n",
      "File \u001b[0;32m~/.virtualenvs/mbml/lib/python3.10/site-packages/pyro/infer/mcmc/nuts.py:437\u001b[0m, in \u001b[0;36mNUTS.sample\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    433\u001b[0m direction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(direction\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    435\u001b[0m     direction \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    436\u001b[0m ):  \u001b[38;5;66;03m# go to the right, start from the right leaf of current tree\u001b[39;00m\n\u001b[0;32m--> 437\u001b[0m     new_tree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_tree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mz_right\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mr_right\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mz_right_grads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdirection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtree_depth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43menergy_current\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;66;03m# update leaf for the next doubling process\u001b[39;00m\n\u001b[1;32m    447\u001b[0m     z_right \u001b[38;5;241m=\u001b[39m new_tree\u001b[38;5;241m.\u001b[39mz_right\n",
      "File \u001b[0;32m~/.virtualenvs/mbml/lib/python3.10/site-packages/pyro/infer/mcmc/nuts.py:259\u001b[0m, in \u001b[0;36mNUTS._build_tree\u001b[0;34m(self, z, r, z_grads, log_slice, direction, tree_depth, energy_current)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_basetree(\n\u001b[1;32m    255\u001b[0m         z, r, z_grads, log_slice, direction, energy_current\n\u001b[1;32m    256\u001b[0m     )\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# build the first half of tree\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m half_tree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_tree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_slice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree_depth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menergy_current\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m z_proposal \u001b[38;5;241m=\u001b[39m half_tree\u001b[38;5;241m.\u001b[39mz_proposal\n\u001b[1;32m    263\u001b[0m z_proposal_pe \u001b[38;5;241m=\u001b[39m half_tree\u001b[38;5;241m.\u001b[39mz_proposal_pe\n",
      "File \u001b[0;32m~/.virtualenvs/mbml/lib/python3.10/site-packages/pyro/infer/mcmc/nuts.py:259\u001b[0m, in \u001b[0;36mNUTS._build_tree\u001b[0;34m(self, z, r, z_grads, log_slice, direction, tree_depth, energy_current)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_basetree(\n\u001b[1;32m    255\u001b[0m         z, r, z_grads, log_slice, direction, energy_current\n\u001b[1;32m    256\u001b[0m     )\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# build the first half of tree\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m half_tree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_tree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_slice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree_depth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menergy_current\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m z_proposal \u001b[38;5;241m=\u001b[39m half_tree\u001b[38;5;241m.\u001b[39mz_proposal\n\u001b[1;32m    263\u001b[0m z_proposal_pe \u001b[38;5;241m=\u001b[39m half_tree\u001b[38;5;241m.\u001b[39mz_proposal_pe\n",
      "File \u001b[0;32m~/.virtualenvs/mbml/lib/python3.10/site-packages/pyro/infer/mcmc/nuts.py:259\u001b[0m, in \u001b[0;36mNUTS._build_tree\u001b[0;34m(self, z, r, z_grads, log_slice, direction, tree_depth, energy_current)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_basetree(\n\u001b[1;32m    255\u001b[0m         z, r, z_grads, log_slice, direction, energy_current\n\u001b[1;32m    256\u001b[0m     )\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# build the first half of tree\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m half_tree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_tree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_slice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree_depth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menergy_current\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m z_proposal \u001b[38;5;241m=\u001b[39m half_tree\u001b[38;5;241m.\u001b[39mz_proposal\n\u001b[1;32m    263\u001b[0m z_proposal_pe \u001b[38;5;241m=\u001b[39m half_tree\u001b[38;5;241m.\u001b[39mz_proposal_pe\n",
      "File \u001b[0;32m~/.virtualenvs/mbml/lib/python3.10/site-packages/pyro/infer/mcmc/nuts.py:281\u001b[0m, in \u001b[0;36mNUTS._build_tree\u001b[0;34m(self, z, r, z_grads, log_slice, direction, tree_depth, energy_current)\u001b[0m\n\u001b[1;32m    279\u001b[0m     r \u001b[38;5;241m=\u001b[39m half_tree\u001b[38;5;241m.\u001b[39mr_left\n\u001b[1;32m    280\u001b[0m     z_grads \u001b[38;5;241m=\u001b[39m half_tree\u001b[38;5;241m.\u001b[39mz_left_grads\n\u001b[0;32m--> 281\u001b[0m other_half_tree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_tree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_slice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree_depth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menergy_current\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_multinomial_sampling:\n\u001b[1;32m    286\u001b[0m     tree_weight \u001b[38;5;241m=\u001b[39m _logaddexp(half_tree\u001b[38;5;241m.\u001b[39mweight, other_half_tree\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/.virtualenvs/mbml/lib/python3.10/site-packages/pyro/infer/mcmc/nuts.py:281\u001b[0m, in \u001b[0;36mNUTS._build_tree\u001b[0;34m(self, z, r, z_grads, log_slice, direction, tree_depth, energy_current)\u001b[0m\n\u001b[1;32m    279\u001b[0m     r \u001b[38;5;241m=\u001b[39m half_tree\u001b[38;5;241m.\u001b[39mr_left\n\u001b[1;32m    280\u001b[0m     z_grads \u001b[38;5;241m=\u001b[39m half_tree\u001b[38;5;241m.\u001b[39mz_left_grads\n\u001b[0;32m--> 281\u001b[0m other_half_tree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_tree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_slice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree_depth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menergy_current\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_multinomial_sampling:\n\u001b[1;32m    286\u001b[0m     tree_weight \u001b[38;5;241m=\u001b[39m _logaddexp(half_tree\u001b[38;5;241m.\u001b[39mweight, other_half_tree\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/.virtualenvs/mbml/lib/python3.10/site-packages/pyro/infer/mcmc/nuts.py:259\u001b[0m, in \u001b[0;36mNUTS._build_tree\u001b[0;34m(self, z, r, z_grads, log_slice, direction, tree_depth, energy_current)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_basetree(\n\u001b[1;32m    255\u001b[0m         z, r, z_grads, log_slice, direction, energy_current\n\u001b[1;32m    256\u001b[0m     )\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# build the first half of tree\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m half_tree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_tree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_slice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree_depth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menergy_current\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m z_proposal \u001b[38;5;241m=\u001b[39m half_tree\u001b[38;5;241m.\u001b[39mz_proposal\n\u001b[1;32m    263\u001b[0m z_proposal_pe \u001b[38;5;241m=\u001b[39m half_tree\u001b[38;5;241m.\u001b[39mz_proposal_pe\n",
      "File \u001b[0;32m~/.virtualenvs/mbml/lib/python3.10/site-packages/pyro/infer/mcmc/nuts.py:254\u001b[0m, in \u001b[0;36mNUTS._build_tree\u001b[0;34m(self, z, r, z_grads, log_slice, direction, tree_depth, energy_current)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_tree\u001b[39m(\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m, z, r, z_grads, log_slice, direction, tree_depth, energy_current\n\u001b[1;32m    252\u001b[0m ):\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tree_depth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 254\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_basetree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m            \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_slice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menergy_current\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# build the first half of tree\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     half_tree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_tree(\n\u001b[1;32m    260\u001b[0m         z, r, z_grads, log_slice, direction, tree_depth \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, energy_current\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/.virtualenvs/mbml/lib/python3.10/site-packages/pyro/infer/mcmc/nuts.py:199\u001b[0m, in \u001b[0;36mNUTS._build_basetree\u001b[0;34m(self, z, r, z_grads, log_slice, direction, energy_current)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_basetree\u001b[39m(\u001b[38;5;28mself\u001b[39m, z, r, z_grads, log_slice, direction, energy_current):\n\u001b[1;32m    198\u001b[0m     step_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_size \u001b[38;5;28;01mif\u001b[39;00m direction \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_size\n\u001b[0;32m--> 199\u001b[0m     z_new, r_new, z_grads, potential_energy \u001b[38;5;241m=\u001b[39m \u001b[43mvelocity_verlet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpotential_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmass_matrix_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkinetic_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mz_grads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz_grads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     r_new_unscaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmass_matrix_adapter\u001b[38;5;241m.\u001b[39munscale(r_new)\n\u001b[1;32m    208\u001b[0m     energy_new \u001b[38;5;241m=\u001b[39m potential_energy \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kinetic_energy(r_new_unscaled)\n",
      "File \u001b[0;32m~/.virtualenvs/mbml/lib/python3.10/site-packages/pyro/ops/integrator.py:39\u001b[0m, in \u001b[0;36mvelocity_verlet\u001b[0;34m(z, r, potential_fn, kinetic_grad, step_size, num_steps, z_grads)\u001b[0m\n\u001b[1;32m     37\u001b[0m r_next \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_steps):\n\u001b[0;32m---> 39\u001b[0m     z_next, r_next, z_grads, potential_energy \u001b[38;5;241m=\u001b[39m \u001b[43m_single_step_verlet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mz_next\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_next\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpotential_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkinetic_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_grads\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m z_next, r_next, z_grads, potential_energy\n",
      "File \u001b[0;32m~/.virtualenvs/mbml/lib/python3.10/site-packages/pyro/ops/integrator.py:61\u001b[0m, in \u001b[0;36m_single_step_verlet\u001b[0;34m(z, r, potential_fn, kinetic_grad, step_size, z_grads)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m site_name \u001b[38;5;129;01min\u001b[39;00m z:\n\u001b[1;32m     59\u001b[0m     z[site_name] \u001b[38;5;241m=\u001b[39m z[site_name] \u001b[38;5;241m+\u001b[39m step_size \u001b[38;5;241m*\u001b[39m r_grads[site_name]  \u001b[38;5;66;03m# z(n+1)\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m z_grads, potential_energy \u001b[38;5;241m=\u001b[39m \u001b[43mpotential_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpotential_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m site_name \u001b[38;5;129;01min\u001b[39;00m r:\n\u001b[1;32m     63\u001b[0m     r[site_name] \u001b[38;5;241m=\u001b[39m r[site_name] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m step_size \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m-\u001b[39mz_grads[site_name])  \u001b[38;5;66;03m# r(n+1)\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/mbml/lib/python3.10/site-packages/pyro/ops/integrator.py:83\u001b[0m, in \u001b[0;36mpotential_grad\u001b[0;34m(potential_fn, z)\u001b[0m\n\u001b[1;32m     81\u001b[0m     node\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m     potential_energy \u001b[38;5;241m=\u001b[39m \u001b[43mpotential_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# handle exceptions as defined in the exception registry\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.virtualenvs/mbml/lib/python3.10/site-packages/pyro/infer/mcmc/util.py:281\u001b[0m, in \u001b[0;36m_PEMaker._potential_fn\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    277\u001b[0m cond_model \u001b[38;5;241m=\u001b[39m poutine\u001b[38;5;241m.\u001b[39mcondition(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, params_constrained)\n\u001b[1;32m    278\u001b[0m model_trace \u001b[38;5;241m=\u001b[39m poutine\u001b[38;5;241m.\u001b[39mtrace(cond_model)\u001b[38;5;241m.\u001b[39mget_trace(\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_kwargs\n\u001b[1;32m    280\u001b[0m )\n\u001b[0;32m--> 281\u001b[0m log_joint \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_prob_evaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_trace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    283\u001b[0m     log_joint \u001b[38;5;241m=\u001b[39m log_joint \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\n\u001b[1;32m    284\u001b[0m         t\u001b[38;5;241m.\u001b[39mlog_abs_det_jacobian(params_constrained[name], params[name])\n\u001b[1;32m    285\u001b[0m     )\n",
      "File \u001b[0;32m~/.virtualenvs/mbml/lib/python3.10/site-packages/pyro/infer/mcmc/util.py:238\u001b[0m, in \u001b[0;36mTraceEinsumEvaluator.log_prob\u001b[0;34m(self, model_trace)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03mReturns the log pdf of `model_trace` by appropriately handling\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03menumerated log prob factors.\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03m:return: log pdf of the trace.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_enumerable_sites:\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_trace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_log_factors(model_trace)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shared_intermediates() \u001b[38;5;28;01mas\u001b[39;00m cache:\n",
      "File \u001b[0;32m~/.virtualenvs/mbml/lib/python3.10/site-packages/pyro/poutine/trace_struct.py:196\u001b[0m, in \u001b[0;36mTrace.log_prob_sum\u001b[0;34m(self, site_filter)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         log_p \u001b[38;5;241m=\u001b[39m \u001b[43msite\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m            \u001b[49m\u001b[43msite\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msite\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43margs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msite\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkwargs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    200\u001b[0m         _, exc_value, traceback \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m~/.virtualenvs/mbml/lib/python3.10/site-packages/torch/distributions/normal.py:82\u001b[0m, in \u001b[0;36mNormal.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# compute the variance\u001b[39;00m\n\u001b[1;32m     81\u001b[0m var \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 82\u001b[0m log_scale \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale, Real) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m((value \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m var) \u001b[38;5;241m-\u001b[39m log_scale \u001b[38;5;241m-\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mpi))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for comp_number in range(1,5):\n",
    "    print(f\"\\n--------------------------------  component {comp_number}  ---------------------------------------\")\n",
    "    mcmc_regression = MCMC_regression_model(labeled_features, comp_number)\n",
    "    mcmc_dataset = mcmc_regression.get_data_for_component()\n",
    "    linear_mcmc = MCMC_model_format.linear\n",
    " \n",
    "\n",
    "    y_linear_mcmc, X_linear_mcmc, X_train_torch_linear_mcmc, y_train_torch_linear_mcmc,  X_test_torch_linear_mcmc, X_test_linear_mcmc, y_test_linear_mcmc, X_train_linear_mcmc, y_train_linear_mcmc, y_std_linear_mcmc, y_mean_linear_mcmc = mcmc_regression.Preprocess(X_init=mcmc_dataset, model=linear_mcmc['name'])\n",
    "  \n",
    "    linear_mcmc_samples = mcmc_regression.pyro_inference(X_train_torch=X_train_torch_linear_mcmc, y_train_torch=y_train_torch_linear_mcmc, model=linear_mcmc['model'], num_samples=100)\n",
    "    linear_mcmc_preds, linear_mcmc_y_true = mcmc_regression.post_process(samples=linear_mcmc_samples, X_train = X_train_linear_mcmc, X_test = X_test_linear_mcmc, y_test=y_test_linear_mcmc, y_std=y_std_linear_mcmc, y_mean=y_mean_linear_mcmc)\n",
    "    \n",
    "    corr_linear_mcmc, mae_linear_mcmc, rae_linear_mcmc, rmse_linear_mcmc, r2_linear_mcmc, mcmc_trues_linear_mcmc, mcmc_pred_linear_mcmc = compute_error(trues=linear_mcmc_y_true, predicted=linear_mcmc_preds, threshold=None)\n",
    "    linear_mcmc_results['linear_mcmc'][f\"comp_{comp_number}\"] = [mae_linear_mcmc]\n",
    "    print(\"CorrCoef: %.3f\\nMAE: %.3f\\nRMSE: %.3f\\nR2: %.3f\" % (corr_linear_mcmc, mae_linear_mcmc, rmse_linear_mcmc, r2_linear_mcmc))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898ac36e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mbml",
   "language": "python",
   "name": "mbml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
