{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7279b216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "import torch\n",
    "import pyro.distributions as dist\n",
    "import torch.nn as nn\n",
    "\n",
    "from pyro.nn import PyroModule, PyroSample\n",
    "from pyro.infer import SVI, Trace_ELBO, MCMC, NUTS\n",
    "from pyro.optim import ClippedAdam, Adam\n",
    "from pyro.contrib.autoguide import AutoDiagonalNormal\n",
    "from pyro.infer import Predictive\n",
    "from typing import Dict, Tuple, List\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas import DataFrame\n",
    "from tqdm.autonotebook import tqdm\n",
    "from pyro.nn import PyroModule, PyroSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c536413",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/raw/raw2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5699889",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44f9301",
   "metadata": {},
   "source": [
    "Read the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf57d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "failures_path = os.path.join(data_path, 'PdM_failures.csv')\n",
    "errors_path = os.path.join(data_path, 'PdM_errors.csv')\n",
    "machines_path = os.path.join(data_path, 'PdM_machines.csv')\n",
    "maint_path = os.path.join(data_path, 'PdM_maint.csv')\n",
    "telemetry_path = os.path.join(data_path, 'PdM_telemetry.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1124d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "failures_df = pd.read_csv(failures_path)\n",
    "errors_df = pd.read_csv(errors_path)\n",
    "machines_df = pd.read_csv(machines_path)\n",
    "maint_df = pd.read_csv(maint_path)\n",
    "telemetry_df = pd.read_csv(telemetry_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01458f9",
   "metadata": {},
   "source": [
    "Transform `datetime` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f197c9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "maint_df['datetime'] = pd.to_datetime(maint_df['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "failures_df['datetime'] = pd.to_datetime(failures_df['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "errors_df['datetime'] = pd.to_datetime(errors_df['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "telemetry_df['datetime'] = pd.to_datetime(telemetry_df['datetime'], format=\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588ef274",
   "metadata": {},
   "source": [
    "#### Dataset transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843f0a23",
   "metadata": {},
   "source": [
    "Maintenance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af80ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function that takes a column and returns its mean and std of a moving window of 3 hours\n",
    "def rolling_mean_std(col,window):\n",
    "    return col.rolling(window).agg(['mean', 'std'])\n",
    "\n",
    "#apply the function to the telemetry data\n",
    "def telem_(telemetry,column,window):\n",
    "    telemetry[[column+'mean_'+str(window)+'h', column+'sd_'+str(window)+'h']] = telemetry.groupby('machineID')[column].apply(rolling_mean_std,window)\n",
    "    return telemetry\n",
    "\n",
    "def lifespan(replacement_event_df: DataFrame)->DataFrame:\n",
    "    \n",
    "    '''\n",
    "    Receives a dataframe with timestamp and columns that signify when a component is replaced, with 1.\n",
    "    Returns a dataframe with the days since the last replacement for the component\n",
    "    '''\n",
    "    \n",
    "    comp_rep=replacement_event_df.copy()\n",
    "    points = comp_rep['machineID'].unique()\n",
    "    final=pd.DataFrame()\n",
    "    \n",
    "    for i in tqdm(points, desc='Machine'):\n",
    "        df = comp_rep[(comp_rep['machineID']==i)][['datetime','machineID','comp_comp1', 'comp_comp2', 'comp_comp3', 'comp_comp4']]\n",
    "        for comp in ['comp_comp1', 'comp_comp2', 'comp_comp3', 'comp_comp4']:\n",
    "            \n",
    "            # keep the last part of component name\n",
    "            life = comp[5:]\n",
    "            \n",
    "            #apply function in each row of df[life] column where if row[comp]==1 then value=0.041667 else 0\n",
    "            df[life+'_maint'] = df.apply(lambda row: 0 if row[comp]==1 else 0.041667, axis=1)\n",
    "\n",
    "            df_maint = df[life+'_maint'] != 0\n",
    "            df[life+'_maint'] = df_maint.cumsum()-df_maint.cumsum().where(~df_maint).ffill().fillna(0).astype(int)\n",
    "            df[life+'_maint'] = df[life+'_maint'].apply(lambda x: x*0.041667)\n",
    "            \n",
    "            \n",
    "        final=pd.concat([final,df],axis=0)\n",
    "        final=final[['datetime', 'machineID', 'comp1_maint', 'comp2_maint', 'comp3_maint', 'comp4_maint']]\n",
    "        \n",
    "    return final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36129b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "telemetry_df = telem_(telemetry_df,'volt',3)\n",
    "telemetry_df = telem_(telemetry_df,'rotate',3)\n",
    "telemetry_df = telem_(telemetry_df,'pressure',3)\n",
    "telemetry_df = telem_(telemetry_df,'vibration',3)\n",
    "telemetry_df = telem_(telemetry_df,'volt',24)\n",
    "telemetry_df = telem_(telemetry_df,'rotate',24)\n",
    "telemetry_df = telem_(telemetry_df,'pressure',24)\n",
    "telemetry_df = telem_(telemetry_df,'vibration',24)\n",
    "#telemetry_df=telemetry_df.drop(['volt','rotate','pressure','vibration'],axis=1)\n",
    "telemetry_df=telemetry_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ef9f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "maint_transf_df = pd.get_dummies(maint_df, columns=['comp'])\n",
    "maint_transf_df = telemetry_df.merge(maint_transf_df, on=['datetime', 'machineID'], how='left')\n",
    "maint_transf_df= maint_transf_df[['datetime', 'machineID', 'comp_comp1', 'comp_comp2', 'comp_comp3', 'comp_comp4']]\n",
    "maint_transf_df = maint_transf_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bd151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "maintenance_df = lifespan(maint_transf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c3f93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "maintenance_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f84547",
   "metadata": {},
   "source": [
    "#### Merging the rest of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdad100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fget dummies for errorID\n",
    "error_count = pd.get_dummies(errors_df, columns=['errorID'])\n",
    "error_count.rename(columns={'errorID_error5':'error5count'}, inplace=True)\n",
    "error_count.rename(columns={'errorID_error4':'error4count'}, inplace=True)\n",
    "error_count.rename(columns={'errorID_error3':'error3count'}, inplace=True)\n",
    "error_count.rename(columns={'errorID_error2':'error2count'}, inplace=True)\n",
    "error_count.rename(columns={'errorID_error1':'error1count'}, inplace=True)\n",
    "\n",
    "features = telemetry_df.merge(error_count, on=['datetime', 'machineID'], how='left')\n",
    "\n",
    "# Propagate the error information per error type\n",
    "features[['error1count','error2count','error3count','error4count','error5count']] = features[['error1count','error2count','error3count','error4count','error5count']].fillna(method='ffill')\n",
    "# Fill the iinital error count with 0\n",
    "features = features.fillna(0)\n",
    "\n",
    "# turn \"model\" variable into dummy variables\n",
    "machines_df['model'] = machines_df['model'].astype('category')\n",
    "machines_dummy = pd.get_dummies(machines_df, drop_first=False)\n",
    "\n",
    "# Add the machine metadata information\n",
    "features = features.merge(machines_df[['machineID','model']], on=['machineID'], how='left')\n",
    "features = features.merge(machines_dummy, on=['machineID'], how='left')\n",
    "features = features.merge(maintenance_df, on=['datetime', 'machineID'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023436ef",
   "metadata": {},
   "source": [
    "Merge the failures dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5592b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "fails = pd.get_dummies(failures_df,columns=['failure'])\n",
    "#fails.rename(columns={'failure_comp1':'comp1'}, inplace=True)\n",
    "#fails.rename(columns={'failure_comp2':'comp2'}, inplace=True)\n",
    "#fails.rename(columns={'failure_comp3':'comp3'}, inplace=True)\n",
    "#fails.rename(columns={'failure_comp4':'comp4'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69342e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lifespan_fails(comp_rep0):\n",
    "    comp_rep=comp_rep0.copy()\n",
    "    points = comp_rep['machineID'].unique()\n",
    "    final=pd.DataFrame()\n",
    "\n",
    "    for i in points:\n",
    "        df = comp_rep[(comp_rep['machineID']==i)][['datetime','machineID','failure_comp1','failure_comp2','failure_comp3','failure_comp4']]\n",
    "        for comp in ['failure_comp1','failure_comp2','failure_comp3','failure_comp4']:\n",
    "            life=comp.split('_')[1]+'_life'\n",
    "#             prob=comp+'_prob'\n",
    "#             probkm=comp+'_probkm'\n",
    "            df[life] = df.apply(lambda row: row['datetime'] if row[comp]==0 else np.nan, axis=1)\n",
    "            df[df[life].isna()==False].index\n",
    "            df[life].fillna(method='backfill', inplace=True)\n",
    "            df[life] = pd.to_datetime(df[life]) - df['datetime']\n",
    "            df[life] = df[life].apply(lambda row: row.total_seconds()/86400)\n",
    "#             df[prob] = df[life]/(df[life]+df[comp])\n",
    "#             df[prob].fillna(1, inplace=True)\n",
    "#             df[probkm] = 1-1/(df[life]+df[comp])\n",
    "            #back fill with pad\n",
    "#             df[probkm]=df[probkm].replace(-np.inf, np.nan)\n",
    "            \n",
    "        final=pd.concat([final,df],axis=0)\n",
    "    return final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ce270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fails_ = telemetry_df.merge(fails, on=['datetime', 'machineID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8599f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fails_ = fails_.fillna(method='bfill', limit=24) # fill backward up to 24h if all data, otherwise it must be  7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07adcd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "fails_transf = lifespan_fails(fails_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6c7774",
   "metadata": {},
   "outputs": [],
   "source": [
    "fails_transf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da269f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10854d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the individual components that fail , the columns with 0 or 1, after the merge and backfill\n",
    "fails_transf = fails_transf.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f218cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features = features.merge(fails_transf, on=['datetime', 'machineID'], how='left')\n",
    "# labeled_features = labeled_features.fillna(method='bfill', limit=24) # fill backward up to 24h if all data, otherwise it must be  7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e9321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert \"failure\" target variables into multiple binary targets \n",
    "# # i.e. one per component indicating failure/no failure\n",
    "# labeled_features['comp1_fail'] = (labeled_features['failure'] == 'comp1').astype(int)\n",
    "# labeled_features['comp2_fail'] = (labeled_features['failure'] == 'comp2').astype(int)\n",
    "# labeled_features['comp3_fail'] = (labeled_features['failure'] == 'comp3').astype(int)\n",
    "# labeled_features['comp4_fail'] = (labeled_features['failure'] == 'comp4').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cd6f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccf6a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82891c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20a218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features.to_csv('../data/processed/labeled_1h_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b787046",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87d28dc",
   "metadata": {},
   "source": [
    "Machine age - machine age by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3c3fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxh_groupby(df, feature_name, by):\n",
    "    \"\"\"\n",
    "    Box plot with groupby\n",
    "    \n",
    "    df: DataFrame\n",
    "    feature_name: Name of the feature to be plotted\n",
    "    by: Name of the feature based on which groups are created\n",
    "    \"\"\"\n",
    "    df.boxplot(column=feature_name, by=by, vert=False, \n",
    "                              figsize=(10, 6))\n",
    "    plt.title(f'Distribution of {feature_name} by {by}')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_histogram(df, attribute, title_name, bins, figsize=(9,3), alpha=1, label=None):\n",
    "    df[attribute].plot(kind='hist', \n",
    "                              bins=bins, \n",
    "                              figsize=figsize,\n",
    "                              alpha=alpha,\n",
    "                              label=label,\n",
    "                              title=f'{title_name.title()} distribution')\n",
    "    \n",
    "def plot_bar_sortvals(df, attribute, title, figsize=(5,5)):\n",
    "    df[attribute].value_counts(\n",
    "                normalize=False, dropna=False).sort_values().plot(\n",
    "                kind='bar', figsize=figsize, grid=True,\n",
    "                title=title)\n",
    "\n",
    "    \n",
    "def plot_scatter(df, x_axis_attr, y_axis_attr, figsize=(5,5), title=None, legend=None):\n",
    "    df.plot.scatter(x_axis_attr, y_axis_attr, \n",
    "                    figsize=figsize, title=title, \n",
    "                    legend=legend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa842fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features.boxplot(column='age', vert=False, figsize=(5,3))\n",
    "plt.title('Age distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8421c49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features.boxplot(column='age', by='model', vert=False, figsize=(8,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba7cf1f",
   "metadata": {},
   "source": [
    "```\n",
    "We can see that `model 4` has the lowest median age\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f521edcc",
   "metadata": {},
   "source": [
    "Failure per component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f8b5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_sortvals(failures_df, 'failure', 'Number of failures per component')\n",
    "plt.ylabel('Failures')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ca890e",
   "metadata": {},
   "source": [
    "Telemetry data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b4e1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(labeled_features, 'volt', 'Voltage', bins=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c078fa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(labeled_features, 'rotate', 'RPM', bins=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236abe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(labeled_features, 'pressure', 'Pressure', bins=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d242ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(labeled_features, 'vibration', 'Vibration', bins=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdb5d03",
   "metadata": {},
   "source": [
    "```\n",
    "All the telemetry data, taking into consideration all machines, look to be normally distributed. The odd one being rotation, which looks a skewed on the left.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93caac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in labeled_features.groupby(['model']):\n",
    "    plot_histogram(model[1], 'volt', 'Voltage',bins=300, alpha=0.3, label=model[0])\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e58888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in labeled_features.groupby(['model']):\n",
    "    plot_histogram(model[1], 'rotate', 'RPM',bins=300, alpha=0.3, label=model[0])\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52339df4",
   "metadata": {},
   "source": [
    "#### Create a temporary dataframe, which is the `labeled_features` dataframe used in training, but augmented with time features:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474ae189",
   "metadata": {},
   "source": [
    "Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b71d930",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_sortvals(errors_df, 'errorID', title='Number of error per type')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44919d20",
   "metadata": {},
   "source": [
    "```\n",
    "The most common error type is 1, while the least common is error 5.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e018b5c2",
   "metadata": {},
   "source": [
    "Plot age vs errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc03edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_per_machine = errors_df.groupby(\"machineID\").size()\n",
    "errors_per_machine = pd.DataFrame(errors_per_machine, columns=[\"num_errors\"]).reset_index()\n",
    "\n",
    "machines_fail_data = pd.merge(machines_df, errors_per_machine, how='left', on=\"machineID\")\n",
    "\n",
    "maint_per_machine = maint_df.groupby(\"machineID\").size()\n",
    "maint_per_machine = pd.DataFrame(maint_per_machine, columns=[\"num_maint\"]).reset_index()\n",
    "\n",
    "machines_fail_data = pd.merge(machines_fail_data, maint_per_machine, how='left', on=\"machineID\")\n",
    "\n",
    "failure_per_machine = failures_df.groupby(\"machineID\").size()\n",
    "failure_per_machine = pd.DataFrame(failure_per_machine, columns=[\"num_failure\"]).reset_index()\n",
    "\n",
    "machines_fail_data = pd.merge(machines_fail_data, failure_per_machine, how='left', on=\"machineID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a6e3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "machines_fail_data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011e0599",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(machines_fail_data, \"age\", \"num_errors\", \n",
    "             title=\"Age versus number of errors\")\n",
    "\n",
    "plot_scatter(machines_fail_data, \"age\", \"num_failure\", \n",
    "             title=\"Age versus number of failures\")\n",
    "\n",
    "plot_scatter(machines_fail_data, \"age\", \"num_maint\", \n",
    "             title=\"Age versus total number of\\ncomponent maintenance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2c1223",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = machines_fail_data.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm').set_precision(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1087117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_features = labeled_features.copy()\n",
    "augmented_features['month'] = augmented_features.datetime.dt.month\n",
    "augmented_features['week_of_year'] = augmented_features.datetime.dt.isocalendar().week\n",
    "augmented_features['hour'] = augmented_features.datetime.dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4061f240",
   "metadata": {},
   "source": [
    "Failures per machine per component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1860527",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = failures_df.groupby([\"machineID\", \"failure\"]).size().reset_index()\n",
    "temp_df.columns = [\"machineID\", \"comp\", \"num_fail\"]\n",
    "temp_df_pivot = pd.pivot(temp_df, index=\"machineID\", columns=\"comp\", values=\"num_fail\").rename_axis(None, axis=1)\n",
    "\n",
    "temp_df_pivot.plot.bar(stacked=True, figsize=(20, 6), title=\"Count of failures per component for different Machines\")\n",
    "plt.xlabel(\"Machine ID\")\n",
    "plt.ylabel(\"Number of components that failed\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0398d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_features['month'].value_counts(\n",
    "                normalize=False, dropna=False).sort_values().plot(\n",
    "                kind='bar', figsize=(5,5), grid=True,\n",
    "                title='Number of error per type')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b56ad53",
   "metadata": {},
   "source": [
    "### data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b384e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features = pd.read_csv('../data/processed/labeled_1h_data.csv',  index_col='datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d27bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c0fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6606d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77036eb6",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c54aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(data, test_size):\n",
    "    \n",
    "    machines = machines_df\n",
    "    \n",
    "    x_train_,x_test_,y_train_,y_test_,train_idx,test_idx = train_test_split(machines, machines.model, machines.machineID.index, test_size=test_size, stratify=machines.model, random_state=42)\n",
    "    \n",
    "    training=data[data['machineID'].isin(train_idx)]\n",
    "    training=training.drop(columns=['machineID']).to_numpy()\n",
    "    test=data[data['machineID'].isin(test_idx)]\n",
    "    test=test.drop(columns=['machineID']).to_numpy()\n",
    "    \n",
    "    x_train=training[:,:-1]\n",
    "    y_train=training[:,-1]\n",
    "    x_test=test[:,:-1]\n",
    "    y_test=test[:,-1]\n",
    "\n",
    "    return x_train,x_test,y_train,y_test\n",
    "\n",
    "def preprocess(X_init, test_size, model:str, classi=False, splitting=True):\n",
    "\n",
    "        if splitting:\n",
    "            X_train_unsc, X_test_unsc, y_train_unsc, y_test_unsc =split(X_init,test_size=test_size)\n",
    "            X_init = X_init.drop(columns=['machineID']).to_numpy()\n",
    "            y = X_init[:,-1]\n",
    "            X = X_init[:,:-1]\n",
    "        else:\n",
    "            X_init = X_init.drop(columns=['machineID']).to_numpy()\n",
    "            y = X_init[:,-1]\n",
    "            X = X_init[:,:-1]\n",
    "            X_train_unsc, X_test_unsc, y_train_unsc, y_test_unsc = train_test_split(X, y, test_size=test_size, random_state=0)\n",
    "            \n",
    "        #print(X_train_unsc.shape, y_train_unsc.shape, X_test_unsc.shape, y_test_unsc.shape)\n",
    "        \n",
    "        X_mean = X_train_unsc.mean(axis=0)\n",
    "        X_std = X_train_unsc.std(axis=0)\n",
    "\n",
    "        y_std = y_train_unsc.std()\n",
    "        y_mean = y_train_unsc.mean()\n",
    "\n",
    "        X_train = (X_train_unsc - X_mean)/ X_std\n",
    "        X_test = (X_test_unsc - X_mean)/X_std\n",
    "\n",
    "        y_train = (y_train_unsc- y_mean)/ y_std \n",
    "        y_test = (y_test_unsc- y_mean)/y_std        \n",
    "\n",
    "        X_train_torch = torch.tensor(X_train).float()\n",
    "        \n",
    "        if model == SVI_model_format.poisson['name']:\n",
    "            y_train_torch = torch.tensor(y_train * y_std + y_mean).int()\n",
    "        else:\n",
    "            y_train_torch = torch.tensor(y_train).float()\n",
    "        \n",
    "        if classi:\n",
    "            y_train_torch = torch.tensor(y_train_unsc).float()\n",
    "            y_train = y_train_unsc\n",
    "            y_test = y_test_unsc\n",
    "        X_test_torch = torch.tensor(X_test).float()\n",
    "\n",
    "        return y, X, X_train_torch, y_train_torch,X_test_torch, X_test, y_test, X_train, y_train, y_std, y_mean\n",
    "\n",
    "    \n",
    "def linear_model(X, obs=None):\n",
    "        \n",
    "    alpha = pyro.sample(\"alpha\", dist.Normal(0., 1.))                   # Prior for the bias/intercept\n",
    "    beta  = pyro.sample(\"beta\", dist.Normal(torch.zeros(X.shape[1]), \n",
    "                                            torch.ones(X.shape[1])).to_event())    # Priors for the regression coeffcients\n",
    "    sigma = pyro.sample(\"sigma\", dist.HalfCauchy(5.))                   # Prior for the variance\n",
    "    \n",
    "    with pyro.plate(\"data\"):\n",
    "        y = pyro.sample(\"y\", dist.Normal(alpha + X.matmul(beta), sigma), obs=obs)\n",
    "      \n",
    "    return y\n",
    "\n",
    "\n",
    "def poisson_model(X, obs=None):\n",
    "    alpha = pyro.sample(\"alpha\", dist.Normal(0., 1.))                   # Prior for the bias/intercept\n",
    "    beta  = pyro.sample(\"beta\", dist.Normal(torch.zeros(X.shape[1]), \n",
    "                                            torch.ones(X.shape[1])).to_event())    # Priors for the regression coeffcients\n",
    "    \n",
    "    with pyro.plate(\"data\"):\n",
    "        y = pyro.sample(\"y\", dist.Poisson(torch.exp(alpha + X.matmul(beta))), obs=obs)\n",
    "        \n",
    "    return y\n",
    "\n",
    "\n",
    "def heteroscedastic_model(X, obs=None):\n",
    "    alpha_mu = pyro.sample(\"alpha_mu\", dist.Normal(0., 1.))                 # Prior for the bias/intercept of the mean\n",
    "    beta_mu  = pyro.sample(\"beta_mu\", dist.Normal(torch.zeros(X.shape[1]), \n",
    "                                               torch.ones(X.shape[1])).to_event())     # Priors for the regression coeffcients of the mean\n",
    "    alpha_v = pyro.sample(\"alpha_v\", dist.Normal(0., 1.))                   # Prior for the bias/intercept of the variance\n",
    "    beta_v  = pyro.sample(\"beta_v\", dist.Normal(torch.zeros(X.shape[1]), \n",
    "                                               torch.ones(X.shape[1])).to_event())     # Priors for the regression coeffcients of the variance\n",
    "    \n",
    "    with pyro.plate(\"data\"):\n",
    "        y = pyro.sample(\"y\", dist.Normal(alpha_mu + X.matmul(beta_mu), torch.exp(alpha_v + X.matmul(beta_v))), obs=obs)\n",
    "        \n",
    "    return y\n",
    "\n",
    "def compute_error(trues: np.array, predicted: np.array, threshold: int):\n",
    "    if threshold:\n",
    "        predicted_thres = predicted[np.where(trues<threshold)]\n",
    "        trues_thres  = trues[np.where(trues<threshold)[0]]\n",
    "    else:\n",
    "        print('No threshold')\n",
    "        pass\n",
    "        \n",
    "    corr = np.corrcoef(predicted, trues)[0,1]\n",
    "    mae = np.mean(np.abs(predicted - trues))\n",
    "    rae = np.sum(np.abs(predicted - trues)) / np.sum(np.abs(trues - np.mean(trues)))\n",
    "    rmse = np.sqrt(np.mean((predicted - trues)**2))\n",
    "    r2 = max(0, 1 - np.sum((trues-predicted)**2) / np.sum((trues - np.mean(trues))**2))\n",
    "    try:\n",
    "        return corr, mae, rae, rmse, r2, predicted_thres, trues_thres\n",
    "    except:\n",
    "         return corr, mae, rae, rmse, r2, predicted, trues\n",
    "        \n",
    "def results(dataset: pd.DataFrame, features_importance: np.array, results_dict: Dict, model:str, comp_number: int, y_trues:np.array, y_preds:np.array) -> Dict:\n",
    "        \n",
    "    dataset.drop(columns='machineID', inplace=True, axis=1)\n",
    "  \n",
    "    corr, mae, rae, rmse, r2, svi_trues, svi_pred = compute_error(trues=y_trues, predicted=y_preds, threshold=None)\n",
    "\n",
    "    results_dict[model][f\"comp_{comp_number}\"]['MAE'] = mae\n",
    "    \n",
    "    print(\"CorrCoef: %.3f\\nMAE: %.3f\\nRMSE: %.3f\\nR2: %.3f\" % (corr, mae, rmse, r2))\n",
    "    \n",
    "    sort = features_importance.argsort()\n",
    "    \n",
    "    feautures_dict = dict(zip(dataset.columns[sort].tolist()[0], features_importance[0][sort].tolist()[0]))\n",
    "    \n",
    "    results_dict[model][f\"comp_{comp_number}\"]['FI'] = list(feautures_dict.keys())[list(feautures_dict.values()).index(max(list(feautures_dict.values())) )]\n",
    "    plt.barh(dataset.columns[sort].tolist()[0], features_importance[0][sort].tolist()[0])\n",
    "    plt.xlabel(\"Feature Importance\")\n",
    "    plt.show()\n",
    "  \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca31e841",
   "metadata": {},
   "source": [
    "### SVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d90e0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVI_model_format():\n",
    "    '''\n",
    "    defining the type of the regression\n",
    "    '''\n",
    "    poisson = {'name':\"POISSON\", 'model':poisson_model}\n",
    "    heterosc = {'name':\"HETEROSCEDASTIC\", 'model':heteroscedastic_model}\n",
    "    linear = {'name':\"LINEAR\", 'model':linear_model}\n",
    "\n",
    "class SVI_regression_model():\n",
    "    '''\n",
    "    SVI regression: choosing the correct data, preprocess them, pyro inference and prediction\n",
    "    '''\n",
    "    def __init__(self, data: pd.DataFrame, component: int) -> None:\n",
    "        \n",
    "        self.data = data\n",
    "        self.component = component\n",
    "        print(self.component)\n",
    "\n",
    "    def get_data_for_component(self) -> pd.DataFrame:\n",
    "        \n",
    "        '''\n",
    "        returns the feautures of the dataset and the component we want to predict for\n",
    "        '''\n",
    "        \n",
    "        components_cols = ['comp1_maint', 'comp2_maint', 'comp3_maint', 'comp4_maint',\n",
    "                           'failure_comp1', 'failure_comp2', 'failure_comp3', 'failure_comp4',\n",
    "                           'comp1_life', 'comp2_life', 'comp3_life', 'comp4_life']\n",
    "    \n",
    "        cols = ['machineID', 'voltmean_3h', 'rotatemean_3h', 'pressuremean_3h', 'vibrationmean_3h',\n",
    "                'voltsd_3h', 'rotatesd_3h', 'pressuresd_3h', 'vibrationsd_3h', 'voltmean_24h',\n",
    "                'rotatemean_24h', 'pressuremean_24h', 'vibrationmean_24h', 'voltsd_24h', 'rotatesd_24h',\n",
    "                'pressuresd_24h', 'vibrationsd_24h', 'error1count', 'error2count','error3count',\n",
    "                'error4count', 'error5count','age', 'model_model1', 'model_model2', \n",
    "                'model_model3', 'model_model4'] + [word for word in components_cols if str(self.component) in ([*word]) and 'failure' not in word.split('_')]\n",
    "\n",
    "\n",
    "        return self.data[cols]\n",
    "    \n",
    "    def Preprocess(self, X_init: pd.DataFrame, model: str) -> \\\n",
    "        Tuple[np.ndarray, np.ndarray,  torch.tensor,  torch.tensor, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.float64, np.float64]:\n",
    "\n",
    "        '''\n",
    "        the necessary data preprocess before procceding in pyro inference\n",
    "        '''\n",
    "        print(f\"{model} Regression {self.component}\")\n",
    "        print(X_init.columns)\n",
    "        print('\\n-----------------------------------------------------------------------')\n",
    "\n",
    "        y, X, X_train_torch, y_train_torch, X_test_torch, X_test, y_test, X_train, y_train, y_std, y_mean = preprocess(X_init, 0.1, model, classi=False, splitting=True)\n",
    " \n",
    "        return y, X, X_train_torch, y_train_torch, X_test_torch, X_test, y_test, X_train, y_train, y_std, y_mean\n",
    "        \n",
    "    def pyro_inference(self, X_train_torch: torch.tensor, y_train_torch: torch.tensor, model: object, steps: int):\n",
    "        '''\n",
    "        pyro inference\n",
    "        '''\n",
    "        \n",
    "        # Reset parameter values\n",
    "        pyro.clear_param_store()\n",
    "\n",
    "        # Define guide function\n",
    "        guide = AutoDiagonalNormal(model)\n",
    "\n",
    "        # Define the number of optimization steps\n",
    "        n_steps = steps\n",
    "\n",
    "        # Setup the optimizer\n",
    "        adam_params = {\"lr\": 0.0001} # learning rate (lr) of optimizer\n",
    "        optimizer = ClippedAdam(adam_params)\n",
    "\n",
    "        # Setup the inference algorithm\n",
    "        elbo = Trace_ELBO(num_particles=1)\n",
    "        svi = SVI(model, guide, optimizer, loss=elbo)\n",
    "\n",
    "        # Do gradient steps\n",
    "        for step in range(n_steps):\n",
    "            elbo = svi.step(X_train_torch, y_train_torch)\n",
    "            if step % 100 == 0:\n",
    "                print(\"[%d] ELBO: %.1f\" % (step, elbo))\n",
    "                \n",
    "        return guide\n",
    "    \n",
    "    def post_process(self, guide: object, model: object, X_train_torch: torch.tensor, y_train_torch: torch.tensor, X_test: np.ndarray, y_test: np.ndarray, y_std: np.float64, y_mean: np.float64):\n",
    "\n",
    "        if model['name'] != SVI_model_format.heterosc['name']:\n",
    "            predictive = Predictive(model=model['model'], guide=guide, num_samples=1000, return_sites=(\"alpha\", \"beta\", \"sigma\"))\n",
    "            samples = predictive(X_train_torch, y_train_torch)\n",
    "\n",
    "            alpha_samples = samples[\"alpha\"].detach().numpy()\n",
    "            beta_samples = samples[\"beta\"].detach().numpy()\n",
    "\n",
    "            y_hat = np.mean(np.exp(alpha_samples.T + np.dot(X_test, beta_samples[:,0].T)), axis=1)\n",
    "\n",
    "        else:\n",
    "            predictive = Predictive(model=model['model'], guide=guide, num_samples=1000,\n",
    "                        return_sites=(\"alpha_mu\", \"beta_mu\", \"alpha_v\", \"beta_v\"))\n",
    "            samples = predictive(X_train_torch, y_train_torch)\n",
    "\n",
    "            alpha_samples = samples[\"alpha_mu\"].detach().numpy()\n",
    "            beta_samples = samples[\"beta_mu\"].detach().numpy()\n",
    "\n",
    "            y_hat = np.mean(alpha_samples.T + np.dot(X_test, beta_samples[:,0].T), axis=1)\n",
    "            \n",
    "        # Calculate feature importance\n",
    "        feature_importance = np.mean(np.abs(beta_samples), axis=0)\n",
    "        feature_importance /= np.sum(feature_importance)\n",
    "\n",
    "        # convert back to the original scale\n",
    "        if model['name'] == SVI_model_format.poisson['name']:\n",
    "            preds = y_hat # no need to do any conversion here because the Poisson model received untransformed y's\n",
    "        else:\n",
    "            preds = preds * y_std + y_mean\n",
    "            \n",
    "        y_true = y_test * y_std + y_mean\n",
    "\n",
    "        return preds, y_true, feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41231a9",
   "metadata": {},
   "source": [
    "#### Poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96dabb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_results = {'poisson': {'comp_1':  {'MAE':0, 'FI':''}, 'comp_2':  {'MAE':0, 'FI':''}, 'comp_3':  {'MAE':0, 'FI':''}, 'comp_4':  {'MAE':0, 'FI':''}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd5a4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_results = {\"poisson\": {'comp_1':  {'MAE':0, 'FI':''}, 'comp_2':  {'MAE':0, 'FI':''}, 'comp_3':  {'MAE':0, 'FI':''}, 'comp_4':  {'MAE':0, 'FI':''}}}\n",
    "\n",
    "for comp_number in range(1,5):\n",
    "    print(f\"\\n--------------------------------  component {comp_number}  ---------------------------------------\")\n",
    "    svi_regression = SVI_regression_model(labeled_features, comp_number)\n",
    "    svi_dataset = svi_regression.get_data_for_component()\n",
    "    poisson = SVI_model_format.poisson \n",
    "    \n",
    "    y_poisson, X_poisson, X_train_torch_poisson, y_train_torch_poisson, X_test_torch_poisson, X_test_poisson, y_test_poisson, X_train_poisson, y_train_poisson, y_std_poisson, y_mean_poisson = svi_regression.Preprocess(X_init=svi_dataset, model=poisson['name'])\n",
    "    poisson_guide = svi_regression.pyro_inference(X_train_torch=X_train_torch_poisson, y_train_torch=y_train_torch_poisson, model=poisson['model'], steps=100)\n",
    "    poisson_preds, poisson_y_true, features_importance = svi_regression.post_process(guide=poisson_guide, model=poisson, X_train_torch=X_train_torch_poisson, y_train_torch=y_train_torch_poisson, X_test=X_test_poisson, y_test=y_test_poisson, y_std=y_std_poisson, y_mean=y_mean_poisson)\n",
    "    \n",
    "    poisson_results = results(svi_dataset, features_importance, poisson_results, 'poisson', comp_number, poisson_y_true, poisson_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1134b886",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "poisson_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46f46fe",
   "metadata": {},
   "source": [
    "#### Heteorscedastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27e2351",
   "metadata": {},
   "outputs": [],
   "source": [
    "hetero_results = {'hetero': {'comp_1': {'MAE':0}, 'comp_2': {'MAE':0}, 'comp_3': {'MAE':0}, 'comp_4': {'MAE':0}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7aa62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for comp_number in range(1,5):\n",
    "    print(f\"\\n--------------------------------  component {comp_number}  ---------------------------------------\")\n",
    "    svi_regression = SVI_regression_model(labeled_features, comp_number)\n",
    "    svi_dataset = svi_regression.get_data_for_component()\n",
    "    hetero = SVI_model_format.heterosc\n",
    "    \n",
    "    y_hetero, X_hetero, X_train_torch_hetero, y_train_torch_hetero, X_test_torch_hetero, X_test_hetero, y_test_hetero, X_train_hetero, y_train_hetero, y_std_hetero, y_mean_hetero = svi_regression.Preprocess(X_init=svi_dataset, model=hetero['name'])\n",
    "   \n",
    "    hetero_guide = svi_regression.pyro_inference(X_train_torch=X_train_torch_hetero, y_train_torch=y_train_torch_hetero, model=hetero['model'], steps=10000)\n",
    "   \n",
    "    hetero_preds, hetero_y_true, features_importance = svi_regression.post_process(guide=hetero_guide, model=hetero, X_train_torch=X_train_torch_hetero, y_train_torch=y_train_torch_hetero, X_test=X_test_hetero, y_test=y_test_hetero, y_std=y_std_hetero, y_mean=y_mean_hetero)\n",
    "    corr_hetero, mae_hetero, rae_hetero, rmse_hetero, r2_hetero, svi_trues_hetero, svi_pred_hetero = compute_error(trues=hetero_y_true, predicted=hetero_preds, threshold=None)\n",
    "    hetero_results['hetero'][f\"comp_{comp_number}\"]['MAE'] = mae_hetero\n",
    "    print(\"CorrCoef: %.3f\\nMAE: %.3f\\nRMSE: %.3f\\nR2: %.3f\" % (corr_hetero, mae_hetero, rmse_hetero, r2_hetero))\n",
    "    sort = features_importance.argsort()\n",
    "    plt.barh(svi_dataset.columns[sort].tolist()[0], features_importance[0][sort].tolist()[0])\n",
    "    plt.xlabel(\"Feature Importance\")\n",
    "    plt.show()\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68981060",
   "metadata": {},
   "outputs": [],
   "source": [
    "hetero_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55829176",
   "metadata": {},
   "source": [
    "#### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4f7b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svi_results = {'linear_svi': {'comp_1': {'MAE':0}, 'comp_2': {'MAE':0}, 'comp_3': {'MAE':0}, 'comp_4': {'MAE':0}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05afa903",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svi_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75397c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for comp_number in range(1,5):\n",
    "    print(f\"\\n--------------------------------  component {comp_number}  ---------------------------------------\")\n",
    "    svi_regression = SVI_regression_model(labeled_features, comp_number)\n",
    "    svi_dataset = svi_regression.get_data_for_component()\n",
    "    linear_svi = SVI_model_format.linear\n",
    " \n",
    "    y_linear_svi, X_linear_svi, X_train_torch_linear_svi, y_train_torch_linear_svi,  X_test_torch_linear_svi, X_test_linear_svi, y_test_linear_svi, X_train_linear_svi, y_train_linear_svi, y_std_linear_svi, y_mean_linear_svi = svi_regression.Preprocess(X_init=svi_dataset, model=linear_svi['name'])\n",
    "  \n",
    "    linear_svi_guide = svi_regression.pyro_inference(X_train_torch=X_train_torch_linear_svi, y_train_torch=y_train_torch_linear_svi, model=linear_svi['model'], steps=1000)\n",
    "    linear_svi_preds, linear_svi_y_true, features_importance = svi_regression.post_process(guide=linear_svi_guide, model=linear_svi, X_train_torch=X_train_torch_linear_svi, y_train_torch=y_train_torch_linear_svi, X_test=X_test_linear_svi, y_test=y_test_linear_svi, y_std=y_std_linear_svi, y_mean=y_mean_linear_svi)\n",
    "    \n",
    "    corr_linear_svi, mae_linear_svi, rae_linear_svi, rmse_linear_svi, r2_linear_svi, svi_trues_linear_svi, svi_pred_linear_svi = compute_error(trues=linear_svi_y_true, predicted=linear_svi_preds, threshold=None)\n",
    "    linear_svi_results['linear_svi'][f\"comp_{comp_number}\"]['MAE'] = mae_linear_svi\n",
    "    #linear_svi_results['linear_svi'][f\"comp_{comp_number}\"]['MIF'] = [svi_dataset.columns[sort].tolist()[0][-1], features_importance[0][sort].tolist()[0][-1]]\n",
    "    print(\"CorrCoef: %.3f\\nMAE: %.3f\\nRMSE: %.3f\\nR2: %.3f\" % (corr_linear_svi, mae_linear_svi, rmse_linear_svi, r2_linear_svi))\n",
    "    #print(features_importance)\n",
    "    sort = features_importance.argsort()\n",
    "    plt.barh(svi_dataset.columns[sort].tolist()[0], features_importance[0][sort].tolist()[0])\n",
    "    plt.xlabel(\"Feature Importance\")\n",
    "    plt.show()\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e101fc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svi_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918eeb21",
   "metadata": {},
   "source": [
    "### MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85da0c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCMC_model_format():\n",
    "    '''\n",
    "    defining the type of the regression\n",
    "    '''\n",
    "    poisson = {'name':\"POISSON\", 'model':poisson_model}\n",
    "    heterosc = {'name':\"HETEROSCEDASTIC\", 'model':heteroscedastic_model}\n",
    "    linear = {'name':\"LINEAR\", 'model':linear_model}\n",
    "\n",
    "class MCMC_regression_model():\n",
    "\n",
    "    def __init__(self, data: pd.DataFrame, component: int) -> None:\n",
    "        \n",
    "        self.data = data\n",
    "        self.component = component\n",
    "        print(self.component)\n",
    "\n",
    "    def get_data_for_component(self) -> pd.DataFrame:\n",
    "        \n",
    "        '''\n",
    "        returns the feautures of the dataset and the component we want to predict for\n",
    "        '''\n",
    "    \n",
    "        components_cols = ['comp1_maint', 'comp2_maint', 'comp3_maint', 'comp4_maint',\n",
    "       'failure_comp1', 'failure_comp2', 'failure_comp3', 'failure_comp4',\n",
    "       'comp1_life', 'comp2_life', 'comp3_life', 'comp4_life']\n",
    "        \n",
    "        cols = ['machineID', 'voltmean_3h', 'rotatemean_3h',\n",
    "                'pressuremean_3h', 'vibrationmean_3h', 'voltsd_3h', 'rotatesd_3h',\n",
    "                'pressuresd_3h', 'vibrationsd_3h', 'voltmean_24h', 'rotatemean_24h',\n",
    "                'pressuremean_24h', 'vibrationmean_24h', 'voltsd_24h', 'rotatesd_24h',\n",
    "                'pressuresd_24h', 'vibrationsd_24h', 'error1count', 'error2count',\n",
    "                'error3count', 'error4count', 'error5count','age',\n",
    "                'model_model1', 'model_model2', 'model_model3', 'model_model4'] + [word for word in components_cols if str(self.component) in ([*word]) and 'failure' not in word.split('_')]\n",
    "\n",
    "\n",
    "        return self.data[cols]\n",
    "    \n",
    "    def Preprocess(self, X_init: pd.DataFrame, model: str) -> \\\n",
    "        Tuple[np.ndarray, np.ndarray,  torch.tensor,  torch.tensor, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.float64, np.float64]:\n",
    "\n",
    "        '''\n",
    "        the necessary data preprocess before procceding in pyro inference\n",
    "        '''\n",
    "        print(f\"{model} Regression {self.component}\")\n",
    "        print(X_init.columns)\n",
    "        print('\\n-----------------------------------------------------------------------')\n",
    "\n",
    "        y, X, X_train_torch, y_train_torch, X_test_torch, X_test, y_test, X_train, y_train, y_std, y_mean = preprocess(X_init,0.1,model,classi=False,splitting=True)\n",
    " \n",
    "        return y, X, X_train_torch, y_train_torch, X_test_torch, X_test, y_test, X_train, y_train, y_std, y_mean\n",
    "    \n",
    "\n",
    "    def pyro_inference(self, X_train_torch, y_train_torch, model, num_samples):\n",
    "\n",
    "        # Reset parameter values\n",
    "        pyro.clear_param_store()\n",
    "\n",
    "        # Run inference in Pyro\n",
    "        nuts_kernel = NUTS(model)\n",
    "        \n",
    "        mcmc = MCMC(nuts_kernel, num_samples=num_samples, warmup_steps=200, num_chains=1)\n",
    "        mcmc.run(X_train_torch, y_train_torch)\n",
    "        \n",
    "        # Compute feature importance\n",
    "        feature_names = self.data.columns[1:-1]\n",
    "        print(feature_names)\n",
    "        \n",
    "        feature_importance = {}\n",
    "\n",
    "        for feature in feature_names:\n",
    "            marginal = EmpiricalMarginal(mcmc.get_samples(), sites=feature)\n",
    "            feature_importance[feature] = torch.std(marginal._get_samples_and_weights()[0]).item()\n",
    "\n",
    "        print(\"Feature Importance:\")\n",
    "        for feature, importance in feature_importance.items():\n",
    "            print(f\"{feature}: {importance}\")\n",
    "\n",
    "        # Show summary of inference results\n",
    "        print(mcmc.summary())\n",
    "\n",
    "        return mcmc.get_samples()\n",
    "\n",
    "    def post_process(self, X_test, X_train, samples, y_std, y_mean, y_test):\n",
    "\n",
    "        posterior_samples = samples\n",
    "\n",
    "        # Compute predictions\n",
    "        y_hat = np.mean(posterior_samples[\"alpha\"].numpy().T + np.dot(X_test, posterior_samples[\"beta\"].numpy().T), axis=1)\n",
    "        y_hat_train = np.mean(posterior_samples[\"alpha\"].numpy().T + np.dot(X_train, posterior_samples[\"beta\"].numpy().T), axis=1)\n",
    "\n",
    "        # Convert back to the original scale\n",
    "        preds = y_hat * y_std + y_mean\n",
    "        preds_train = y_hat_train * y_std + y_mean\n",
    "        y_true = y_test * y_std + y_mean\n",
    "\n",
    "        return preds, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5e50bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_mcmc_results = {'linear_mcmc': {'comp_1': 0, 'comp_2':0, 'comp_3': 0, 'comp_4':0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194898c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for comp_number in range(1,5):\n",
    "    print(f\"\\n--------------------------------  component {comp_number}  ---------------------------------------\")\n",
    "    mcmc_regression = MCMC_regression_model(labeled_features, comp_number)\n",
    "    mcmc_dataset = mcmc_regression.get_data_for_component()\n",
    "    linear_mcmc = MCMC_model_format.linear\n",
    " \n",
    "\n",
    "    y_linear_mcmc, X_linear_mcmc, X_train_torch_linear_mcmc, y_train_torch_linear_mcmc,  X_test_torch_linear_mcmc, X_test_linear_mcmc, y_test_linear_mcmc, X_train_linear_mcmc, y_train_linear_mcmc, y_std_linear_mcmc, y_mean_linear_mcmc = mcmc_regression.Preprocess(X_init=mcmc_dataset, model=linear_mcmc['name'])\n",
    "  \n",
    "    linear_mcmc_samples = mcmc_regression.pyro_inference(X_train_torch=X_train_torch_linear_mcmc, y_train_torch=y_train_torch_linear_mcmc, model=linear_mcmc['model'], num_samples=100)\n",
    "    linear_mcmc_preds, linear_mcmc_y_true = mcmc_regression.post_process(samples=linear_mcmc_samples, X_train = X_train_linear_mcmc, X_test = X_test_linear_mcmc, y_test=y_test_linear_mcmc, y_std=y_std_linear_mcmc, y_mean=y_mean_linear_mcmc)\n",
    "    \n",
    "    corr_linear_mcmc, mae_linear_mcmc, rae_linear_mcmc, rmse_linear_mcmc, r2_linear_mcmc, mcmc_trues_linear_mcmc, mcmc_pred_linear_mcmc = compute_error(trues=linear_mcmc_y_true, predicted=linear_mcmc_preds, threshold=None)\n",
    "    linear_mcmc_results['linear_mcmc'][f\"comp_{comp_number}\"] = [mae_linear_mcmc]\n",
    "    print(\"CorrCoef: %.3f\\nMAE: %.3f\\nRMSE: %.3f\\nR2: %.3f\" % (corr_linear_mcmc, mae_linear_mcmc, rmse_linear_mcmc, r2_linear_mcmc))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f42cfe",
   "metadata": {},
   "source": [
    "### NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b497cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelFormat():\n",
    "    '''\n",
    "    defining the type of the regression\n",
    "    '''\n",
    "    neural_netwrok = {'name':'NN'}\n",
    "    poisson = {'name':\"POISSON\", 'model':poisson_model}\n",
    "    heterosc = {'name':\"HETEROSCEDASTIC\", 'model':heteroscedastic_model}\n",
    "    linear = {'name':\"LINEAR\", 'model':linear_model}\n",
    "\n",
    "class FFNN(PyroModule):\n",
    "    def __init__(self, n_in, n_hidden, n_out, type_forward):\n",
    "        \n",
    "        self.type_forward = type_forward\n",
    "        super(FFNN, self).__init__()\n",
    "        \n",
    "        # Architecture\n",
    "        self.in_layer = PyroModule[nn.Linear](n_in, n_hidden)\n",
    "        self.in_layer.weight = PyroSample(dist.Normal(0., 1.).expand([n_hidden, n_in]).to_event(2))\n",
    "\n",
    "        self.h_layer = PyroModule[nn.Linear](n_hidden, n_hidden)\n",
    "        self.h_layer.weight = PyroSample(dist.Normal(0., 1.).expand([n_hidden, n_hidden]).to_event(2))\n",
    "\n",
    "        self.h_layer = PyroModule[nn.Linear](n_hidden, n_hidden)\n",
    "        self.h_layer.weight = PyroSample(dist.Normal(0., 1.).expand([n_hidden, n_hidden]).to_event(2))\n",
    "\n",
    "        self.out_layer = PyroModule[nn.Linear](n_hidden, n_out)\n",
    "        self.out_layer.weight = PyroSample(dist.Normal(0., 1.).expand([n_out, n_hidden]).to_event(2))\n",
    "\n",
    "        # Activation functions\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, X, y=None):\n",
    "        \n",
    "        if self.type_forward == 'simple':\n",
    "            X = self.tanh(self.in_layer(X))\n",
    "            X = self.tanh(self.h_layer(X))\n",
    "            X = self.out_layer(X)\n",
    "            prediction_mean = X.squeeze(-1)\n",
    "            with pyro.plate(\"observations\"):\n",
    "                y = pyro.sample(\"obs\", dist.Normal(prediction_mean, 0.1), obs=y)\n",
    "        \n",
    "        elif self.type_forward == 'interpretable':\n",
    "            X_nn = X[:,1:]\n",
    "            X_nn = self.tanh(self.in_layer(X_nn))\n",
    "            X_nn = self.tanh(self.h_layer(X_nn))\n",
    "            X_nn = self.out_layer(X_nn)\n",
    "            nn_out = X_nn.squeeze(-1)\n",
    "\n",
    "            beta_lin = pyro.sample(\"beta\", dist.Normal(0, 1))\n",
    "            X_linear = X[:,0]\n",
    "            with pyro.plate(\"observations\"):\n",
    "                linear_out = X_linear*beta_lin\n",
    "                y = pyro.sample(\"obs\", dist.Normal(nn_out+linear_out, 0.1), obs=y)\n",
    "\n",
    "        return y\n",
    "\n",
    "def train_nn(model0, X_train_torch, y_train_torch):\n",
    "    \n",
    "    if model0 ==\"simple\":\n",
    "        model = FFNN(n_in=X_train_torch.shape[1], n_hidden=32, n_out=1, type_forward=model0)\n",
    "        \n",
    "    elif model0 == \"interpretable\":\n",
    "        model = FFNN(n_in=X_train_torch.shape[1]-1, n_hidden=32, n_out=1, type_forward=model0)\n",
    "        \n",
    "    guide = AutoDiagonalNormal(model)\n",
    "    pyro.clear_param_store()\n",
    "    # Define the number of optimization steps\n",
    "    n_steps = 1000#10000 kanonika!\n",
    "\n",
    "    # Setup the optimizer\n",
    "    adam_params = {\"lr\": 0.01}\n",
    "    optimizer = Adam(adam_params)\n",
    "\n",
    "    # Setup the inference algorithm\n",
    "    elbo = Trace_ELBO(num_particles=1)\n",
    "    svi = SVI(model, guide, optimizer, loss=elbo)\n",
    "\n",
    "    # Do gradient steps\n",
    "    for step in range(n_steps):\n",
    "        elbo = svi.step(X_train_torch,y_train_torch)\n",
    "        if step % 500 == 0:\n",
    "            print(\"[%d] ELBO: %.1f\" % (step, elbo))\n",
    "    \n",
    "    return model, guide\n",
    "\n",
    "def get_data_for_component(data, component) -> pd.DataFrame:\n",
    "        \n",
    "    '''\n",
    "    returns the feautures of the dataset and the component we want to predict for\n",
    "    '''\n",
    "    \n",
    "    components_cols = ['comp1_maint', 'comp2_maint', 'comp3_maint', 'comp4_maint',\n",
    "                       'failure_comp1', 'failure_comp2', 'failure_comp3', 'failure_comp4',\n",
    "                       'comp1_life', 'comp2_life', 'comp3_life', 'comp4_life']\n",
    "        \n",
    "    cols = ['machineID', 'voltmean_3h', 'rotatemean_3h', 'pressuremean_3h', 'vibrationmean_3h',\n",
    "            'voltsd_3h', 'rotatesd_3h', 'pressuresd_3h', 'vibrationsd_3h', 'voltmean_24h',\n",
    "            'rotatemean_24h', 'pressuremean_24h', 'vibrationmean_24h', 'voltsd_24h', 'rotatesd_24h',\n",
    "            'pressuresd_24h', 'vibrationsd_24h', 'error1count', 'error2count', 'error3count',\n",
    "            'error4count', 'error5count','age', 'model_model1', 'model_model2',\n",
    "            'model_model3', 'model_model4'] + [word for word in components_cols if str(component) in ([*word]) and 'failure' not in word.split('_')]\n",
    "\n",
    "\n",
    "    return data[cols]\n",
    "\n",
    "def test_nn(model,guide,X_test_torch):\n",
    "    \n",
    "    # Predict\n",
    "    predictive = pyro.infer.Predictive(model, guide=guide, num_samples=1000,return_sites=(\"obs\", \"_RETURN\"))\n",
    "    samples = predictive(X_test_torch)\n",
    "    y_pred = samples[\"obs\"].mean(axis=0).detach().numpy()\n",
    "    \n",
    "    y_preds = y_pred * y_std + y_mean\n",
    "    y_true = y_test * y_std + y_mean\n",
    "    \n",
    "    return y_preds, y_true\n",
    "\n",
    "def test_nn_beta(model,guide,X_test_torch):\n",
    "    # Predict\n",
    "    predictive = pyro.infer.Predictive(model, guide=guide, num_samples=1000,return_sites=(\"beta\",))\n",
    "    samples = predictive(X_test_torch)\n",
    "    print(\"Estimated beta:\", samples[\"beta\"].mean(axis=0).detach().numpy())\n",
    "\n",
    "def mae_test(y_pred, y_test):\n",
    "\n",
    "    mae = np.mean(np.abs(y_pred - y_test))\n",
    "    print(\"MAE:\", mae)\n",
    "    \n",
    "    return mae\n",
    "\n",
    "def plot_pred(y_pred,y_test,y_std,y_mean,threshold,start=None,end=None):\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "    start = start\n",
    "    end = end\n",
    "    y_true = y_test * y_std + y_mean\n",
    "    y_pre = y_pred * y_std + y_mean\n",
    "\n",
    "    plt.plot(y_true[y_pre>threshold], 'r.-', label='test')\n",
    "    plt.plot(y_pre[y_pre>threshold], 'b-', label='pred')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return y_true,y_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fc75a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for comp_number in range(3,5):\n",
    "\n",
    "    print(f\"\\n--------------------------------  component {comp_number}  ---------------------------------------\")\n",
    "    nn_dataset = get_data_for_component(labeled_features, comp_number)\n",
    "    y, X, X_train_torch, y_train_torch,X_test_torch, X_test, y_test, X_train, y_train, y_std, y_mean = preprocess(nn_dataset, 0.1, 'neural_network')\n",
    "    model_FFNN, guide = train_nn(\"simple\", X_train_torch, y_train_torch)\n",
    "    y_preds, y_true = test_nn(model_FFNN, guide, X_test_torch)\n",
    "#   corr, mae, rae, rmse, r2, trues, predicted=compute_error(y_pred,y_test, y_std, y_mean, False)\n",
    "#   print(\"CorrCoef: %.3f\\nMAE: %.3f\\nRMSE: %.3f\\nR2: %.3f\" % (corr, mae, rmse, r2))\n",
    "    mae = mae_test(y_preds, y_true)\n",
    "    print('\\n')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756bb9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mbml",
   "language": "python",
   "name": "mbml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
