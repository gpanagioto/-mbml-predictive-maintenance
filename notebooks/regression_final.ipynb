{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7279b216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "import torch\n",
    "import pyro.distributions as dist\n",
    "import torch.nn as nn\n",
    "\n",
    "from pyro.nn import PyroModule, PyroSample\n",
    "from pyro.infer import SVI, Trace_ELBO, MCMC, NUTS\n",
    "from pyro.optim import ClippedAdam, Adam\n",
    "from pyro.contrib.autoguide import AutoDiagonalNormal\n",
    "from pyro.infer import Predictive\n",
    "from typing import Dict, Tuple, List\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas import DataFrame\n",
    "from tqdm.autonotebook import tqdm\n",
    "from pyro.nn import PyroModule, PyroSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c536413",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/raw/raw2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5699889",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44f9301",
   "metadata": {},
   "source": [
    "Read the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caf57d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "failures_path = os.path.join(data_path, 'PdM_failures.csv')\n",
    "errors_path = os.path.join(data_path, 'PdM_errors.csv')\n",
    "machines_path = os.path.join(data_path, 'PdM_machines.csv')\n",
    "maint_path = os.path.join(data_path, 'PdM_maint.csv')\n",
    "telemetry_path = os.path.join(data_path, 'PdM_telemetry.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1124d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "failures_df = pd.read_csv(failures_path)\n",
    "errors_df = pd.read_csv(errors_path)\n",
    "machines_df = pd.read_csv(machines_path)\n",
    "maint_df = pd.read_csv(maint_path)\n",
    "telemetry_df = pd.read_csv(telemetry_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01458f9",
   "metadata": {},
   "source": [
    "Transform `datetime` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f197c9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "maint_df['datetime'] = pd.to_datetime(maint_df['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "failures_df['datetime'] = pd.to_datetime(failures_df['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "errors_df['datetime'] = pd.to_datetime(errors_df['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "telemetry_df['datetime'] = pd.to_datetime(telemetry_df['datetime'], format=\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588ef274",
   "metadata": {},
   "source": [
    "#### Dataset transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843f0a23",
   "metadata": {},
   "source": [
    "Maintenance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af80ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function that takes a column and returns its mean and std of a moving window of 3 hours\n",
    "def rolling_mean_std(col,window):\n",
    "    return col.rolling(window).agg(['mean', 'std'])\n",
    "\n",
    "#apply the function to the telemetry data\n",
    "def telem_(telemetry,column,window):\n",
    "    telemetry[[column+'mean_'+str(window)+'h', column+'sd_'+str(window)+'h']] = telemetry.groupby('machineID')[column].apply(rolling_mean_std,window)\n",
    "    return telemetry\n",
    "\n",
    "def lifespan(replacement_event_df: DataFrame)->DataFrame:\n",
    "    \n",
    "    '''\n",
    "    Receives a dataframe with timestamp and columns that signify when a component is replaced, with 1.\n",
    "    Returns a dataframe with the days since the last replacement for the component\n",
    "    '''\n",
    "    \n",
    "    comp_rep=replacement_event_df.copy()\n",
    "    points = comp_rep['machineID'].unique()\n",
    "    final=pd.DataFrame()\n",
    "    \n",
    "    for i in tqdm(points, desc='Machine'):\n",
    "        df = comp_rep[(comp_rep['machineID']==i)][['datetime','machineID','comp_comp1', 'comp_comp2', 'comp_comp3', 'comp_comp4']]\n",
    "        for comp in ['comp_comp1', 'comp_comp2', 'comp_comp3', 'comp_comp4']:\n",
    "            \n",
    "            # keep the last part of component name\n",
    "            life = comp[5:]\n",
    "            \n",
    "            #apply function in each row of df[life] column where if row[comp]==1 then value=0.041667 else 0\n",
    "            df[life+'_maint'] = df.apply(lambda row: 0 if row[comp]==1 else 0.041667, axis=1)\n",
    "\n",
    "            df_maint = df[life+'_maint'] != 0\n",
    "            df[life+'_maint'] = df_maint.cumsum()-df_maint.cumsum().where(~df_maint).ffill().fillna(0).astype(int)\n",
    "            df[life+'_maint'] = df[life+'_maint'].apply(lambda x: x*0.041667)\n",
    "            \n",
    "            \n",
    "        final=pd.concat([final,df],axis=0)\n",
    "        final=final[['datetime', 'machineID', 'comp1_maint', 'comp2_maint', 'comp3_maint', 'comp4_maint']]\n",
    "        \n",
    "    return final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36129b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "telemetry_df = telem_(telemetry_df,'volt',3)\n",
    "telemetry_df = telem_(telemetry_df,'rotate',3)\n",
    "telemetry_df = telem_(telemetry_df,'pressure',3)\n",
    "telemetry_df = telem_(telemetry_df,'vibration',3)\n",
    "telemetry_df = telem_(telemetry_df,'volt',24)\n",
    "telemetry_df = telem_(telemetry_df,'rotate',24)\n",
    "telemetry_df = telem_(telemetry_df,'pressure',24)\n",
    "telemetry_df = telem_(telemetry_df,'vibration',24)\n",
    "#telemetry_df=telemetry_df.drop(['volt','rotate','pressure','vibration'],axis=1)\n",
    "telemetry_df=telemetry_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ef9f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "maint_transf_df = pd.get_dummies(maint_df, columns=['comp'])\n",
    "maint_transf_df = telemetry_df.merge(maint_transf_df, on=['datetime', 'machineID'], how='left')\n",
    "maint_transf_df= maint_transf_df[['datetime', 'machineID', 'comp_comp1', 'comp_comp2', 'comp_comp3', 'comp_comp4']]\n",
    "maint_transf_df = maint_transf_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bd151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "maintenance_df = lifespan(maint_transf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c3f93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "maintenance_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f84547",
   "metadata": {},
   "source": [
    "#### Merging the rest of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdad100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fget dummies for errorID\n",
    "error_count = pd.get_dummies(errors_df, columns=['errorID'])\n",
    "error_count.rename(columns={'errorID_error5':'error5count'}, inplace=True)\n",
    "error_count.rename(columns={'errorID_error4':'error4count'}, inplace=True)\n",
    "error_count.rename(columns={'errorID_error3':'error3count'}, inplace=True)\n",
    "error_count.rename(columns={'errorID_error2':'error2count'}, inplace=True)\n",
    "error_count.rename(columns={'errorID_error1':'error1count'}, inplace=True)\n",
    "\n",
    "features = telemetry_df.merge(error_count, on=['datetime', 'machineID'], how='left')\n",
    "\n",
    "# Propagate the error information per error type\n",
    "features[['error1count','error2count','error3count','error4count','error5count']] = features[['error1count','error2count','error3count','error4count','error5count']].fillna(method='ffill')\n",
    "# Fill the iinital error count with 0\n",
    "features = features.fillna(0)\n",
    "\n",
    "# turn \"model\" variable into dummy variables\n",
    "machines_df['model'] = machines_df['model'].astype('category')\n",
    "machines_dummy = pd.get_dummies(machines_df, drop_first=False)\n",
    "\n",
    "# Add the machine metadata information\n",
    "features = features.merge(machines_df[['machineID','model']], on=['machineID'], how='left')\n",
    "features = features.merge(machines_dummy, on=['machineID'], how='left')\n",
    "features = features.merge(maintenance_df, on=['datetime', 'machineID'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023436ef",
   "metadata": {},
   "source": [
    "Merge the failures dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5592b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "fails = pd.get_dummies(failures_df,columns=['failure'])\n",
    "#fails.rename(columns={'failure_comp1':'comp1'}, inplace=True)\n",
    "#fails.rename(columns={'failure_comp2':'comp2'}, inplace=True)\n",
    "#fails.rename(columns={'failure_comp3':'comp3'}, inplace=True)\n",
    "#fails.rename(columns={'failure_comp4':'comp4'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69342e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lifespan_fails(comp_rep0):\n",
    "    comp_rep=comp_rep0.copy()\n",
    "    points = comp_rep['machineID'].unique()\n",
    "    final=pd.DataFrame()\n",
    "\n",
    "    for i in points:\n",
    "        df = comp_rep[(comp_rep['machineID']==i)][['datetime','machineID','failure_comp1','failure_comp2','failure_comp3','failure_comp4']]\n",
    "        for comp in ['failure_comp1','failure_comp2','failure_comp3','failure_comp4']:\n",
    "            life=comp.split('_')[1]+'_life'\n",
    "#             prob=comp+'_prob'\n",
    "#             probkm=comp+'_probkm'\n",
    "            df[life] = df.apply(lambda row: row['datetime'] if row[comp]==0 else np.nan, axis=1)\n",
    "            df[df[life].isna()==False].index\n",
    "            df[life].fillna(method='backfill', inplace=True)\n",
    "            df[life] = pd.to_datetime(df[life]) - df['datetime']\n",
    "            df[life] = df[life].apply(lambda row: row.total_seconds()/86400)\n",
    "#             df[prob] = df[life]/(df[life]+df[comp])\n",
    "#             df[prob].fillna(1, inplace=True)\n",
    "#             df[probkm] = 1-1/(df[life]+df[comp])\n",
    "            #back fill with pad\n",
    "#             df[probkm]=df[probkm].replace(-np.inf, np.nan)\n",
    "            \n",
    "        final=pd.concat([final,df],axis=0)\n",
    "    return final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ce270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fails_ = telemetry_df.merge(fails, on=['datetime', 'machineID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8599f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fails_ = fails_.fillna(method='bfill', limit=24) # fill backward up to 24h if all data, otherwise it must be  7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07adcd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "fails_transf = lifespan_fails(fails_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6c7774",
   "metadata": {},
   "outputs": [],
   "source": [
    "fails_transf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da269f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10854d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the individual components that fail , the columns with 0 or 1, after the merge and backfill\n",
    "fails_transf = fails_transf.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f218cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features = features.merge(fails_transf, on=['datetime', 'machineID'], how='left')\n",
    "# labeled_features = labeled_features.fillna(method='bfill', limit=24) # fill backward up to 24h if all data, otherwise it must be  7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e9321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert \"failure\" target variables into multiple binary targets \n",
    "# # i.e. one per component indicating failure/no failure\n",
    "# labeled_features['comp1_fail'] = (labeled_features['failure'] == 'comp1').astype(int)\n",
    "# labeled_features['comp2_fail'] = (labeled_features['failure'] == 'comp2').astype(int)\n",
    "# labeled_features['comp3_fail'] = (labeled_features['failure'] == 'comp3').astype(int)\n",
    "# labeled_features['comp4_fail'] = (labeled_features['failure'] == 'comp4').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cd6f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccf6a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82891c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20a218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features.to_csv('../data/processed/labeled_1h_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b787046",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87d28dc",
   "metadata": {},
   "source": [
    "Machine age - machine age by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3c3fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxh_groupby(df, feature_name, by):\n",
    "    \"\"\"\n",
    "    Box plot with groupby\n",
    "    \n",
    "    df: DataFrame\n",
    "    feature_name: Name of the feature to be plotted\n",
    "    by: Name of the feature based on which groups are created\n",
    "    \"\"\"\n",
    "    df.boxplot(column=feature_name, by=by, vert=False, \n",
    "                              figsize=(10, 6))\n",
    "    plt.title(f'Distribution of {feature_name} by {by}')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_histogram(df, attribute, title_name, bins, figsize=(9,3), alpha=1, label=None):\n",
    "    df[attribute].plot(kind='hist', \n",
    "                              bins=bins, \n",
    "                              figsize=figsize,\n",
    "                              alpha=alpha,\n",
    "                              label=label,\n",
    "                              title=f'{title_name.title()} distribution')\n",
    "    \n",
    "def plot_bar_sortvals(df, attribute, title, figsize=(5,5)):\n",
    "    df[attribute].value_counts(\n",
    "                normalize=False, dropna=False).sort_values().plot(\n",
    "                kind='bar', figsize=figsize, grid=True,\n",
    "                title=title)\n",
    "\n",
    "    \n",
    "def plot_scatter(df, x_axis_attr, y_axis_attr, figsize=(5,5), title=None, legend=None):\n",
    "    df.plot.scatter(x_axis_attr, y_axis_attr, \n",
    "                    figsize=figsize, title=title, \n",
    "                    legend=legend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa842fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features.boxplot(column='age', vert=False, figsize=(5,3))\n",
    "plt.title('Age distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8421c49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features.boxplot(column='age', by='model', vert=False, figsize=(8,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba7cf1f",
   "metadata": {},
   "source": [
    "```\n",
    "We can see that `model 4` has the lowest median age\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f521edcc",
   "metadata": {},
   "source": [
    "Failure per component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f8b5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_sortvals(failures_df, 'failure', 'Number of failures per component')\n",
    "plt.ylabel('Failures')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ca890e",
   "metadata": {},
   "source": [
    "Telemetry data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b4e1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(labeled_features, 'volt', 'Voltage', bins=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c078fa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(labeled_features, 'rotate', 'RPM', bins=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236abe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(labeled_features, 'pressure', 'Pressure', bins=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d242ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(labeled_features, 'vibration', 'Vibration', bins=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdb5d03",
   "metadata": {},
   "source": [
    "```\n",
    "All the telemetry data, taking into consideration all machines, look to be normally distributed. The odd one being rotation, which looks a skewed on the left.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93caac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in labeled_features.groupby(['model']):\n",
    "    plot_histogram(model[1], 'volt', 'Voltage',bins=300, alpha=0.3, label=model[0])\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e58888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in labeled_features.groupby(['model']):\n",
    "    plot_histogram(model[1], 'rotate', 'RPM',bins=300, alpha=0.3, label=model[0])\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52339df4",
   "metadata": {},
   "source": [
    "#### Create a temporary dataframe, which is the `labeled_features` dataframe used in training, but augmented with time features:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474ae189",
   "metadata": {},
   "source": [
    "Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b71d930",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_sortvals(errors_df, 'errorID', title='Number of error per type')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44919d20",
   "metadata": {},
   "source": [
    "```\n",
    "The most common error type is 1, while the least common is error 5.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e018b5c2",
   "metadata": {},
   "source": [
    "Plot age vs errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc03edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_per_machine = errors_df.groupby(\"machineID\").size()\n",
    "errors_per_machine = pd.DataFrame(errors_per_machine, columns=[\"num_errors\"]).reset_index()\n",
    "\n",
    "machines_fail_data = pd.merge(machines_df, errors_per_machine, how='left', on=\"machineID\")\n",
    "\n",
    "maint_per_machine = maint_df.groupby(\"machineID\").size()\n",
    "maint_per_machine = pd.DataFrame(maint_per_machine, columns=[\"num_maint\"]).reset_index()\n",
    "\n",
    "machines_fail_data = pd.merge(machines_fail_data, maint_per_machine, how='left', on=\"machineID\")\n",
    "\n",
    "failure_per_machine = failures_df.groupby(\"machineID\").size()\n",
    "failure_per_machine = pd.DataFrame(failure_per_machine, columns=[\"num_failure\"]).reset_index()\n",
    "\n",
    "machines_fail_data = pd.merge(machines_fail_data, failure_per_machine, how='left', on=\"machineID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a6e3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "machines_fail_data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011e0599",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(machines_fail_data, \"age\", \"num_errors\", \n",
    "             title=\"Age versus number of errors\")\n",
    "\n",
    "plot_scatter(machines_fail_data, \"age\", \"num_failure\", \n",
    "             title=\"Age versus number of failures\")\n",
    "\n",
    "plot_scatter(machines_fail_data, \"age\", \"num_maint\", \n",
    "             title=\"Age versus total number of\\ncomponent maintenance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2c1223",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = machines_fail_data.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm').set_precision(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1087117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_features = labeled_features.copy()\n",
    "augmented_features['month'] = augmented_features.datetime.dt.month\n",
    "augmented_features['week_of_year'] = augmented_features.datetime.dt.isocalendar().week\n",
    "augmented_features['hour'] = augmented_features.datetime.dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4061f240",
   "metadata": {},
   "source": [
    "Failures per machine per component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1860527",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = failures_df.groupby([\"machineID\", \"failure\"]).size().reset_index()\n",
    "temp_df.columns = [\"machineID\", \"comp\", \"num_fail\"]\n",
    "temp_df_pivot = pd.pivot(temp_df, index=\"machineID\", columns=\"comp\", values=\"num_fail\").rename_axis(None, axis=1)\n",
    "\n",
    "temp_df_pivot.plot.bar(stacked=True, figsize=(20, 6), title=\"Count of failures per component for different Machines\")\n",
    "plt.xlabel(\"Machine ID\")\n",
    "plt.ylabel(\"Number of components that failed\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0398d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_features['month'].value_counts(\n",
    "                normalize=False, dropna=False).sort_values().plot(\n",
    "                kind='bar', figsize=(5,5), grid=True,\n",
    "                title='Number of error per type')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b56ad53",
   "metadata": {},
   "source": [
    "### data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b384e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features = pd.read_csv('../data/processed/labeled_1h_data.csv',  index_col='datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8d27bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39c0fa3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(874906, 44)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6606d7fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['machineID', 'volt', 'rotate', 'pressure', 'vibration', 'voltmean_3h',\n",
       "       'voltsd_3h', 'rotatemean_3h', 'rotatesd_3h', 'pressuremean_3h',\n",
       "       'pressuresd_3h', 'vibrationmean_3h', 'vibrationsd_3h', 'voltmean_24h',\n",
       "       'voltsd_24h', 'rotatemean_24h', 'rotatesd_24h', 'pressuremean_24h',\n",
       "       'pressuresd_24h', 'vibrationmean_24h', 'vibrationsd_24h', 'error1count',\n",
       "       'error2count', 'error3count', 'error4count', 'error5count', 'model',\n",
       "       'age', 'model_model1', 'model_model2', 'model_model3', 'model_model4',\n",
       "       'comp1_maint', 'comp2_maint', 'comp3_maint', 'comp4_maint',\n",
       "       'failure_comp1', 'failure_comp2', 'failure_comp3', 'failure_comp4',\n",
       "       'comp1_life', 'comp2_life', 'comp3_life', 'comp4_life'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_features.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77036eb6",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c54aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(data, test_size):\n",
    "    \n",
    "    machines = machines_df\n",
    "    \n",
    "    x_train_,x_test_,y_train_,y_test_,train_idx,test_idx = train_test_split(machines, machines.model, machines.machineID.index, test_size=test_size, stratify=machines.model, random_state=42)\n",
    "    \n",
    "    training=data[data['machineID'].isin(train_idx)]\n",
    "    training=training.drop(columns=['machineID']).to_numpy()\n",
    "    test=data[data['machineID'].isin(test_idx)]\n",
    "    test=test.drop(columns=['machineID']).to_numpy()\n",
    "    \n",
    "    x_train=training[:,:-1]\n",
    "    y_train=training[:,-1]\n",
    "    x_test=test[:,:-1]\n",
    "    y_test=test[:,-1]\n",
    "\n",
    "    return x_train,x_test,y_train,y_test\n",
    "\n",
    "def preprocess(X_init, test_size, model:str, classi=False, splitting=True):\n",
    "\n",
    "        if splitting:\n",
    "            X_train_unsc, X_test_unsc, y_train_unsc, y_test_unsc =split(X_init,test_size=test_size)\n",
    "            X_init = X_init.drop(columns=['machineID']).to_numpy()\n",
    "            y = X_init[:,-1]\n",
    "            X = X_init[:,:-1]\n",
    "        else:\n",
    "            X_init = X_init.drop(columns=['machineID']).to_numpy()\n",
    "            y = X_init[:,-1]\n",
    "            X = X_init[:,:-1]\n",
    "            X_train_unsc, X_test_unsc, y_train_unsc, y_test_unsc = train_test_split(X, y, test_size=test_size, random_state=0)\n",
    "            \n",
    "        #print(X_train_unsc.shape, y_train_unsc.shape, X_test_unsc.shape, y_test_unsc.shape)\n",
    "        \n",
    "        X_mean = X_train_unsc.mean(axis=0)\n",
    "        X_std = X_train_unsc.std(axis=0)\n",
    "\n",
    "        y_std = y_train_unsc.std()\n",
    "        y_mean = y_train_unsc.mean()\n",
    "\n",
    "        X_train = (X_train_unsc - X_mean)/ X_std\n",
    "        X_test = (X_test_unsc - X_mean)/X_std\n",
    "     \n",
    "        y_train = (y_train_unsc- y_mean)/ y_std \n",
    "        y_test = (y_test_unsc- y_mean)/y_std        \n",
    "\n",
    "        X_train_torch = torch.tensor(X_train).float()\n",
    "        \n",
    "        if model == SVI_model_format.poisson['name']:\n",
    "            y_train_torch = torch.tensor(y_train).int()\n",
    "        else:\n",
    "            y_train_torch = torch.tensor(y_train).float()\n",
    "              \n",
    "        if classi:\n",
    "            y_train_torch = torch.tensor(y_train_unsc).float()\n",
    "            y_train = y_train_unsc\n",
    "            y_test = y_test_unsc\n",
    "            \n",
    "        X_test_torch = torch.tensor(X_test).float()\n",
    "\n",
    "        return y, X, X_train_torch, y_train_torch, X_test_torch, X_test, y_test, X_train, y_train, y_std, y_mean\n",
    "\n",
    "    \n",
    "def linear_model(X, obs=None):\n",
    "        \n",
    "    alpha = pyro.sample(\"alpha\", dist.Normal(0., 1.))                   # Prior for the bias/intercept\n",
    "    beta  = pyro.sample(\"beta\", dist.Normal(torch.zeros(X.shape[1]), \n",
    "                                            torch.ones(X.shape[1])).to_event())    # Priors for the regression coeffcients\n",
    "    sigma = pyro.sample(\"sigma\", dist.HalfCauchy(5.))                   # Prior for the variance\n",
    "    \n",
    "    with pyro.plate(\"data\"):\n",
    "        y = pyro.sample(\"y\", dist.Normal(alpha + X.matmul(beta), sigma), obs=obs)\n",
    "      \n",
    "    return y\n",
    "\n",
    "\n",
    "def poisson_model(X, obs=None):\n",
    "    alpha = pyro.sample(\"alpha\", dist.Normal(0., 1.))                   # Prior for the bias/intercept\n",
    "    beta  = pyro.sample(\"beta\", dist.Normal(torch.zeros(X.shape[1]), \n",
    "                                            torch.ones(X.shape[1])).to_event())    # Priors for the regression coeffcients\n",
    "    \n",
    "    with pyro.plate(\"data\"):\n",
    "        y = pyro.sample(\"y\", dist.Poisson(torch.exp(alpha + X.matmul(beta))), obs=obs)\n",
    "        \n",
    "    return y\n",
    "\n",
    "\n",
    "def heteroscedastic_model(X, obs=None):\n",
    "    alpha_mu = pyro.sample(\"alpha_mu\", dist.Normal(0., 1.))                 # Prior for the bias/intercept of the mean\n",
    "    beta_mu  = pyro.sample(\"beta_mu\", dist.Normal(torch.zeros(X.shape[1]), \n",
    "                                               torch.ones(X.shape[1])).to_event())     # Priors for the regression coeffcients of the mean\n",
    "    alpha_v = pyro.sample(\"alpha_v\", dist.Normal(0., 1.))                   # Prior for the bias/intercept of the variance\n",
    "    beta_v  = pyro.sample(\"beta_v\", dist.Normal(torch.zeros(X.shape[1]), \n",
    "                                               torch.ones(X.shape[1])).to_event())     # Priors for the regression coeffcients of the variance\n",
    "    \n",
    "    with pyro.plate(\"data\"):\n",
    "        y = pyro.sample(\"y\", dist.Normal(alpha_mu + X.matmul(beta_mu), torch.exp(alpha_v + X.matmul(beta_v))), obs=obs)\n",
    "        \n",
    "    return y\n",
    "\n",
    "def compute_error(trues: np.array, predicted: np.array, threshold: int):\n",
    "    \n",
    "    if threshold:\n",
    "        print(threshold)\n",
    "        predicted_thres = predicted[np.where(trues<threshold)]\n",
    "        trues_thres  = trues[np.where(trues<threshold)[0]]\n",
    "    else:\n",
    "        print('No threshold')\n",
    "        pass\n",
    "        \n",
    "    corr = np.corrcoef(predicted, trues)[0,1]\n",
    "    mae = np.mean(np.abs(predicted - trues))\n",
    "    rae = np.sum(np.abs(predicted - trues)) / np.sum(np.abs(trues - np.mean(trues)))\n",
    "    rmse = np.sqrt(np.mean((predicted - trues)**2))\n",
    "    r2 = max(0, 1 - np.sum((trues-predicted)**2) / np.sum((trues - np.mean(trues))**2))\n",
    "    try:\n",
    "        return corr, mae, rae, rmse, r2, predicted_thres, trues_thres\n",
    "    except:\n",
    "         return corr, mae, rae, rmse, r2, predicted, trues\n",
    "        \n",
    "def results(dataset: pd.DataFrame, features_importance: np.array, results_dict: Dict, model:str, comp_number: int, y_trues:np.array, y_preds:np.array, threshold=None) -> Dict:\n",
    "    \n",
    "    print(threshold)\n",
    "    dataset.drop(columns='machineID', inplace=True, axis=1)\n",
    "  \n",
    "    corr, mae, rae, rmse, r2, svi_trues, svi_pred = compute_error(trues=y_trues, predicted=y_preds, threshold=threshold)\n",
    "\n",
    "    results_dict[model][f\"comp_{comp_number}\"]['MAE'] = mae\n",
    "    \n",
    "    print(\"CorrCoef: %.3f\\nMAE: %.3f\\nRMSE: %.3f\\nR2: %.3f\" % (corr, mae, rmse, r2))\n",
    "    \n",
    "    sort = features_importance.argsort()\n",
    "    \n",
    "    feautures_dict = dict(zip(dataset.columns[sort].tolist()[0], features_importance[0][sort].tolist()[0]))\n",
    "    \n",
    "    results_dict[model][f\"comp_{comp_number}\"]['FI'] = list(feautures_dict.keys())[list(feautures_dict.values()).index(max(list(feautures_dict.values())) )]\n",
    "    plt.barh(dataset.columns[sort].tolist()[0], features_importance[0][sort].tolist()[0])\n",
    "    plt.xlabel(\"Feature Importance\")\n",
    "    plt.show()\n",
    "  \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca31e841",
   "metadata": {},
   "source": [
    "### SVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d90e0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVI_model_format():\n",
    "    '''\n",
    "    defining the type of the regression\n",
    "    '''\n",
    "    poisson = {'name':\"POISSON\", 'model':poisson_model}\n",
    "    heterosc = {'name':\"HETEROSCEDASTIC\", 'model':heteroscedastic_model}\n",
    "    linear = {'name':\"LINEAR\", 'model':linear_model}\n",
    "\n",
    "class SVI_regression_model():\n",
    "    '''\n",
    "    SVI regression: choosing the correct data, preprocess them, pyro inference and prediction\n",
    "    '''\n",
    "    def __init__(self, data: pd.DataFrame, component: int) -> None:\n",
    "        \n",
    "        self.data = data\n",
    "        self.component = component\n",
    "        print(self.component)\n",
    "\n",
    "    def get_data_for_component(self) -> pd.DataFrame:\n",
    "        \n",
    "        '''\n",
    "        returns the feautures of the dataset and the component we want to predict for\n",
    "        '''\n",
    "        \n",
    "        components_cols = ['comp1_maint', 'comp2_maint', 'comp3_maint', 'comp4_maint',\n",
    "                           'failure_comp1', 'failure_comp2', 'failure_comp3', 'failure_comp4',\n",
    "                           'comp1_life', 'comp2_life', 'comp3_life', 'comp4_life']\n",
    "    \n",
    "        cols = ['machineID', 'voltmean_3h', 'rotatemean_3h', 'pressuremean_3h', 'vibrationmean_3h',\n",
    "                'voltsd_3h', 'rotatesd_3h', 'pressuresd_3h', 'vibrationsd_3h', 'voltmean_24h',\n",
    "                'rotatemean_24h', 'pressuremean_24h', 'vibrationmean_24h', 'voltsd_24h', 'rotatesd_24h',\n",
    "                'pressuresd_24h', 'vibrationsd_24h', 'error1count', 'error2count','error3count',\n",
    "                'error4count', 'error5count','age', 'model_model1', 'model_model2', \n",
    "                'model_model3', 'model_model4'] + [word for word in components_cols if str(self.component) in ([*word]) and 'failure' not in word.split('_')]\n",
    "\n",
    "\n",
    "        return self.data[cols]\n",
    "    \n",
    "    def Preprocess(self, X_init: pd.DataFrame, model: str) -> \\\n",
    "        Tuple[np.ndarray, np.ndarray,  torch.tensor,  torch.tensor, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.float64, np.float64]:\n",
    "\n",
    "        '''\n",
    "        the necessary data preprocess before procceding in pyro inference\n",
    "        '''\n",
    "        print(f\"{model} Regression {self.component}\")\n",
    "        print(X_init.columns)\n",
    "        print('\\n-----------------------------------------------------------------------')\n",
    "\n",
    "        y, X, X_train_torch, y_train_torch, X_test_torch, X_test, y_test, X_train, y_train, y_std, y_mean = preprocess(X_init, 0.1, model, classi=False, splitting=True)\n",
    " \n",
    "        return y, X, X_train_torch, y_train_torch, X_test_torch, X_test, y_test, X_train, y_train, y_std, y_mean\n",
    "        \n",
    "    def pyro_inference(self, X_train_torch: torch.tensor, y_train_torch: torch.tensor, model: object, steps: int):\n",
    "        '''\n",
    "        pyro inference\n",
    "        '''\n",
    "        \n",
    "        # Reset parameter values\n",
    "        pyro.clear_param_store()\n",
    "\n",
    "        # Define guide function\n",
    "        guide = AutoDiagonalNormal(model)\n",
    "\n",
    "        # Define the number of optimization steps\n",
    "        n_steps = steps\n",
    "\n",
    "        # Setup the optimizer\n",
    "        adam_params = {\"lr\": 0.0001} # learning rate (lr) of optimizer\n",
    "        optimizer = ClippedAdam(adam_params)\n",
    "\n",
    "        # Setup the inference algorithm\n",
    "        elbo = Trace_ELBO(num_particles=1)\n",
    "        svi = SVI(model, guide, optimizer, loss=elbo)\n",
    "\n",
    "        # Do gradient steps\n",
    "        for step in range(n_steps):\n",
    "            elbo = svi.step(X_train_torch, y_train_torch)\n",
    "            if step % 100 == 0:\n",
    "                print(\"[%d] ELBO: %.1f\" % (step, elbo))\n",
    "                \n",
    "        return guide\n",
    "    \n",
    "    def post_process(self, guide: object, model: object, X_train_torch: torch.tensor, y_train_torch: torch.tensor, X_test: np.ndarray, y_test: np.ndarray, y_std: np.float64, y_mean: np.float64):\n",
    "\n",
    "        if model['name'] != SVI_model_format.heterosc['name']:\n",
    "            predictive = Predictive(model=model['model'], guide=guide, num_samples=1000, return_sites=(\"alpha\", \"beta\", \"sigma\"))\n",
    "            samples = predictive(X_train_torch, y_train_torch)\n",
    "\n",
    "            alpha_samples = samples[\"alpha\"].detach().numpy()\n",
    "            beta_samples = samples[\"beta\"].detach().numpy()\n",
    "\n",
    "            y_hat = np.mean(np.exp(alpha_samples.T + np.dot(X_test, beta_samples[:,0].T)), axis=1)\n",
    "\n",
    "        else:\n",
    "            predictive = Predictive(model=model['model'], guide=guide, num_samples=1000,\n",
    "                        return_sites=(\"alpha_mu\", \"beta_mu\", \"alpha_v\", \"beta_v\"))\n",
    "            samples = predictive(X_train_torch, y_train_torch)\n",
    "\n",
    "            alpha_samples = samples[\"alpha_mu\"].detach().numpy()\n",
    "            beta_samples = samples[\"beta_mu\"].detach().numpy()\n",
    "\n",
    "            y_hat = np.mean(alpha_samples.T + np.dot(X_test, beta_samples[:,0].T), axis=1)\n",
    "            \n",
    "        # Calculate feature importance\n",
    "        feature_importance = np.mean(np.abs(beta_samples), axis=0)\n",
    "        feature_importance /= np.sum(feature_importance)\n",
    "\n",
    "        # convert back to the original scale\n",
    "        if model['name'] == SVI_model_format.poisson['name']:\n",
    "            preds = y_hat # no need to do any conversion here because the Poisson model received untransformed y's\n",
    "        else:\n",
    "            preds = y_hat * y_std + y_mean\n",
    "            \n",
    "        y_true = y_test * y_std + y_mean\n",
    "\n",
    "        return preds, y_true, feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41231a9",
   "metadata": {},
   "source": [
    "#### Poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd5a4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_results = {\"poisson\": {'comp_1':  {'MAE':0, 'FI':''}, 'comp_2':  {'MAE':0, 'FI':''}, 'comp_3':  {'MAE':0, 'FI':''}, 'comp_4':  {'MAE':0, 'FI':''}}}\n",
    "\n",
    "for comp_number in range(1,5):\n",
    "    print(f\"\\n--------------------------------  component {comp_number}  ---------------------------------------\")\n",
    "    svi_regression = SVI_regression_model(labeled_features, comp_number)\n",
    "    svi_dataset = svi_regression.get_data_for_component()\n",
    "    poisson = SVI_model_format.poisson \n",
    "    \n",
    "    y_poisson, X_poisson, X_train_torch_poisson, y_train_torch_poisson, X_test_torch_poisson, X_test_poisson, y_test_poisson, X_train_poisson, y_train_poisson, y_std_poisson, y_mean_poisson = svi_regression.Preprocess(X_init=svi_dataset, model=poisson['name'])\n",
    "    poisson_guide = svi_regression.pyro_inference(X_train_torch=X_train_torch_poisson, y_train_torch=y_train_torch_poisson, model=poisson['model'], steps=5000)\n",
    "    poisson_preds, poisson_y_true, poisson_features_importance = svi_regression.post_process(guide=poisson_guide, model=poisson, X_train_torch=X_train_torch_poisson, y_train_torch=y_train_torch_poisson, X_test=X_test_poisson, y_test=y_test_poisson, y_std=y_std_poisson, y_mean=y_mean_poisson)\n",
    "    \n",
    "    poisson_results = results(svi_dataset, poisson_features_importance, poisson_results, 'poisson', comp_number, poisson_y_true, poisson_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1134b886",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "poisson_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46f46fe",
   "metadata": {},
   "source": [
    "#### Heteorscedastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7aa62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hetero_results = {'hetero': {'comp_1':  {'MAE':0, 'FI':''}, 'comp_2':  {'MAE':0, 'FI':''}, 'comp_3':  {'MAE':0, 'FI':''}, 'comp_4':  {'MAE':0, 'FI':''}}}\n",
    "\n",
    "for comp_number in range(1,5):\n",
    "    \n",
    "    print(f\"\\n--------------------------------  component {comp_number}  ---------------------------------------\")\n",
    "    svi_regression = SVI_regression_model(labeled_features, comp_number)\n",
    "    svi_dataset = svi_regression.get_data_for_component()\n",
    "    hetero = SVI_model_format.heterosc\n",
    "    \n",
    "    y_hetero, X_hetero, X_train_torch_hetero, y_train_torch_hetero, X_test_torch_hetero, X_test_hetero, y_test_hetero, X_train_hetero, y_train_hetero, y_std_hetero, y_mean_hetero = svi_regression.Preprocess(X_init=svi_dataset, model=hetero['name'])\n",
    "    hetero_guide = svi_regression.pyro_inference(X_train_torch=X_train_torch_hetero, y_train_torch=y_train_torch_hetero, model=hetero['model'], steps=5000)\n",
    "    hetero_preds, hetero_y_true, hetero_features_importance = svi_regression.post_process(guide=hetero_guide, model=hetero, X_train_torch=X_train_torch_hetero, y_train_torch=y_train_torch_hetero, X_test=X_test_hetero, y_test=y_test_hetero, y_std=y_std_hetero, y_mean=y_mean_hetero)\n",
    "\n",
    "    hetero_results = results(svi_dataset, hetero_features_importance, hetero_results, 'hetero', comp_number, hetero_y_true, hetero_preds)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68981060",
   "metadata": {},
   "outputs": [],
   "source": [
    "hetero_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55829176",
   "metadata": {},
   "source": [
    "#### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75397c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svi_results = {'linear_svi': {'comp_1':  {'MAE':0, 'FI':''}, 'comp_2':  {'MAE':0, 'FI':''}, 'comp_3':  {'MAE':0, 'FI':''}, 'comp_4':  {'MAE':0, 'FI':''}}}\n",
    "\n",
    "for comp_number in range(1,5):\n",
    "    \n",
    "    print(f\"\\n--------------------------------  component {comp_number}  ---------------------------------------\")\n",
    "    svi_regression = SVI_regression_model(labeled_features, comp_number)\n",
    "    svi_dataset = svi_regression.get_data_for_component()\n",
    "    linear_svi = SVI_model_format.linear\n",
    " \n",
    "    y_linear_svi, X_linear_svi, X_train_torch_linear_svi, y_train_torch_linear_svi,  X_test_torch_linear_svi, X_test_linear_svi, y_test_linear_svi, X_train_linear_svi, y_train_linear_svi, y_std_linear_svi, y_mean_linear_svi = svi_regression.Preprocess(X_init=svi_dataset, model=linear_svi['name'])\n",
    "    linear_svi_guide = svi_regression.pyro_inference(X_train_torch=X_train_torch_linear_svi, y_train_torch=y_train_torch_linear_svi, model=linear_svi['model'], steps=5000)\n",
    "    linear_svi_preds, linear_svi_y_true, linear_features_importance = svi_regression.post_process(guide=linear_svi_guide, model=linear_svi, X_train_torch=X_train_torch_linear_svi, y_train_torch=y_train_torch_linear_svi, X_test=X_test_linear_svi, y_test=y_test_linear_svi, y_std=y_std_linear_svi, y_mean=y_mean_linear_svi)\n",
    "    \n",
    "    linear_svi_results = results(svi_dataset, linear_features_importance, linear_svi_results, 'linear_svi', comp_number, linear_svi_y_true, linear_svi_preds)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e101fc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svi_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918eeb21",
   "metadata": {},
   "source": [
    "### MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85da0c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCMC_model_format():\n",
    "    '''\n",
    "    defining the type of the regression\n",
    "    '''\n",
    "    poisson = {'name':\"POISSON\", 'model':poisson_model}\n",
    "    heterosc = {'name':\"HETEROSCEDASTIC\", 'model':heteroscedastic_model}\n",
    "    linear = {'name':\"LINEAR\", 'model':linear_model}\n",
    "\n",
    "class MCMC_regression_model():\n",
    "\n",
    "    def __init__(self, data: pd.DataFrame, component: int) -> None:\n",
    "        \n",
    "        self.data = data\n",
    "        self.component = component\n",
    "        print(self.component)\n",
    "\n",
    "    def get_data_for_component(self) -> pd.DataFrame:\n",
    "        \n",
    "        '''\n",
    "        returns the feautures of the dataset and the component we want to predict for\n",
    "        '''\n",
    "    \n",
    "        components_cols = ['comp1_maint', 'comp2_maint', 'comp3_maint', 'comp4_maint',\n",
    "                           'failure_comp1', 'failure_comp2', 'failure_comp3', 'failure_comp4',\n",
    "                           'comp1_life', 'comp2_life', 'comp3_life', 'comp4_life']\n",
    "        \n",
    "        cols = ['machineID', 'voltmean_3h', 'rotatemean_3h',\n",
    "                'pressuremean_3h', 'vibrationmean_3h', 'voltsd_3h', 'rotatesd_3h',\n",
    "                'pressuresd_3h', 'vibrationsd_3h', 'voltmean_24h', 'rotatemean_24h',\n",
    "                'pressuremean_24h', 'vibrationmean_24h', 'voltsd_24h', 'rotatesd_24h',\n",
    "                'pressuresd_24h', 'vibrationsd_24h', 'error1count', 'error2count',\n",
    "                'error3count', 'error4count', 'error5count','age',\n",
    "                'model_model1', 'model_model2', 'model_model3', 'model_model4'] + [word for word in components_cols if str(self.component) in ([*word]) and 'failure' not in word.split('_')]\n",
    "\n",
    "\n",
    "        return self.data[cols]\n",
    "    \n",
    "    def Preprocess(self, X_init: pd.DataFrame, model: str) -> \\\n",
    "        Tuple[np.ndarray, np.ndarray,  torch.tensor,  torch.tensor, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.float64, np.float64]:\n",
    "\n",
    "        '''\n",
    "        the necessary data preprocess before procceding in pyro inference\n",
    "        '''\n",
    "        print(f\"{model} Regression {self.component}\")\n",
    "        print(X_init.columns)\n",
    "        print('\\n-----------------------------------------------------------------------')\n",
    "\n",
    "        y, X, X_train_torch, y_train_torch, X_test_torch, X_test, y_test, X_train, y_train, y_std, y_mean = preprocess(X_init,0.1,model,classi=False,splitting=True)\n",
    " \n",
    "        return y, X, X_train_torch, y_train_torch, X_test_torch, X_test, y_test, X_train, y_train, y_std, y_mean\n",
    "    \n",
    "\n",
    "    def pyro_inference(self, X_train_torch, y_train_torch, model, num_samples):\n",
    "\n",
    "        # Reset parameter values\n",
    "        pyro.clear_param_store()\n",
    "\n",
    "        # Run inference in Pyro\n",
    "        nuts_kernel = NUTS(model)\n",
    "        \n",
    "        mcmc = MCMC(nuts_kernel, num_samples=num_samples, warmup_steps=200, num_chains=1)\n",
    "        mcmc.run(X_train_torch, y_train_torch)\n",
    "        \n",
    "        # Compute feature importance\n",
    "        feature_names = self.data.columns[1:-1]\n",
    "        print(feature_names)\n",
    "        \n",
    "        feature_importance = {}\n",
    "\n",
    "        for feature in feature_names:\n",
    "            marginal = EmpiricalMarginal(mcmc.get_samples(), sites=feature)\n",
    "            feature_importance[feature] = torch.std(marginal._get_samples_and_weights()[0]).item()\n",
    "\n",
    "        print(\"Feature Importance:\")\n",
    "        for feature, importance in feature_importance.items():\n",
    "            print(f\"{feature}: {importance}\")\n",
    "\n",
    "        # Show summary of inference results\n",
    "        print(mcmc.summary())\n",
    "\n",
    "        return mcmc.get_samples()\n",
    "\n",
    "    def post_process(self, X_test, X_train, samples, y_std, y_mean, y_test):\n",
    "\n",
    "        posterior_samples = samples\n",
    "\n",
    "        # Compute predictions\n",
    "        y_hat = np.mean(posterior_samples[\"alpha\"].numpy().T + np.dot(X_test, posterior_samples[\"beta\"].numpy().T), axis=1)\n",
    "        y_hat_train = np.mean(posterior_samples[\"alpha\"].numpy().T + np.dot(X_train, posterior_samples[\"beta\"].numpy().T), axis=1)\n",
    "\n",
    "        # Convert back to the original scale\n",
    "        preds = y_hat * y_std + y_mean\n",
    "        preds_train = y_hat_train * y_std + y_mean\n",
    "        y_true = y_test * y_std + y_mean\n",
    "\n",
    "        return preds, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5e50bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_mcmc_results = {'linear_mcmc': {'comp_1': 0, 'comp_2':0, 'comp_3': 0, 'comp_4':0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194898c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for comp_number in range(1,5):\n",
    "    print(f\"\\n--------------------------------  component {comp_number}  ---------------------------------------\")\n",
    "    mcmc_regression = MCMC_regression_model(labeled_features, comp_number)\n",
    "    mcmc_dataset = mcmc_regression.get_data_for_component()\n",
    "    linear_mcmc = MCMC_model_format.linear\n",
    " \n",
    "\n",
    "    y_linear_mcmc, X_linear_mcmc, X_train_torch_linear_mcmc, y_train_torch_linear_mcmc,  X_test_torch_linear_mcmc, X_test_linear_mcmc, y_test_linear_mcmc, X_train_linear_mcmc, y_train_linear_mcmc, y_std_linear_mcmc, y_mean_linear_mcmc = mcmc_regression.Preprocess(X_init=mcmc_dataset, model=linear_mcmc['name'])\n",
    "  \n",
    "    linear_mcmc_samples = mcmc_regression.pyro_inference(X_train_torch=X_train_torch_linear_mcmc, y_train_torch=y_train_torch_linear_mcmc, model=linear_mcmc['model'], num_samples=100)\n",
    "    linear_mcmc_preds, linear_mcmc_y_true = mcmc_regression.post_process(samples=linear_mcmc_samples, X_train = X_train_linear_mcmc, X_test = X_test_linear_mcmc, y_test=y_test_linear_mcmc, y_std=y_std_linear_mcmc, y_mean=y_mean_linear_mcmc)\n",
    "    \n",
    "    corr_linear_mcmc, mae_linear_mcmc, rae_linear_mcmc, rmse_linear_mcmc, r2_linear_mcmc, mcmc_trues_linear_mcmc, mcmc_pred_linear_mcmc = compute_error(trues=linear_mcmc_y_true, predicted=linear_mcmc_preds, threshold=None)\n",
    "    linear_mcmc_results['linear_mcmc'][f\"comp_{comp_number}\"] = [mae_linear_mcmc]\n",
    "    print(\"CorrCoef: %.3f\\nMAE: %.3f\\nRMSE: %.3f\\nR2: %.3f\" % (corr_linear_mcmc, mae_linear_mcmc, rmse_linear_mcmc, r2_linear_mcmc))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f42cfe",
   "metadata": {},
   "source": [
    "### NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57b497cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelFormat():\n",
    "    '''\n",
    "    defining the type of the regression\n",
    "    '''\n",
    "    neural_netwrok = {'name':'NN'}\n",
    "    poisson = {'name':\"POISSON\", 'model':poisson_model}\n",
    "    heterosc = {'name':\"HETEROSCEDASTIC\", 'model':heteroscedastic_model}\n",
    "    linear = {'name':\"LINEAR\", 'model':linear_model}\n",
    "\n",
    "class FFNN(PyroModule):\n",
    "    def __init__(self, n_in, n_hidden, n_out, type_forward):\n",
    "        \n",
    "        self.type_forward = type_forward\n",
    "        super(FFNN, self).__init__()\n",
    "        \n",
    "        # Architecture\n",
    "        self.in_layer = PyroModule[nn.Linear](n_in, n_hidden)\n",
    "        self.in_layer.weight = PyroSample(dist.Normal(0., 1.).expand([n_hidden, n_in]).to_event(2))\n",
    "\n",
    "        self.h_layer = PyroModule[nn.Linear](n_hidden, n_hidden)\n",
    "        self.h_layer.weight = PyroSample(dist.Normal(0., 1.).expand([n_hidden, n_hidden]).to_event(2))\n",
    "\n",
    "        self.h_layer = PyroModule[nn.Linear](n_hidden, n_hidden)\n",
    "        self.h_layer.weight = PyroSample(dist.Normal(0., 1.).expand([n_hidden, n_hidden]).to_event(2))\n",
    "\n",
    "        self.out_layer = PyroModule[nn.Linear](n_hidden, n_out)\n",
    "        self.out_layer.weight = PyroSample(dist.Normal(0., 1.).expand([n_out, n_hidden]).to_event(2))\n",
    "\n",
    "        # Activation functions\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, X, y=None):\n",
    "        \n",
    "        if self.type_forward == 'simple':\n",
    "            X = self.tanh(self.in_layer(X))\n",
    "            X = self.tanh(self.h_layer(X))\n",
    "            X = self.out_layer(X)\n",
    "            prediction_mean = X.squeeze(-1)\n",
    "            with pyro.plate(\"observations\"):\n",
    "                y = pyro.sample(\"obs\", dist.Normal(prediction_mean, 0.1), obs=y)\n",
    "        \n",
    "        elif self.type_forward == 'interpretable':\n",
    "            X_nn = X[:,1:]\n",
    "            X_nn = self.tanh(self.in_layer(X_nn))\n",
    "            X_nn = self.tanh(self.h_layer(X_nn))\n",
    "            X_nn = self.out_layer(X_nn)\n",
    "            nn_out = X_nn.squeeze(-1)\n",
    "\n",
    "            beta_lin = pyro.sample(\"beta\", dist.Normal(0, 1))\n",
    "            X_linear = X[:,0]\n",
    "            with pyro.plate(\"observations\"):\n",
    "                linear_out = X_linear*beta_lin\n",
    "                y = pyro.sample(\"obs\", dist.Normal(nn_out+linear_out, 0.1), obs=y)\n",
    "\n",
    "        return y\n",
    "\n",
    "def train_nn(model0, X_train_torch, y_train_torch):\n",
    "    \n",
    "    if model0 ==\"simple\":\n",
    "        model = FFNN(n_in=X_train_torch.shape[1], n_hidden=32, n_out=1, type_forward=model0)\n",
    "        \n",
    "    elif model0 == \"interpretable\":\n",
    "        model = FFNN(n_in=X_train_torch.shape[1]-1, n_hidden=32, n_out=1, type_forward=model0)\n",
    "        \n",
    "    guide = AutoDiagonalNormal(model)\n",
    "    pyro.clear_param_store()\n",
    "    # Define the number of optimization steps\n",
    "    n_steps = 1000\n",
    "\n",
    "    # Setup the optimizer\n",
    "    adam_params = {\"lr\": 0.01}\n",
    "    optimizer = Adam(adam_params)\n",
    "\n",
    "    # Setup the inference algorithm\n",
    "    elbo = Trace_ELBO(num_particles=1)\n",
    "    svi = SVI(model, guide, optimizer, loss=elbo)\n",
    "\n",
    "    # Do gradient steps\n",
    "    for step in range(n_steps):\n",
    "        elbo = svi.step(X_train_torch,y_train_torch)\n",
    "        if step % 500 == 0:\n",
    "            print(\"[%d] ELBO: %.1f\" % (step, elbo))\n",
    "    \n",
    "    return model, guide\n",
    "\n",
    "def get_data_for_component(data, component) -> pd.DataFrame:\n",
    "        \n",
    "    '''\n",
    "    returns the feautures of the dataset and the component we want to predict for\n",
    "    '''\n",
    "    \n",
    "    components_cols = ['comp1_maint', 'comp2_maint', 'comp3_maint', 'comp4_maint',\n",
    "                       'failure_comp1', 'failure_comp2', 'failure_comp3', 'failure_comp4',\n",
    "                       'comp1_life', 'comp2_life', 'comp3_life', 'comp4_life']\n",
    "        \n",
    "    cols = ['machineID', 'voltmean_3h', 'rotatemean_3h', 'pressuremean_3h', 'vibrationmean_3h',\n",
    "            'voltsd_3h', 'rotatesd_3h', 'pressuresd_3h', 'vibrationsd_3h', 'voltmean_24h',\n",
    "            'rotatemean_24h', 'pressuremean_24h', 'vibrationmean_24h', 'voltsd_24h', 'rotatesd_24h',\n",
    "            'pressuresd_24h', 'vibrationsd_24h', 'error1count', 'error2count', 'error3count',\n",
    "            'error4count', 'error5count','age', 'model_model1', 'model_model2',\n",
    "            'model_model3', 'model_model4'] + [word for word in components_cols if str(component) in ([*word]) and 'failure' not in word.split('_')]\n",
    "\n",
    "\n",
    "    return data[cols]\n",
    "\n",
    "def test_nn(model,guide,X_test_torch):\n",
    "    \n",
    "    # Predict\n",
    "    predictive = pyro.infer.Predictive(model, guide=guide, num_samples=1000,return_sites=(\"obs\", \"_RETURN\"))\n",
    "    samples = predictive(X_test_torch)\n",
    "    y_pred = samples[\"obs\"].mean(axis=0).detach().numpy()\n",
    "    \n",
    "    y_preds = y_pred * y_std + y_mean\n",
    "    y_true = y_test * y_std + y_mean\n",
    "    \n",
    "    return y_preds, y_true\n",
    "\n",
    "def test_nn_beta(model,guide,X_test_torch):\n",
    "    # Predict\n",
    "    predictive = pyro.infer.Predictive(model, guide=guide, num_samples=1000,return_sites=(\"beta\",))\n",
    "    samples = predictive(X_test_torch)\n",
    "    print(\"Estimated beta:\", samples[\"beta\"].mean(axis=0).detach().numpy())\n",
    "    \n",
    "    y_pred = samples[\"beta\"].mean(axis=0).detach().numpy()\n",
    "    \n",
    "    y_preds = y_pred * y_std + y_mean\n",
    "    y_true = y_test * y_std + y_mean\n",
    "    \n",
    "    return y_preds, y_true\n",
    "\n",
    "\n",
    "def mae_test(y_pred, y_test):\n",
    "\n",
    "    mae = np.mean(np.abs(y_pred - y_test))\n",
    "    print(\"MAE:\", mae)\n",
    "    \n",
    "    return mae\n",
    "\n",
    "def plot_pred(y_pred,y_test,y_std,y_mean,threshold,start=None,end=None):\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "    start = start\n",
    "    end = end\n",
    "    y_true = y_test * y_std + y_mean\n",
    "    y_pre = y_pred * y_std + y_mean\n",
    "\n",
    "    plt.plot(y_true[y_pre>threshold], 'r.-', label='test')\n",
    "    plt.plot(y_pre[y_pre>threshold], 'b-', label='pred')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return y_true,y_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4baec336",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svi_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m      4\u001b[0m     feature \u001b[38;5;241m=\u001b[39m poisson_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoisson\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomp_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFI\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m     feature_for_comp[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomp_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msvi_dataset\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(feature)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'svi_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "feature_for_comp = {'comp_1':  {'Feature':0}, 'comp_2':  {'Feature':0}, 'comp_3':  {'Feature':0}, 'comp_4':  {'Feature':0}}\n",
    "for i in range(1,5):\n",
    "    feature = poisson_results['poisson'][f\"comp_{i}\"]['FI']\n",
    "    feature_for_comp[f\"comp_{i}\"]['Feature'] = svi_dataset.columns.get_loc(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f579cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_for_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9b8294",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,5):\n",
    "    print(f\"\\n--------------------------------  component {i}  ---------------------------------------\")\n",
    "    nn_dataset = get_data_for_component(labeled_features, i)\n",
    "    y, X, X_train_torch, y_train_torch,X_test_torch, X_test, y_test, X_train, y_train, y_std, y_mean = preprocess(nn_dataset, 0.1, 'neural_network')\n",
    "    \n",
    "    model_FFNN, guide = train_nn(\"interpretable\", X_train_torch, y_train_torch, feature_for_comp[f\"comp_{i}\"]['Feature']-1)\n",
    "    y_preds, y_true = test_nn_beta(model_FFNN, guide, X_test_torch)\n",
    "    mae = mae_test(y_preds, y_true)\n",
    "    print('\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae62b976",
   "metadata": {},
   "source": [
    "### Data Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e199752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(labeled_features[[\"comp1_life\",\"comp2_life\",\"comp3_life\",\"comp4_life\"]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119b4b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_results = {\"poisson\": {'comp_1':  {'MAE':0, 'FI':''}, 'comp_2':  {'MAE':0, 'FI':''}, 'comp_3':  {'MAE':0, 'FI':''}, 'comp_4':  {'MAE':0, 'FI':''}}}\n",
    "\n",
    "for comp_number in range(1,5):\n",
    "    print(f\"\\n--------------------------------  component {comp_number}  ---------------------------------------\")\n",
    "    data = labeled_features[labeled_features[f\"comp{comp_number}_life\"]<=30]\n",
    "    print(data.shape)\n",
    "    svi_regression = SVI_regression_model(data, comp_number)\n",
    "    svi_dataset = svi_regression.get_data_for_component()\n",
    "    poisson = SVI_model_format.poisson \n",
    "    \n",
    "    y_poisson, X_poisson, X_train_torch_poisson, y_train_torch_poisson, X_test_torch_poisson, X_test_poisson, y_test_poisson, X_train_poisson, y_train_poisson, y_std_poisson, y_mean_poisson = svi_regression.Preprocess(X_init=svi_dataset, model=poisson['name'])\n",
    "    poisson_guide = svi_regression.pyro_inference(X_train_torch=X_train_torch_poisson, y_train_torch=y_train_torch_poisson, model=poisson['model'], steps=5000)\n",
    "    poisson_preds, poisson_y_true, poisson_features_importance = svi_regression.post_process(guide=poisson_guide, model=poisson, X_train_torch=X_train_torch_poisson, y_train_torch=y_train_torch_poisson, X_test=X_test_poisson, y_test=y_test_poisson, y_std=y_std_poisson, y_mean=y_mean_poisson)\n",
    "    \n",
    "    poisson_results = results(svi_dataset, poisson_features_importance, poisson_results, 'poisson', comp_number, poisson_y_true, poisson_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e3f7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f303c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_results = {\"poisson\": {'comp_1':  {'MAE':0, 'FI':''}, 'comp_2':  {'MAE':0, 'FI':''}, 'comp_3':  {'MAE':0, 'FI':''}, 'comp_4':  {'MAE':0, 'FI':''}}}\n",
    "\n",
    "for comp_number in range(1,5):\n",
    "    print(f\"\\n--------------------------------  component {comp_number}  ---------------------------------------\")\n",
    "    svi_regression = SVI_regression_model(labeled_features, comp_number)\n",
    "    svi_dataset = svi_regression.get_data_for_component()\n",
    "    poisson = SVI_model_format.poisson \n",
    "    \n",
    "    y_poisson, X_poisson, X_train_torch_poisson, y_train_torch_poisson, X_test_torch_poisson, X_test_poisson, y_test_poisson, X_train_poisson, y_train_poisson, y_std_poisson, y_mean_poisson = svi_regression.Preprocess(X_init=svi_dataset, model=poisson['name'])\n",
    "    poisson_guide = svi_regression.pyro_inference(X_train_torch=X_train_torch_poisson, y_train_torch=y_train_torch_poisson, model=poisson['model'], steps=1000)\n",
    "    poisson_preds, poisson_y_true, poisson_features_importance = svi_regression.post_process(guide=poisson_guide, model=poisson, X_train_torch=X_train_torch_poisson, y_train_torch=y_train_torch_poisson, X_test=X_test_poisson, y_test=y_test_poisson, y_std=y_std_poisson, y_mean=y_mean_poisson)\n",
    "    \n",
    "    poisson_results = results(svi_dataset, poisson_features_importance, poisson_results, 'poisson', comp_number, poisson_y_true, poisson_preds, threshold=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba75cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mbml",
   "language": "python",
   "name": "mbml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
